# Extract text from HMD titles
The XML we are making freely available as part of the Heritage Made Digital project uses the METS/ALTO standard. 

METS and ALTO are XML standards maintained by the Library of Congress. An XML standard is a set of desciptions The combination of METS and ALTO 
## Build functions

## Extract text



```{r}
library(tidyverse)
library(furrr)
```




```{r}
load('hmd_file_index')
```


```{r}
hmd_file_index = hmd_file_index %>% 
mutate(value = str_replace(value, "//", "/")) %>% 
mutate(value = paste0("/",value))
```


```{r}
# Functions to extract .csv files from Newspapers using the FMP METS/ALTO and folder structure.

# First function is from https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/

extractor <- function(string, regex, all = FALSE){
  if(all) {
    string %>%
      str_extract_all(regex) %>%
      flatten_chr() %>%
      str_extract_all("[:alnum:]+", simplify = FALSE) %>%
      map(paste, collapse = "_") %>%
      flatten_chr()
  } else {
    string %>%
      str_extract(regex) %>%
      str_extract_all("[:alnum:]+", simplify = TRUE) %>%
      paste(collapse = " ") %>%
      tolower()
  }
}

extract_page_groups <- function(article){
id <- article %>%
extractor("(?<=<mets:smLocatorLink xlink:href=\"#)(.*?)(?=\" xlink:label=\")", all = TRUE)
type =
tibble::tribble(~id,
id)
}
```

```{r}

# Next this extracts a single ALTO page:


  get_page = function(alto){
    page = alto %>%  read_file() %>%
      str_split("\n", simplify = TRUE) %>%
      keep(str_detect(., "CONTENT|<TextBlock ID=")) %>%
      str_extract("(?<=CONTENT=\")(.*?)(?=WC)|(?<=<TextBlock ID=)(.*?)(?= HPOS=)")%>%
      discard(is.na) %>%
      as.tibble() %>%
      mutate(pa = ifelse(str_detect(value, "pa[0-9]{7}"),
                         str_extract(value, "pa[0-9]{7}"), NA)) %>%
      fill(pa) %>%
      filter(str_detect(pa, "pa[0-9]{7}")) %>%
      filter(!str_detect(value, "pa[0-9]{7}")) %>%
      filter(!str_detect(value, "SUBS_TYPE=HypPart1 SUBS_CONTENT=")) %>% mutate(value =
                                                                                  str_remove_all(value, "STYLE=\"subscript\" ")) %>% mutate(value = str_remove_all(value, "\"")) %>%
      mutate(value = str_remove_all(value, "SUBS_TYPE=HypPart1 SUBS_CONTENT=")) %>%
      mutate(value = str_remove_all(value, "SUBS_TYPE=HypPart2 SUBS_CONTENT="))

  }

```

```{r}
# This uses the above to go through a vector of folder names and convert each issue into a single .csv, with a row per article.

make_articles_sample <- function(foldername,path = "/data/shared/data/bllabs/devnas2/Newspapers/completed_extracted_files/"){

  metsfilename =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), ".*mets.xml") %>%
    discard(is.na)

  csvfoldername = metsfilename %>% str_remove("_mets.xml") %>% str_replace(path, "") %>% str_remove("/BLIP_[0-9]{8}_[0-9]{2}") %>% str_remove("/[0-9]{7}/[0-9]{4}/[0-9]{4}/")

  metsfile = read_file(metsfilename)

  page_list =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), ".*[0-9]{4}.xml") %>%
    discard(is.na)



  metspagegroups = metsfile %>% str_split("<mets:smLinkGrp>")%>%
    flatten_chr() %>%
    as_tibble() %>% filter(str_detect(value, '#art[0-9]{4}')) %>% mutate(articleid = str_extract(value,"[0-9]{4}"))

  tic <- Sys.time()
  future_map(page_list, get_page) %>%
    bind_rows()  %>% left_join(extract_page_groups(metspagegroups$value) %>%
                                 unnest() %>%
                                 mutate(art = ifelse(str_detect(id, "art"), str_extract(id, "[0-9]{4}"), NA)) %>%
                                 fill(art) %>% filter(!str_detect(id, "art[0-9]{4}")), by = c('pa' = 'id')) %>%
    group_by(art) %>%
    summarise(text = paste0(value, collapse = ' ')) %>%
    mutate(issue_name = metsfilename ) %>%
    write_csv(path = paste0(csvfoldername, ".csv"))

  toc = Sys.time()

  toc - tic

}

```


```{r}
# This next function uses a dataframe of all the folders
# (otherwise it's way too time-consuming to search through them each time
# so this large dataframe needs to be present)

# Here you can specify an NLP, a year and a month/date pair. It takes a regular expression, so you could
# also use something like "1801|1802" and get multiple years. You can also specify the number of files you
# want back.

pickSample = function(df, NLP = "0002194", yyyy = "[0-9]{4}", mmdd = "[0-9]{4}", sample = 1){

  df %>% filter(str_detect(value, paste0("/",NLP,"/",yyyy,"/",mmdd))) %>% sample_n(sample) %>% pull(value)


}


# Last this wraps it all in a function which allows you to specify the variables and run them through
# the

extract_news_csv = function(path, yearpick = NULL, mmddpick = "[0-9]{4}", nlppick = "0002194",samplepick = 1){

  folderslist = hmd_file_index %>% pickSample(yyyy = yearpick, mmdd = mmddpick,NLP = nlppick, sample = samplepick)

  future_map(folderslist, make_articles_sample)

}

```


```{r}
g = extract_news_csv(samplepick = 5)
```


```{r}
pickSample = function(df, NLP = "[0-9]{7}", yyyy = "[0-9]{4}", mmdd = "[0-9]{4}", sample = 5){

  df %>% filter(str_detect(value, paste0("/",NLP,"/",yyyy,"/",mmdd))) %>% sample_n(sample) %>% pull(value)


}
```


```{r}
hmd_file_index %>% pickSample(yyyy = "1850", mmdd = "[0-9]{4}", sample = 100)
```




```{r}
extractor <- function(string, regex, all = FALSE){
  if(all) {
    string %>%
      str_extract_all(regex) %>%
      flatten_chr() %>%
      str_extract_all("[:alnum:]+", simplify = FALSE) %>%
      map(paste, collapse = "_") %>%
      flatten_chr()
  } else {
    string %>%
      str_extract(regex) %>%
      str_extract_all("[:alnum:]+", simplify = TRUE) %>%
      paste(collapse = " ") %>%
      tolower()
  }
}
```


```{r}
extract_page_groups <- function(article){
id <- article %>%
extractor("(?<=<mets:smLocatorLink xlink:href=\"#)(.*?)(?=\" xlink:label=\")", all = TRUE)
type =
tibble::tribble(~id,
id)
}
```


```{r}
 get_page = function(alto){
    page = alto %>%  read_file() %>%
      str_split("\n", simplify = TRUE) %>%
      keep(str_detect(., "CONTENT|<TextBlock ID=")) %>%
      str_extract("(?<=CONTENT=\")(.*?)(?=WC)|(?<=<TextBlock ID=)(.*?)(?= HPOS=)")%>%
      discard(is.na) %>%
      as.tibble() %>%
      mutate(pa = ifelse(str_detect(value, "pa[0-9]{7}"),
                         str_extract(value, "pa[0-9]{7}"), NA)) %>%
      fill(pa) %>%
      filter(str_detect(pa, "pa[0-9]{7}")) %>%
      filter(!str_detect(value, "pa[0-9]{7}")) %>%
      filter(!str_detect(value, "SUBS_TYPE=HypPart1 SUBS_CONTENT=")) %>% mutate(value =
                                                                                  str_remove_all(value, "STYLE=\"subscript\" ")) %>% mutate(value = str_remove_all(value, "\"")) %>%
      mutate(value = str_remove_all(value, "SUBS_TYPE=HypPart1 SUBS_CONTENT=")) %>%
      mutate(value = str_remove_all(value, "SUBS_TYPE=HypPart2 SUBS_CONTENT="))

  }
```


```{r}
make_articles_sample <- function(foldername){

  metsfilename =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), ".*mets.xml") %>%
    discard(is.na)
 


  metsfile = read_file(metsfilename)

  page_list =  str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), ".*[0-9]{4}.xml") %>%
    discard(is.na)



  metspagegroups = metsfile %>% str_split("<mets:smLinkGrp>")%>%
    flatten_chr() %>%
    as_tibble() %>% filter(str_detect(value, '#art[0-9]{4}')) %>% mutate(articleid = str_extract(value,"[0-9]{4}"))

  future_map(page_list, get_page) %>%
    bind_rows()  %>% left_join(extract_page_groups(metspagegroups$value) %>%
                                 unnest() %>%
                                 mutate(art = ifelse(str_detect(id, "art"), str_extract(id, "[0-9]{4}"), NA)) %>%
                                 fill(art) %>% filter(!str_detect(id, "art[0-9]{4}")), by = c('pa' = 'id')) %>%
    group_by(art) %>%
    summarise(text = paste0(value, collapse = ' ')) %>%
    mutate(issue_name = metsfilename) %>%
    separate(issue_name, into = c("a","b","c","d","e","f","g","h","i","j","k","l"), .keep_all = TRUE, sep = "/") %>%
    select(art_code = art, text, nlp = j, year = k, date = l)
    



}
```


```{r}
%>% 
    mutate(nlp_date = str_extract(issue_name, "/[0-9]{7}_[0-9]{8}_$")) %>% 
           separate(nlp_date, into = c('nlp', 'date'), .keep_all = TRUE, sep = "_")
    
```


```{r}
pickSample = function(df, NLP = "0002194", yyyy = "[0-9]{4}", mmdd = "[0-9]{4}", sample = 1){

  df %>% filter(str_detect(value, paste0("/",NLP,"/",yyyy,"/",mmdd))) %>% sample_n(sample) %>% pull(value)


}
```


```{r}
extract_news_csv = function(path, yearpick = "[0-9]{4}", mmddpick = "[0-9]{4}", nlppick = "0002194",samplepick = 1){

  folderslist = hmd_file_index %>% pickSample(yyyy = yearpick, mmdd = mmddpick,NLP = nlppick, sample = samplepick)

  future_map(folderslist, make_articles_sample)

}
```


```{r}
yearpick = "1850"
mmddpick = "[0-9]{4}"
nlppick = "0002194"
samplepick = 10
```


```{r}
folderslist = hmd_file_index %>% pickSample(yyyy = yearpick, mmdd = mmddpick,NLP = nlppick, sample = samplepick)
```


```{r}
future_map(folderslist, make_articles_sample)
```


```{r}
df = extract_news_csv(samplepick = 100)
```

```{r}
df = extract_news_csv(samplepick = 100, nlppick = "0002194")
```
