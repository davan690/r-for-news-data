# OCR and its problems

## What is OCR?

## What is it like in BL newspapers?
In a word, terrible. Well, it varies a lot, and the truth is, nobody _really_ knows what it's like, because that would involve having large sets of very accurate, manually transcribed newspapers, to compare to the OCR text. Subjectively, we can probably make a few generalisations. 

It gets better as the software gets better, but not particularly quickly, because much of the quality is dependant on things to do with the physical form. 

Digitising from print is much better than from microfilm. But print can still be bad. 

Standard text is much better than non-standard. For example, different fonts, sizes, and so forth.

Advertisements seem to have particularly bad OCR - they are generally not in regular blocks of text, which the OCR software finds difficult, and they often used non-standard characters or fonts to stand out. 

The time dimension is not clear: type probably got better, but it also got smaller, more columns. 

Problems with the physical page have a huge effect: rips, tears, foxing, dark patches and so forth. Many errors are not because of the microfilm, digital image or software, and may not be fixable. 

What does this all mean? Well, it introduces bias, and probably in non-random ways, but in ways that have implications for our work. If things are digitised from a mix of print and microfilm, for example, we might get very different results for the print portion, which might easily be mis-attributed to a historical finding. Perhaps there were twice as many mentions of cheese in the 1850s than in the 1890s? It's probably best to rule out that this is not just because later newspapers had a difficult font, or they were digitised from microfilm instead of print. [@hill-ocr, @Cordell_2017, @Piotrowski_2012, @cordell-ocr, @evershed-ocr]

## What information do we have on OCR?

Can you add anything on difference between JISC and BNA 'confidence scores'? Do a comparison?

Make a sample, evenly spread across years?

### Error rate

The error rate reported by the software is a confidence score. The overall confidence score is an average of all these scores. It's not testing against anything but itself: it could have a high confidence score but be completely wrong - perhaps the computer has a misguided sense of its own accuracy? Or it could be crippled with self-doubt, and report very low scores whilst in fact getting everything correct. 

### Ground truth - testing against transcription

## Impact on analysis
It depends. Broad analysis still seems to work - keyword searches, for example, come up with broadly expected results. It might be more important in finer work, for example Natural Language Processing (NLP). NLP relies on 

[Why You (A Humanist) Should Care About Optical Character Recognition](https://ocr.northeastern.edu/report/)

Ryan Cordell's large report

Can you do a tutorial to extract OCR accuracy from the newspaper data

Do microfilm vs print, and time periods

Character Count: 6547 
Predicted Word Accuracy: 77.5% 
Suspicious Character Count: 481 
Word Count: 1098 
Suspicious Word Count: 432

First read the alto page, which should be an argument to the function. Here's one page to use as an example:
```{r}
library(tidyverse)
```

```{r}
alto = "0002644/1809/0101/0002644_18090101_0001.xml"

altofile = alto %>%  read_file()
```

Split the file on each new line:

```{r}
altofile = altofile %>%
        str_split("\n", simplify = TRUE)

altofile %>% glimpse()

```
```{r}
altofile[18]
```
```{r}
filelist = list.files("0002644/1809/", full.names = TRUE, recursive = TRUE, pattern = "[0-9]{4}\\.xml")[-1]
```

```{r}
library(data.table)
```

```{r}

  y = NULL

tic = Sys.time()
for(file in filelist){
  
  suschar = file  %>%  read_file() %>%
        str_split("\n") 
  
  y = rbindlist(list(y, data.table(suschar[18], file)))
  
}

toc = Sys.time()

elapsed = toc-tic
print(elapsed)
```

```{r}
library(lubridate)
```


```{r}
y %>% separate(suschar.18., into = c('text', 'percent'), sep = ':') %>% 
  mutate(percent = as.numeric(str_remove(percent, '%'))) %>% 
  mutate(file = str_remove(file, "0002644\\/[0-9]{4}\\//[0-9]{4}\\/")) %>%
  separate(file, into = c('nlp','date', 'page')) %>% 
  mutate(date = ymd(date)) %>% 
  ggplot() + 
  geom_point(aes(x = date, y = percent, color = percent, shape = page)) + theme(legend.position = 'bottom')
```

