# Data

It's all very well being able to access the content, but for the purposes of the kind of things we'd like to do, access to the data is needed. 

Lancaster Newsbook Corpus

## JISC Newspaper digitisation projects
Maybe a separate chapter?

Most of the projects in the UK which have used newspaper data hvae been using the _British Libraryâ€™s 19th Century Newspapers_ collection. This is an interesting collection of content and worth outlining in some detail. Knowing the sources, geographical makeup and motivation behind the titles in the collection can be really helpful in thinking about what is and is not in the sources. 

The JISC newspaper digitisation program began in 2004, when The British Library received two million pounds from the Joint Information Systems Committee (JISC) to for some digitisation projects. A plan was made to digitise up to two million pages, across 49 titles (reference https://www.gale.com/intl/essays/ed-king-digitisation-of-british-newspapers-1800-1900). A second phase of the project digitised a further 22 titles (figure \@ref(fig:jisc-map)).[@shaw-newspapers; @shaw-billion for a good brief overview to the selection process for JISC 1]

>The forty-eight titles chosen represent a very large cross-section of 19th century press and
publishing history. Three principles guided the work of the selection panel: firstly, that
newspapers from all over the UK would be represented in the database; in practice, this
meant selecting a significant regional or city title, from a large number of potential
candidate titles. Secondly, the whole of the nineteenth century would be covered; and
thirdly, that, once a newspaper title was selected, all of the issues available at the British
Library would be digitised. To maximise content, only the last timed edition was
digitised. No variant editions were included. Thirdly, once a newspaper was selected, all
of its run of issue would be digitised.[@ed-king-digi]


Jane Shaw wrote, in 2007: 

>The academic panel made their selection using the following eligibility criteria:

>To ensure that complete runs of newspapers are scanned
>To have the most complete date range, 1800-1900, covered by the titles selected
To have the greatest UK-wide coverage as possible
To include the specialist area of Chartism (many of which are short runs)
To consider the coverage of the title: e.g., the London area; a large urban area (e.g., Birmingham); a larger regional/rural area
To consider the numbers printed - a large circulation
The paper was successful in its time via its sales
To consider the different editions for dailies and weeklies and their importance for article inclusion or exclusion
To consider special content, e.g., the newspaper espoused a certain political viewpoint (radical/conservative)
The paper was influential via its editorials. [@shaw-newspapers]

What's really clear, is that the selection was driven by assumed historical need by the Library's users, plus some practicalities around copyright, microfilm and 

The result was a heavily curated collection, albeit with decent academic rigour and good intentions and like all collections created in this way, it is subject, quite rightly, to a lot of scrutiny by historians.

[list the titles]


```{r include=FALSE}
library(rgeos)
library(rgdal)
library(tidyverse)
library(snakecase)
libraryPalette = c("#DA2F65","#FFC82E","#00788B","#CEE055",   "#7E3E98",  "#1E6EB8", "#018074", "#865BE7", "#D44202")

load('data/shp_df_all_uk_1891.dms')
counties = read_csv('data/county_data.csv')
jisc = read_csv('data/jisc.csv')

colnames(counties) = to_snake_case(colnames(counties))

title_list = read_csv('data/BritishAndIrishNewspapersTitleList_20191118.csv')

```

```{r jisc-map, echo=FALSE, fig.cap="JISC 1 & 2 Digitisation Programmes", message=FALSE, warning=FALSE}
title_list %>% mutate(title_id = str_pad(title_id, width = 9, pad = '0')) %>% 
  left_join(jisc) %>% filter(!is.na(JISC)) %>% 
  left_join(counties) %>% 
  distinct(title_id, .keep_all = TRUE) %>% group_by(county_data) %>% 
  tally() %>% 
  right_join(shp_df_all_uk_1891, by = c("county_data" = 'id')) %>% 
  mutate(n = coalesce(n.x, n.y)) %>% 
  ggplot() + 
  geom_polygon(color = 'black', lwd = .1, aes(x = long, y = lat, fill = n.x, group = group)) + coord_fixed(1) + 
  scale_fill_gradient(low = "#DA2F65", high ="#FFC82E",na.value="gray90") + theme_void() + theme(legend.direction = 'horizontal', legend.position = 'left') + 
  guides(fill = guide_legend(title = 'Total Titles:'))
```


[draw a map of the JISC titles]

This is all covered in lots of detail elsewhere, including some really interesting critiques of the access and so forth.[@smits_making_2016; @mussell-elemental both include some discussion and critique of the British Library Newspaper Collection] But the overall makeup of it is clear, and this was a very specifically curated collection, though it was also influenced by contingency, in that it used microfilm (sometimes new microfilm). But overall, one might say that the collection has specific historical relevant, and was in ways representative. It does, though, only represent a tiny fraction of the newspaper collection, and by being relevant and restricted to 'important' titles, it does of course miss other voices. For example, much of the Library's collection consists of short runs, and much of it has not been microfilmed, which means it won't have been selected for digitisation. This means that 2019 digitisation selection policies are indirectly _greatly_ influenced by microfilm selection policies of the 70s, 80s, and 90s. Subsequent digitisation projects are trying to rectify these motivations, but again, it's good to keep in mind the 



Currently researchers access this either through Gale, or through the British Library as an external researcher. Many researchers have requested access to the collection through Gale, which they will apparently do in exchange for a fee for the costs of the hard drives and presumably some labour time. The specifics of the XML used, and some code for extracting the data, are available in the following chapter.

Some researchers have also got access to the collection through 

## GALE

## HMD data- on repository

## Luxembourg data


