# Introduction


## What is newspaper data?

Difference between content and data? It's arbitrary, all of the digitised content could be considered data. Perhaps it takes on that name when we do certain acts on it? For example the images become data 

Get a nice image for here

## Why is it useful?
There is a _lot_ of newspaper data available now for historical researchers. Across the globe, Heritage Organisations are digitising their collections. Most news digitisation projects do OCR and zoning, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It's far from perfect - we'll show some examples in a later chapter - but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure.  Once you get hold of this data, the rewards can be huge: looking just at English-language users in the last few years, researchers have used it to understand Victorian jokes, trace the movement of information and ideas, understand the effects of industrialisation, track the meetings of radical groups, and of course understand more about the newspapers themselves and the print culture that surrounds them.

While there has been a lot digitised, their is much, much more still to be done. The collection, in any country, is far from representative. But we must work with what we've got. The new histories of the press will be written by looking at text at scale, drawing broad conclusions, understanding genre, authorship and so forth through data analysis. 

We're just at the beginning: in the last few years projects have been using neural networks methods to improve the kinds of things we can do: the Living with machines project, for example, or several projects at the Royal Library in the Netherlands. The methods I describe here are simplistic, but they can still add to our understanding. 

[//]: # Change these out for real plots, using title list and shapefile - need to make a UK-Ireland shapefile!

[//] # Data - adds to statistics, gives another point of view. Another way of thinking about historical fact?

[//] # 

```{r echo=FALSE, fig.show = "hold", out.width = "50%", fig.cap="The portion of British Library Digitised Newspaper Content, and the physical collection, mapped", message=FALSE, warning=FALSE, fig.align = "default"}
knitr::include_graphics(c("images/online-circles.png", "images/title_holdings.png"))                      
```

## Short history of newspapers, newspaper digitisation?

### Burney and EEBO? 

[//]: # (check order)

### JISC 1
### JISC 2
### BNA

## Goals
Hopefully, by the end of this book, you will have:

* Know what newspaper data is available, in what format, across a variety of countries and languages.
* Understand something of the various XML formats which make up most newspaper data sources
* Have been introduced to a number of tools which are particularly useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use. Including some specific to news, such as the R library _Newsflow_.
* Understand how the tools can be used to answer some basic historical questions (if not provide any answers)

Historians have used newspaper data to do x and y. Newspapers have long bee thought of as a proxy for public opinion, a historical source or 'first draft of history', used to study the movement and genesis of knowledge and information, help to understand the mix of private and public, how power can be advanced through news, and so forth. Some people might not even be interested in the news in its own right: how it was created, distributed, shaped, what the political pressures were, how it was read at different times, and how it came to shape those who read it.

What do you need in advance?
Probably a mix of coding skills etc. 

## Format of the book - bookdown and github
Will try and explain as much as possible, but will take shortcuts
Not a programming expert, so it may not be optimised, the best of way doing things. It's just the way I've found that works for me.
Part of the reason for doing this is because writing in bookdown and GitHub is quite geeky fun. I will unashamedly do things because I _can_, on occasion.   

Bookdown allows for code and figures directly in the text. I'll make a very simple dataframe and plot it.

First load a couple of packages:

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

Make a dataframe:

```{r}
a <- c(10,20,30,40)
b <- c('book', 'pen', 'textbook', 'pencil_case')
c <- c(TRUE,FALSE,TRUE,FALSE)
d <- c(2.5, 8, 10, 7)
df <- data.frame(a,b,c,d)
```

Plot it:

```{r fig.cap="plotting example"}
df %>% ggplot() + geom_point(aes(x = a, y = d, color = b), size = 5)
```


There are bits of Python throughout, where I've only managed to work something out using that language. This is done using ```Reticulate```, which allows for Python within R markdown. I hope I'll be able to revise at some stage and do everything through one language.

How did I write the book? In R-studio, bookdown and bibdesk
There are a million things I could and would like to explain detail.

## Why R?
Used to be idiosyncratic, is becoming very widely used by data scientist, digital humanities, social scientists. 
A lot of this is because of developers like Hadley Wickham and R studio - the tidyverse, but also data.table.

### Who uses R?

### The Tidyverse

Historians using the language: Sharon Howard, work on criminal tattoos. Bruno Rodrigues

Writing a book with such a specific goal is a bit of a weird proposition

## Sources

### Country by country

## Methods

### Network analysis of seventeenth century

### Mapping

### Geocoding

## Text mining

## Format of the book

The rest of this book is divided into three sections: 
* Data, which goes through the availability of various news datasets, for the moment with a heavy emphasis on the UK
* Methods, which talks through some of the techniques in a broad sense
* Examples, which uses the techniques above to actually do some stuff.

