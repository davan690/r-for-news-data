# Introduction

## Unlocking the past with newspaper data

Write an example here. Start with metadata, find a bunch of newspapers you're interested in. From this make a corpus. Map, plot, text mine, make an interactive visualisation. Understand patterns of text reuse, or use relative frequencies of keywords to see when they were emphasised.

The historian now has a lot of newspaper data at her disposal. She might 

She goes online, is interested in the reception of news about the Crimean War in Manchester. But where to start? Well, first she narrows down the list of newspapers to consult using an interactive map. For this research, she'll look at everything published in Manchester between 1853 and 1857. But also, she's interested in a more fine-grained group of titles: this will specifically be a study of the established press: so those that lasted a long time, at least fifty years in total. And while she's at it, she'll specify that she's only interested in the Liberal press, using an enriched database of political leanings.

List of titles in hand, she goes the BL Open Repository and finds the openly accessible ones, and downloads the relevant files, in .xml format. From here she extracts the text, article by article, into a single corpus. She makes a list of search terms which she thinks will help to narrow things down, and, using this, restricts her new corpus to articles about the Crimean war, as best as possible. 

First she looks at the most frequent terms: the usual suspects are there, of course - but once she filters out the 'stop words' she sees some potentially patterns, and notes down some words to dig into later. Giving the top ten per month is also interesting, and shows a shift from words relating to the diplomacy of the conflict, to more potentially more 'emotive' language, describing individual battles. 

Next she creates a 20 topic model to see if there is any difference between the types of articles in her corpus, which shows a few 'themes' under which the articles can be grouped together: one with words like _steamer_, _navy_, _Sebastopol_ as its most important words is an unusual grouping, and might be worth exploring. 

Using sentiment analysis the historian of the Crimean war notices a shift towards reporting with more negative words, and a cluster of particularly negative articles in late 1854: when the reports of the failed military action during the Battle of Baklava started to trickle through: an event which was immortalised only weeks later in Tennyson's narrative poem _The Charge of the Light Brigade_. Not surprising, perhaps, but a reassuring confirmation.

How were these titles sharing information? Using techniques to find overlapping shared text across multiple documents, she works out that the flow of information moved from the established dailies to the weekly titles. 

This is not too far into the future: we're starting to make data openly available. The tools, which only a few years ago were restricted to experts, are now unimaginably easier to use. 

Things have moved on from the first generation of 'how-to' guides for digital humanities students: it's now fairly reasonable to pick a language, probably R or Python, and do all analysis, writing, documentation and so forth without ever leaving the safety of its ecosystem. These modern languages have a huge number of packages for doing all sorts of interesting analysis of text, and even authoring entire books.

On the other hand, the promise of scraping the web for open data, while still with its place, has in many ways been superseded. The historian looking to use newspaper data must wrestle with closed systems, proprietary formats and so forth. The sheer quantity of newspaper data, and its commercial roots (and perhaps new commercial future), mean that it has not been treated in the same way as many other historical datasets. Newspaper data has, up until recently, had several financial, legal, bureaucratic and technical hurdles. 

The promise of newspaper data is great, but the practicalities are complex, and can be offputting for the non-specialist. This book hopes to simplify some of these techniques, demystify the formats, and provide a way in for those who would like to dip their toe in the waters of newspaper data. 

## What is newspaper data?

Difference between content and data? It's arbitrary, all of the digitised content could be considered data. Perhaps it takes on that name when we do certain acts on it? For example the images become data 

Get a nice image for here <could use southern right whale image?>

## Why is it useful?

There is a _lot_ of newspaper data available now for historical researchers. Across the globe, Heritage Organisations are digitising their collections. Most news digitisation projects do OCR and zoning, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It's far from perfect - we'll show some examples in a later chapter - but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure.  Once you get hold of this data, the rewards can be huge: looking just at English-language users in the last few years, researchers have used it to understand Victorian jokes, trace the movement of information and ideas, understand the effects of industrialisation, track the meetings of radical groups, and of course understand more about the newspapers themselves and the print culture that surrounds them.

While there has been a lot digitised, their is much, much more still to be done. The collection, in any country, is far from representative. But we must work with what we've got. The new histories of the press will be written by looking at text at scale, drawing broad conclusions, understanding genre, authorship and so forth through data analysis. 

We're just at the beginning: in the last few years projects have been using neural networks methods to improve the kinds of things we can do: the Living with machines project, for example, or several projects at the Royal Library in the Netherlands. The methods I describe here are simplistic, but they can still add to our understanding. 

[//]: # Change these out for real plots, using title list and shapefile - need to make a UK-Ireland shapefile!

[//] # Data - adds to statistics, gives another point of view. Another way of thinking about historical fact?

[//] # 

```{r echo=FALSE, fig.show = "hold", out.width = "50%", fig.cap="The portion of British Library Digitised Newspaper Content, and the physical collection, mapped", message=FALSE, warning=FALSE, fig.align = "default"}
knitr::include_graphics(c("images/online-circles.png", "images/title_holdings.png"))                      
```

## Short history of newspapers, newspaper digitisation?

### Burney and EEBO? 

[//]: # (check order)

### JISC 1
### JISC 2
### BNA

## Goals
Hopefully, by the end of this book, you will have:

* Know what newspaper data is available, in what format, across a variety of countries and languages.
* Understand something of the various XML formats which make up most newspaper data sources
* Have been introduced to a number of tools which are particularly useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use. Including some specific to news, such as the R library _Newsflow_.
* Understand how the tools can be used to answer some basic historical questions (whether they provide answers, I'll leave for the reader and historian to decide)

Historians have used newspaper data to do x and y.[@Hills:2019aa] Newspapers have long been thought of as a proxy for public opinion, a historical source or 'first draft of history', used to study the movement and genesis of knowledge and information, help to understand the mix of private and public, how power can be advanced through news, and so forth. Information, and access to it, was key to understanding the movements and processes that shaped early modern and Victorian society. Understanding newspapers at scale can help with that.

ome people might not even be interested in the news in its own right: how it was created, distributed, shaped, what the political pressures were, how it was read at different times, and how it came to shape those who read it.

What do you need in advance?
Probably a mix of coding skills etc. 

## Format of the book - bookdown and github
Will try and explain as much as possible, but will take shortcuts
Not a programming expert, so it may not be optimised, the best of way doing things. It's just the way I've found that works for me.
Part of the reason for doing this is because writing in bookdown and GitHub is quite geeky fun. I will unashamedly do things because I _can_, on occasion.   

Bookdown allows for code and figures directly in the text. I'll make a very simple dataframe and plot it.

First load a couple of packages:

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

Make a dataframe:

```{r}
a <- c(10,20,30,40)
b <- c('book', 'pen', 'textbook', 'pencil_case')
c <- c(TRUE,FALSE,TRUE,FALSE)
d <- c(2.5, 8, 10, 7)
df <- data.frame(a,b,c,d)
```

Plot it:

```{r fig.cap="plotting example"}
df %>% ggplot() + geom_point(aes(x = a, y = d, color = b), size = 5)
```


There are bits of Python throughout, where I've only managed to work something out using that language. This is done using ```Reticulate```, which allows for Python within R markdown. I hope I'll be able to revise at some stage and do everything through one language.

How did I write the book? In R-studio, bookdown and bibdesk
There are a million things I could and would like to explain detail.

There are a couple of things you'll need in order to build a copy of the book yourself. This may not be important to you. One thing that needs to be done is entering a bing API key for the relevant chapter. The book won't complete without this. 

## Why R?
This is a book made _with_ R, and _using_ R, but it is not really _about_ R. I wanted to show some techniques. It should be readable by anyone, but it's possible bits will not be so easy to follow.
Used to be idiosyncratic, is becoming very widely used by data scientist, digital humanities, social scientists. 
A lot of this is because of developers like Hadley Wickham and R studio - the tidyverse, but also data.table.

CRAN, packages and environments much easier to use. I'll never be a full-time coder, so not worried in learning a 'proper' language. R works. 

Have a look at the list http://dh-r.lincolnmullen.com here, and also Hadley Wickham's R for data science.

[Why I use R](https://blog.shotwell.ca/posts/why_i_use_r/)

### Who uses R?

### The Tidyverse

Historians using the language: Sharon Howard, work on criminal tattoos. Bruno Rodrigues

Writing a book with such a specific goal is a bit of a weird proposition

## Who is the book for?
Historians looking to understand news at scale. Undergraduates and postgrads looking to dip their feet into computational methods.

Imagine a PhD student, hasn't used newspaper data before. What is available? How can she access it? Does she need to visit the library, or will she be happy with what's available online? 

## Format of the book

The rest of this book is divided into three sections: 
* Data, which goes through the availability of various news datasets, for the moment with a heavy emphasis on the UK
* Methods, which talks through some of the techniques in a broad sense
* Examples, which uses the techniques above to actually do some stuff - these are like mini research projects, intending to show how R can help with newspapers and also show something interesting. 

The book doesn't need to be read in its entirety: if you're just interested in finding newspaper data, specifically that collected by the British Library, you could stick with the first part. You might be a much better programmer than me and just interested in the scope of the Library's datasets for some analysis - for instance, it might be useful to look at the chapters on the BNA and JISC digitisation projects, to get a handle on how representative the data you have is. You might have arrived at the Library to do some research and not know where to begin with our folder structure and so forth. As much of that information as possible has been included.



