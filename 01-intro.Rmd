# Introduction

There is a _lot_ of newspaper data available now for historical researchers. Across the globe, Heritage Organisations are digitising their collections. Most news digitisation projects do OCR and zoning, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It's far from perfect - we'll show some examples in a later chapter - but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure.  Once you get hold of this data, the rewards can be huge: looking just at English-language users in the last few years, researchers have used it to understand Victorian jokes, trace the movement of information and ideas, understand the effects of industrialisation, track the meetings of radical groups, and of course understand more about the newspapers themselves and the print culture that surrounds them.

While there has been a lot digitised, their is much, much more still to be done. The collection, in any country, is far from representative. But we must work with what we've got.

This text is usually in one of a variety of XML formats.

## Goals
Hopefully, by the end of this book, you will have:

* Know what newspaper data is available, in what format, across a variety of countries and languages.
* Understand something of the various XML formats which make up most newspaper data sources
* Have been introduced to a number of tools which are particulalry useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use. Including some specific to news, such as the R library _Newsflow_.
* Understand how the tools can be used to answer some basic historical questions (if not provide any answers)

Historians have used newspaper data to do x and y. It can be used as a proxy for public opinion, used to study the movement and genesis of knowledge and information, help to understand the mix of private and public, how power can be advanced through news, and so forth. Some people might not even be interested in the news in its own right: 

What do you need in advance?
Probably a mix of coding skills etc. 

## Format of the book - bookdown and github
Will try and explain as much as possible, but will take shortcuts
Not a programming expert, so it may not be optimised, the best of way doing things. It's just the way I've found that works for me.

Bookdown allows for code and figures directly in the text. I'll make a very simple dataframe and plot it.

First load a couple of packages:

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

Make a dataframe:

```{r}
a <- c(10,20,30,40)
b <- c('book', 'pen', 'textbook', 'pencil_case')
c <- c(TRUE,FALSE,TRUE,FALSE)
d <- c(2.5, 8, 10, 7)
df <- data.frame(a,b,c,d)
```

Plot it:

```{r fig.cap="plotting example"}
df %>% ggplot() + geom_point(aes(x = a, y = d, color = b), size = 5)
```


There are bits of Python throughout, where I've only managed to work something out using that language. This is done using ```Reticulate```, which allows for Python within R markdown. I hope I'll be able to revise at some stage and do everything through one language.

How did I write the book? In R-studio, bookdown and bibdesk

## Why R?
Used to be idiosyncratic, is becoming very widely used by data scientist, digital humanities, social scientists. 
A lot of this is because of developers like Hadley Wickham and R studio - the tidyverse, but also data.table.

### Who uses R?

### The Tidyverse

Historians using the language: Sharon Howard, 

Writing a book with such a specific goal is a bit of a weird proposition

## Sources
### Country by country

## Methods
### Network analysis of seventeenth century

### Mapping

### Geocoding

## Text mining