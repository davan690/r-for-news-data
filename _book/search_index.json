[
["index.html", "Demystifying Newspaper Data with R (and a tiny bit of Python) 1 Preface &amp; Acknowledgements", " Demystifying Newspaper Data with R (and a tiny bit of Python) Yann Ryan 2020-02-04 1 Preface &amp; Acknowledgements I spent the last year or so working at the British Library as a Curator of Newspaper Data. It was an experimental position, conceived on the premise that the data generated by newspaper digitisation projects, in this case the Heritage Made Digital project, deserves its own curatorial role and set of practices. It was an exciting time to work on such a project while ‘big data’ analysis using newspaper content, by historians, is no longer in its infancy, it is firmly in a soul-searching adolescence, and it’s been exciting working on an evolving discipline, one which is developing its own set of practices, understanding where and how its bias works and the implications this might have, and generally moving from a sense of naive optimism to a practice which is more academically and mathematically rigorous and self-reflective. This book aims to give basic handbook for those who would like to dip their toes into big data analysis but don’t know where to begin. "],
["introduction.html", "2 Introduction 2.1 Unlocking the past with newspaper data 2.2 Why is it useful? 2.3 Goals 2.4 Why R? 2.5 Who is the book for? 2.6 Format of the book", " 2 Introduction 2.1 Unlocking the past with newspaper data Imagine a historian, similar to that described in the Historian’s Macroscope, sits down to do some research. She is interested in the reception of news about the Crimean War, in Manchester. But where to start? Well, first she narrows down the list of newspapers to consult using an interactive map. For this research, she’ll look at everything published in Manchester between 1853 and 1857. But also, she’s interested in a more fine-grained group of titles: this will specifically be a study of the established press: so those that lasted a long time, at least fifty years in total. And while she’s at it, she’ll specify that she’s only interested in the Liberal press, using an enriched database of political leanings. List of titles in hand, she goes the BL Open Repository and finds the openly accessible ones, and downloads the relevant files, in .xml format. From here she extracts the text, article by article, into a single corpus. She makes a list of search terms which she thinks will help to narrow things down, and, using this, restricts her new corpus to articles about the Crimean war, as best as possible. First she looks at the most frequent terms: the usual suspects are there, of course - but once she filters out the ‘stop words’ she sees some potentially patterns, and notes down some words to dig into later. Giving the top ten per month is also interesting, and shows a shift from words relating to the diplomacy of the conflict, to more potentially more ‘emotive’ language, describing individual battles. Next she creates a 20 topic model to see if there is any difference between the types of articles in her corpus, which shows a few ‘themes’ under which the articles can be grouped together: one with words like steamer, navy, Sebastopol as its most important words is an unusual grouping, and might be worth exploring. Using sentiment analysis the historian of the Crimean war notices a shift towards reporting with more negative words, and a cluster of particularly negative articles in late 1854: when the reports of the failed military action during the Battle of Baklava started to trickle through: an event which was immortalised only weeks later in Tennyson’s narrative poem The Charge of the Light Brigade. Not surprising, perhaps, but a reassuring confirmation. How were these titles sharing information? Using techniques to find overlapping shared text across multiple documents, she works out that the flow of information moved from the established dailies to the weekly titles. This is not too far into the future: we’re starting to make data openly available. The tools, which only a few years ago were restricted to experts, are now unimaginably easier to use. Things have moved on from the first generation of ‘how-to’ guides for digital humanities students: it’s now fairly reasonable to pick a language, probably R or Python, and do all analysis, writing, documentation and so forth without ever leaving the safety of its ecosystem. These modern languages have a huge number of packages for doing all sorts of interesting analysis of text, and even authoring entire books. On the other hand, the promise of scraping the web for open data, while still with its place, has in many ways been superseded. The historian looking to use newspaper data must wrestle with closed systems, proprietary formats and so forth. The sheer quantity of newspaper data, and its commercial roots (and perhaps new commercial future), mean that it has not been treated in the same way as many other historical datasets. Newspaper data has, up until recently, had several financial, legal, bureaucratic and technical hurdles. 2.2 Why is it useful? There is a lot of newspaper data available now for historical researchers. Across the globe, the keepers of cultural memory are digitising their collections. Most news digitisation projects do OCR and zoning, meaning that the digitised images are processed so that the text is machine readable, and then divided into articles. It’s far from perfect - we’ll show some examples in a later chapter - but it does generate a large amount of data: both the digitised images, and the underlying text and information about the structure. Once you get hold of this data, the rewards can be huge: looking just at English-language users in the last few years, researchers have used it to understand Victorian jokes, trace the movement of information and ideas, understand the effects of industrialisation, track the meetings of radical groups, and of course understand more about the newspapers themselves and the print culture that surrounds them. While there has been a lot digitised, their is much, much more still to be done. The collection, in any country, is far from representative. But we must work with what we’ve got. The new histories of the press will be written by looking at text at scale, drawing broad conclusions, understanding genre, authorship and so forth through data analysis. We’re just at the beginning: in the last few years projects have been using neural networks methods to improve the kinds of things we can do: the Living with machines project, for example, or several projects at the Royal Library in the Netherlands. The methods I describe here are simplistic, but they can still add to our understanding. 2.3 Goals This short book hopes to help you: Know what British Library newspaper data is openly available, where it is, and where to look for more coming onstream. Understand something of the XML format which make up the Library’s current crop of openly available newspapers. Have a process for extracting the plain text of the newspaper in a format which is easy to use. Have been introduced to a number of tools which are particularly useful for large-scale text mining of huge corpora: n-gram counters, topic modelling, text re-use. Understand how the tools can be used to answer some basic historical questions (whether they provide answers, I’ll leave for the reader and historian to decide) 1 2.4 Why R? This is a handbook made with R, and using R, but it is not really primarily about R. I wanted to show some techniques. It should be readable by anyone, but it’s possible bits will not be so easy to follow. Used to be idiosyncratic, is becoming very widely used by data scientist, digital humanities, social scientists. A lot of this is because of developers like Hadley Wickham and R studio - the tidyverse, but also data.table. Have a look at the list http://dh-r.lincolnmullen.com here, and also Hadley Wickham’s R for data science. Why I use R 2.5 Who is the book for? Historians looking to understand news at scale. Undergraduates and postgrads looking to dip their feet into computational methods. Imagine a PhD student, hasn’t used newspaper data before. What is available? How can she access it? Does she need to visit the library, or will she be happy with what’s available online? 2.6 Format of the book The book doesn’t need to be read in its entirety: if you’re just interested in finding newspaper data, specifically that collected by the British Library, you could stick with the first part. You might be a much better programmer than me and just interested in the scope of the Library’s datasets for some analysis - for instance, it might be useful to look at the bits on the BNA and JISC digitisation projects, to get a handle on how representative the data you have is. You might have arrived at the Library to do some research and not know where to begin with our folder structure and so forth. As much of that information as possible has been included. Thomas T. Hills and others, ‘Historical Analysis of National Subjective Wellbeing Using Millions of Digitized Books’, Nature Human Behaviour, 3.12 (2019), 1271–5 &lt;https://doi.org/10.1038/s41562-019-0750-z&gt;.↩ "],
["uk-newspaper-data.html", "3 UK Newspaper Data 3.1 Intro to British Library Newspapers 3.2 Burney Collection 3.3 JISC Newspaper digitisation projects 3.4 BRITISH NEWSPAPER ARCHIVE 3.5 HMD data- on repository 3.6 What access do you need?", " 3 UK Newspaper Data 3.1 Intro to British Library Newspapers The British Library holds about 60 million issues, or 450 million pages of newspapers. These cover over 400 years of British and world events, but the collection has not alway been systematic. As Ed King has written: Systematic collection of newspapers at the British Museum (the precursor of the British Library) did not really begin until 1822. At that time publishers were obliged to supply copies of their newspapers to the Stamp Office so that they could be taxed. In 1822 it was agreed that these copies would be passed to the British Museum after a period of three years. From 1869 onwards newspapers were included in the legal deposit legislation and were then deposited directly at the British Museum. This systematic application of legal deposit requirements means that many thousands of complete runs of newspapers have accumulated. The majority of newspapers collected are those published since 1800.2 knitr::include_app(&quot;https://yannryan.shinyapps.io/newspapers_by_title/&quot;) Figure 3.1: An interactive map of the British Library’s physical newspaper collection. Links are to the catelogue entry. https://melissaterras.files.wordpress.com/2020/01/selectioncriterianewspapers_hauswedell_nyhan_beals_terras_bell-3.pdf What does this all mean for the data? First of all, it means that only a fraction of what was published has been preserved or collected, and only a fraction of that which has been collected has been digitised. Some very rough numbers: The ‘newspaper-year’ is a good standard unit. This is all the issues for one title, for one year. Its not perfect, because a newspaper-year of a weekly is worth the same as a newspaper-year for a daily. But it’s an easy unit to count. There’s currently about 40,000 newspaper-years on the British Newspaper Archive. The entire collection of British and Irish Newspapers is probably, at a guess, about 350,000 newspaper-years. It’s all very well being able to access the content, but for the purposes of the kind of things we’d like to do, access to the data is needed. The following are the main British Library digitised newspaper sources. 3.2 Burney Collection The Burney Collection contains about one million pages, from the very earliest newspapers in the seventeenth century to the beginning of the 19th, collected by Rev. Charles Burney in the 18th century, and purchased by the Library in 1818.3 It’s actually a mixture of both Burney’s own collction and stuff inserted since. It was microfilmed in its entirety in the 1970s. As Andrew Prescott has written, ‘our use of the digital resource is still profoundly shaped by the technology and limitations of the microfilm set’4 The collection was imaged in the 90s but because of technological restrictions it wasn’t until 2007 when, with Gale, the British Library released the Burney Collection as a digital resource. The accuracy of the OCR has been measured, and one report found that the ocr for the Burney newspapers offered character accuracy of 75.6% and word accuracy of 65%.5. 3.3 JISC Newspaper digitisation projects Most of the projects in the UK which have used newspaper data have been using the British Library’s 19th Century Newspapers collection. This is an interesting collection of content and worth outlining in some detail. Knowing the sources, geographical makeup and motivation behind the titles in the collection can be really helpful in thinking about its representativeness. Figure 3.2: Very approximate chart of JISC titles, assuming that we had complete runs for all. Counted by year rather than number of pages digitised. The JISC newspaper digitisation program began in 2004, when The British Library received two million pounds from the Joint Information Systems Committee (JISC) to complete a newspaper digitisation project. A plan was made to digitise up to two million pages, across 49 titles.6 A second phase of the project digitised a further 22 titles.7 The titles cover England, Scotland, Wales and Ireland, and it should be noted that the latter is underrepresented although it was obviously an integral part of the United Kingdom at the time of the publication of these newspapers - something that’s often overlooked in projects using the JISC data. They cover about 40 cities ??, and are spread across 24 counties within Great Britain ??, plus Dublin and Belfast. The forty-eight titles chosen represent a very large cross-section of 19th century press and publishing history. Three principles guided the work of the selection panel: firstly, that newspapers from all over the UK would be represented in the database; in practice, this meant selecting a significant regional or city title, from a large number of potential candidate titles. Secondly, the whole of the nineteenth century would be covered; and thirdly, that, once a newspaper title was selected, all of the issues available at the British Library would be digitised. To maximise content, only the last timed edition was digitised. No variant editions were included. Thirdly, once a newspaper was selected, all of its run of issue would be digitised.8 Jane Shaw wrote, in 2007: The academic panel made their selection using the following eligibility criteria: To ensure that complete runs of newspapers are scanned To have the most complete date range, 1800-1900, covered by the titles selected To have the greatest UK-wide coverage as possible To include the specialist area of Chartism (many of which are short runs) To consider the coverage of the title: e.g., the London area; a large urban area (e.g., Birmingham); a larger regional/rural area To consider the numbers printed - a large circulation The paper was successful in its time via its sales To consider the different editions for dailies and weeklies and their importance for article inclusion or exclusion To consider special content, e.g., the newspaper espoused a certain political viewpoint (radical/conservative) The paper was influential via its editorials.9 What’s really clear, is that the selection was driven by assumed historical need by the Library’s users, plus some practicalities around copyright, microfilm and The result was a heavily curated collection, albeit with decent academic rigour and good intentions and like all collections created in this way, it is subject, quite rightly, to a lot of scrutiny by historians.10 This is all covered in lots of detail elsewhere, including some really interesting critiques of the access and so forth.11 But the overall makeup of it is clear, and this was a very specifically curated collection, though it was also influenced by contingency, in that it used microfilm (sometimes new microfilm). But overall, one might say that the collection has specific historical relevant, and was in ways representative. It does, though, only represent a tiny fraction of the newspaper collection, and by being relevant and restricted to ‘important’ titles, it does of course miss other voices. For example, much of the Library’s collection consists of short runs, and much of it has not been microfilmed, which means it won’t have been selected for digitisation. This means that 2019 digitisation selection policies are indirectly greatly influenced by microfilm selection policies of the 70s, 80s, and 90s. Subsequent digitisation projects are trying to rectify these motivations, but again, it’s good to keep in mind the lat &lt;- 53.442788 lng &lt;- -2.244708 zoom &lt;- 5 pal1871 &lt;- colorBin(&quot;viridis&quot;, domain=leaflet_list$n, bins = c(0,5,10,15,20,25,30,35,Inf)) ## Warning in validateCoords(lng, lat, funcName): Data contains 1 rows with either ## missing or invalid lat/lon values and will be ignored ## Warning in is.na(values): is.na() applied to non-(list or vector) of type ## &#39;closure&#39; Currently researchers access this either through Gale, or through the British Library as an external researcher. Many researchers have requested access to the collection through Gale, which they will apparently do in exchange for a fee for the costs of the hard drives and presumably some labour time. The specifics of the XML used, and some code for extracting the data, are available in the following chapter. Some researchers have also got access to the collection through 3.4 BRITISH NEWSPAPER ARCHIVE Most of the British Library’s digitised newspaper collection is available on the British Newspaper Archive (BNA). The BNA is a commercial product run by a family history company called FindMyPast. FindMyPast is responsible for digitising large amounts of the Library’s newspapers, mostly through microfilm. As such, they have a very different focus to the JISC digitisation projects. The BNA is constantly growing, and it already dwarfs the JISC projects by number of pages: the BNA currently hosts about 33 million pages, against the 3 million or so of the two JISC projects 3.3 There are several important implications for this. First, most data-driven historical work carried out on newspapers has used the JISC data, rather than the BNA collection, because of relative ease-of-access. It’s an important point, as there may be an assumption that this kind of work is generally In addition, as it is a contantly evolving dataset, reproducibility is difficult. There are some exceptions: the Bristol N-Gram and named entity datasets used the FMP data, processing the top phrases and words from about 16 million pages. The collection has doubled in size since then: it’s likely that were it to be run again the results would be different. This is not only because of volume but also because of the change in focus and digitisation policy. Newspapers are selected for various reasons, but an underlying principle of coverage seems to be important: newspapers are obviously not selected at random, which necessarily results in a changing, evolving idea of bias. Figure 3.3: Newspaper-years on the British Newspaper Archive. Includes JISC content above. Figure 3.4: Titles on the British Newspaper Archive.12 3.5 HMD data- on repository https://blogs.bl.uk/thenewsroom/2019/01/heritage-made-digital-the-newspapers.html The Heritage Made Digital Project is a project within the British Library to digitise 19th century newspapers. It has a specific curatorial focus. It picked titles which are completely out of copyright, which means that they all finished publication before 1879. It also aimed to preserve: because of this, it chose titles which were not on microfilm, and were also in poor or unfit condition. Newspaper volumes in unfit condition cannot be called up by readers: this meant that if a volume is not microfilmed and is in this state, it can’t be read by anyone. quote Luke’s blog here The other curatorial goal was to focus on ‘national’ titles. In practice this meant choosing titles printed in London but without a specific London focus. This might seem like a strange selection policy considering the general view that stuff outside London is neglected, but it sort of makes sense: JISC focused on regional titles, then local, and all the ‘big’ nationals like the Times or the Manchester Guardian have been digitised but sit behind paywalls within those organisations. This means that a bunch of historically important titles may have fallen through the cracks, and this projects is digitising some of those.13 The important thing to note is that the is, like JISC, a selection made by historians for preservation and curatorial purposes. Again, it’s worth thinking about how this affects the bias. The good news is that as these newspapers are deemed out of copyright, the data can be made freely downloadable. Currently the first batch of newspapers, in METS/ALTO format, are available on the British LIbrary’s Open Repository. They have a CC-0 licence, which means they can be used for any purpose whatsoever. Much of the data examples in the following chapters will use this data. 3.6 What access do you need? 3.6.1 Want to find individual articles. Access through the BNA or Gale data, depending on access. BNA if you need the most coverage, and are interested particularly in local or regional titles. Gale has better search facilities but less content. ### Want to do some simple text mining, not so bothered about regional coverage Probably access HMD titles through the repository 3.6.2 Want to do text mining on a large corpus Request access to the Library’s newspapers through BL labs or elsewhere. Get access to raw files. Do you own analysis or else follow steps here to extract the data. There’s instructions for JISC and HMD titles. 3.6.3 Want to do text mining on the entire digitised collection You’ll need to speak to FMP, who run the British Newspaper Archive. They do take requests for access. There is a dataset of n-grams which has been used for text mining, sentiment analysis etc. and might be useful, though the collection has grown significantly since this point. 3.6.4 Want to do something involving the images, such as computer vision techniques, You’ll probably need to request access to newspapers through the British Library, or through Gale. Gale will send a copy of the JISC 1 &amp; 2 data (with OCR enhancements) on a hard drive to researchers, for a fee. Access through the Library will allow for image analysis but might be difficult to take away. The images up to 1878 are cleared for reuse. Ed King, ‘Digitisation of British Newspapers 1800-1900’, 2007 &lt;https://www.gale.com/intl/essays/ed-king-digitisation-of-british-newspapers-1800-1900&gt; [accessed 2007].↩ Andrew Prescott, ‘Travelling Chronicles: News and Newspapers from the Early Modern Period to the Eighteenth Century’, in, ed. by Siv Gøril Brandtzæg, Paul Goring, and Christine Watson (Leiden, The Netherlands: Brill, 2018), pp. 51–71.↩ Prescott.↩ Prescott.↩ King.↩ Jane Shaw, ‘Selection of Newspapers’, British Library Newspapers, 2007 &lt;https://www.gale.com/intl/essays/jane-shaw-selection-of-newspapers&gt;; Jane Shaw, ‘10 Billion Words: The British Library British Newspapers 1800-1900 Project: Some Guidelines for Large-Scale Newspaper Digitisation’, 2005 &lt;https://archive.ifla.org/IV/ifla71/papers/154e-Shaw.pdf&gt; for a good brief overview to the selection process for JISC 1.↩ Ed King, ‘British Library Digitisation: Access and Copyright’, 2008.↩ Shaw.↩ Paul Fyfe, ‘An Archaeology of Victorian Newspapers’, Victorian Periodicals Review, 49.4 (2016), 546–77 &lt;https://doi.org/10.1353/vpr.2016.0039&gt; for example.↩ Thomas Smits, ‘Making the News National: Using Digitized Newspapers to Study the Distribution of the Queen’s Speech by W. H. Smith &amp; Son, 1846–1858’, Victorian Periodicals Review, 49.4 (2016), 598–625 &lt;https://doi.org/10.1353/vpr.2016.0041&gt;; James Mussell, ‘Elemental Forms: Elemental Forms: The Newspaper as Popular Genre in the Nineteenth Century’, Media History, 20.1 (2014), 4–20 &lt;https://doi.org/10.1080/13688804.2014.880264&gt; both include some discussion and critique of the British Library Newspaper Collection.↩ data from https://www.britishnewspaperarchive.co.uk/titles/↩ The term national is debatable, but it’s been used to try and distinguish from titles which clearly had a focus on one region. Even this is difficult: regionals would have often had a national focus, and were in any case reprinting many national stories. But their audience would have been primarily in a limited geographical area, unlike a bunch of London-based titles, which were printed and sent out across the country, first by train, then the stories themselves by telegraph.↩ "],
["ocr-and-its-problems.html", "4 OCR and its problems 4.1 What is OCR? 4.2 What is it like in BL newspapers? 4.3 Introduction 4.4 Extract predicted word scores from the ALTO pages 4.5 Visualisations: 4.6 Highest and lowest results: 4.7 Page-by-page OCR visualisation 4.8 Microfilm vs print: 4.9 Conclusions 4.10 Impact on analysis", " 4 OCR and its problems 4.1 What is OCR? 4.2 What is it like in BL newspapers? This is a difficult question to answer, because it varies so much between projects, format and dates. The truth is, nobody really knows what it’s like, because that would involve having large sets of very accurate, manually transcribed newspapers, to compare to the OCR text. Subjectively, we can probably make a few generalisations. It gets better as the software gets better, but not particularly quickly, because much of the quality is dependant on things to do with the physical form. Digitising from print is much better than from microfilm. But print can still be bad. Standard text is much better than non-standard. For example, different fonts, sizes, and so forth. Advertisements seem to have particularly bad OCR - they are generally not in regular blocks of text, which the OCR software finds difficult, and they often used non-standard characters or fonts to stand out. The time dimension is not clear: type probably got better, but it also got smaller, more columns. Problems with the physical page have a huge effect: rips, tears, foxing, dark patches and so forth. Many errors are not because of the microfilm, digital image or software, and may not be fixable. What does this all mean? Well, it introduces bias, and probably in non-random ways, but in ways that have implications for our work. If things are digitised from a mix of print and microfilm, for example, we might get very different results for the print portion, which might easily be mis-attributed to a historical finding. Perhaps there were twice as many mentions of cheese in the 1850s than in the 1890s? It’s probably best to rule out that this is not just because later newspapers had a difficult font, or they were digitised from microfilm instead of print.14 4.3 Introduction The first two batches of digitised content received from Find My Past as part of the Heritage Made Digital Project consist of about 290,000 pages (see separate report). This short report evaluates the OCR quality (using one metric), comparing across time and formats. Conclusions are as expected: Processing from print results in significantly higher reported accuracy scores (though this may not reflect real-world OCR tests). The reported accuracy increases dramatically across time, particularly for print titles. The files returned from FMP contain a ‘predicted word accuracy score’ percentage for each page. These can be extracted and visualised, with some interesting conclusions. However it’s important to note these are not calculated by comparing actual results to the OCR, but rather use an internal algorithm. Some links worth reading to understand more about OCR and confidence scores: OCR software calculates a confidence level for each character it detects. Word and page confidence levels can be calculated from the character confidences using algorithms either inside the OCR software or as an external customised process. The OCR software doesn’t know whether any character is correct or not – it can only be confident or not confident that it is correct. It will give it a confidence level from 0-9. True accuracy, i.e., whether a character is actually correct, can only be determined by an independent arbiter, a human. This can be done by proofreading articles or pages, or by manually re-keying the entire article or page and comparing the output to the OCR output. These methods are very time consuming. (http://www.dlib.org/dlib/march09/holley/03holley.html) Because Abbyy Finereader is a commercial product, the software that predicts its accuracy is not freely available for inspection. As such, we should not make too much of the figure presented here, which certainly does not align with a human reader’s assessment of the page’s overall similarity to the words on the page images. (https://ryancordell.org/research/qijtb-the-raven-mla/) 4.4 Extract predicted word scores from the ALTO pages Generate Library colour scheme palettes: 4.5 Visualisations: 4.5.1 What’s in the data? The data includes 290,000 separate ALTO files, each representing one page. From the files, the ‘predicted word accuracy’ score has been extracted, and turned into a dataframe. The data contains about 117,000 files digitised from microfilm, and 173,000 digitised from print. This makes an interesting dataset to compare OCR quality scores across two different formats, by the same company at the same time. Comparison between pages: This visualisation shows pages on the y axis and time on the x axis. Each page is a separate ‘band’. Lighter colours (yellow) represent a higher reported score. Front pages have consistently lower scores than other pages. This is mostly because the front pages of 19th century newspapers contained mostly adverts, which OCR software finds difficult to process because of the variety in type and layout. This visualisation also shows the existence of multiple editions: dark lines on pages 9, 17 etc. are front pages of subsequent editions which have also been scanned under the same date. Points have been randomly spaced out for readability. Figure 4.1: OCR accuracy visualised by page, across the dataset. Lighter colours represent higher accuracy. Clear difference between the front and subsequent pages can be seen. 4.6 Highest and lowest results: The lowest results are all from the Lady’s Newspaper - this was an illustrated title and so the score is probably meaningless. knitr::kable(all_ocr %&gt;% arrange(accuracy) %&gt;% head(10)) accuracy nlp date page 15.8 0002254 1859-09-03 0005 16.5 0002254 1859-12-31 0019 16.5 0002254 1860-01-07 0019 16.5 0002254 1860-07-21 0013 16.5 0002254 1862-01-25 0013 16.5 0002254 1862-04-19 0013 16.6 0002254 1859-10-22 0019 16.7 0002254 1859-11-12 0012 16.7 0002254 1860-01-21 0019 16.7 0002254 1860-09-08 0012 The highest scores are blank pages: accuracy nlp date page 100 0002083 1853-10-15 0006 100 0002083 1853-10-26 0010 100 0002083 1853-11-09 0010 100 0002083 1853-11-14 0010 100 0002083 1853-11-15 0010 100 0002083 1853-11-16 0010 100 0002083 1853-11-17 0010 100 0002083 1853-11-19 0010 100 0002083 1853-11-21 0010 100 0002083 1853-11-23 0010 4.7 Page-by-page OCR visualisation Another way of looking at pages. Page one is consistently lowest predicted accuracy. The exception is a group of page 1 files in late 1860s: these were copies of the Sun and Central Press which were printed with two columns and large type, and without adverts on the first page. In general the predicted accuracy scores move upwards over time, and variation decreases. This is particularly clear in titles processed from print as the ntext Figure 4.2: Visualising OCR accuracy scores. Each dot represents a single page, positioned by date and reported accuracy. Pages are coloured by page position. Only the first four page positions are shown, for readability 4.8 Microfilm vs print: Approximately half of the data is from titles which were processed from microfilm, allowing a useful comparison between the scores of microfilm and print titles. The microfilm titles have, as expected, consistently lower accuracy, particularly the distribution. Particularly apparent is the difference in improvement over time: There’s no obvious increase in the scores of microfilm titles over time, but there is a significant change in print titles: from 1825 the predicted accuracy scores for print increase significantly, and the variation reduces noticeably. Figure 4.3: Microfilm vs Print: difference in the distribution and evolution of accuracy scores for titles digitised from both formats. Charting the average score (averaged over an entire year, so take with a pinch of salt) shows the different between microfilm and print more starkly: Figure 4.4: A broad view of improvement. Print titles show much more improvement in the assessed accuracy of the OCR over time 4.9 Conclusions This short report shows that the OCR accuracy -if the predicted word accuracy score included in the ALTO metadata is in any way a useful proxy - improves over time, and from 1825 onwards, the predicted scores for titles scanned from print are particularly high and consistent. Pages of advertising, as expected, show the lowest accuracy scores, and the scores are meaningless for illustrated titles. These reports could be generated for each batch going forward, and made available to researchers using the OCR for research. 4.10 Impact on analysis It depends. Broad analysis still seems to work - keyword searches, for example, come up with broadly expected results. It might be more important in finer work, for example Natural Language Processing (NLP). NLP relies on Why You (A Humanist) Should Care About Optical Character Recognition Mark John Hill and Simon Hengchen, ‘Quantifying the Impact of Dirty Ocr on Historical Text Analysis: Eighteenth Century Collections Online as a Case Study’, Digital Scholarship in the Humanities : DSH, 2019 &lt;https://doi.org/10.1093/llc/fqz024&gt;, @Cordell_2017, @Piotrowski_2012, @cordell-ocr, @evershed-ocr.↩ "],
["quick-introduction-to-r-and-the-tidyverse.html", "5 Quick introduction to R and the tidyverse 5.1 What and why? 5.2 Using R 5.3 Tidyverse", " 5 Quick introduction to R and the tidyverse The motivation behind this book was to provide a way to access and analyse newspaper data using a programming language that I am familiar with. The reason is simple: you write what you know, and I know R best. With its interface, R-Studio, I think it has the easiest transition from someone used to spreadsheet programs, and you’ll realise that most of what you do is filter, sort, count and select columns in a data format called a dataframe. 5.1 What and why? I think R and R-Studio have the easiest transition from someone used to Excel, and you’ll realise that most of what you do is filter, sort, count and select columns in a data format called a dataframe. A dataframe is basically a spreadsheet - it contains rows with observations, and columns with variables. Each row is generally a thing, for want of a better word. A thing that wants to be counted, either by summarising it as a more general thing, or turning it into something else and then counting it, or removing some of the things first and then counting the leftovers. For me, thing might be a record of a newspaper title, or a newspaper article (and its text), or it might be a single word. You can do a lot more interesting tasks with a thing in a dataframe. A thing might be a single polygon, in a huge dataframe of polygons or lines, all of which add up to a map, which we can then count, sort, filter and render as an image or even an interactive. 5.2 Using R 5.2.1 Base R commands I don’t use them very much, but R does have a bunch of very well-developed commands for doing the sorting, filtering and counting mentioned above. If you want to learn base R, I recommend the following: It is worth understanding the main types of data that you’ll come across, in your environment window. First, you’ll have dataframes. These are the spreadsheet-like objects which you’ll use in most analyses. They have rows and columns. Next are variables. A variable is assigned to a name, and then used for various purposes. You’ll often hear of an item called a vector. A vector is like a python list, if that means anything to you. A vector can be a single column in a dataframe (spreadsheet), which means they are used very often in R to manipulate data. A vector can have different types: for example, a character vector looks like this c(&quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;) A dataframe is just a bunch of vectors side by side. A vector is created with the command c(), with each item in the vector placed between the brackets, and followed by a comma. If your vector is a vector of words, the words need to be in inverted commas or quotation marks. fruit = c(&quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;, &quot;apples&quot;) colour = c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;) amount = c(2,5,10,8) You can create a dataframe using the data.frame() command. You just need to pass the function each of your vectors, which will become your columns. fruit_data = data.frame(fruit,colour,amount, stringsAsFactors = FALSE) Notice above that the third column, the amount, has under it instead of . That’s because R is treating it as a number, rather than a character. This means you can add them up and do all sorts of other mathy type things to them. All the items in a vector are coerced to the same type. So if you try to make a vector with a combination of numbers and strings, the numbers will be converted to strings. I wouldn’t worried too much about that for now. So for example if you create this vector, the numbers will get converted into strings. fruit = c(&quot;apples&quot;, 5, &quot;oranges&quot;, 3) fruit ## [1] &quot;apples&quot; &quot;5&quot; &quot;oranges&quot; &quot;3&quot; Anyway, that’s a dataframe. 5.3 Tidyverse Most of the work in these notebooks is done using a set of packages developed for R called the ‘tidyverse’. These enhance and improve a large range of R functions, with much nice syntax - and they’re faster too. It’s really a bunch of individual packages for sorting, filtering and plotting data frames. They can be divided into a number of diferent categories. All these functions work in the same way. The first argument is the thing you want to operate on. This is nearly always a data frame. After come other arguments, which are often specific columns, or certain variables you want to do something with. You installed the package in the last notebook. Make sure the library is loaded by running the following in an R chunk in a notebook: library(tidyverse) Here are a couple of the most important ones 5.3.1 select(), pull() select() allows you to select columns. You can use names or numbers to pick the columns, and you can use a - sign to select everything but a given column. Using the fruit data frame we created above: We can select just the fruit and colour columns: select(fruit_data, fruit, colour) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red Select everything but the colour column: select(fruit_data, -colour) ## fruit amount ## 1 apples 2 ## 2 bananas 5 ## 3 oranges 10 ## 4 apples 8 Select the first two columns: select(fruit_data, 1:2) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red 5.3.2 group_by(), tally(), summarise() The next group of functions group things together and count them. Sounds boring but you would be amazed by how much of data science just seems to be doing those two things in various combinations. group_by() puts rows with the same value in a column of your dataframe into a group. Once they’re in a group, you can count them or summarise them by another variable. First you need to create a new dataframe with the grouped fruit. grouped_fruit = group_by(fruit_data, fruit) Next we use tally(). This counts all the instances of each fruit group. tally(grouped_fruit) ## # A tibble: 3 x 2 ## fruit n ## &lt;chr&gt; &lt;int&gt; ## 1 apples 2 ## 2 bananas 1 ## 3 oranges 1 See? Now the apples are grouped together rather than being two separate rows, and there’s a new column called n, which contains the result of the count. If we specify that we want to count by something else, we can add that in as a ‘weight’, by adding wt = as an argument in the function. tally(grouped_fruit, wt = amount) ## # A tibble: 3 x 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 bananas 5 ## 3 oranges 10 That counts the amounts of each fruit, ignoring the colour. 5.3.3 filter() Another quite obviously useful function. This filters the dataframe based on a condition which you set within the function. The first argument is the data to be filtered. The second is a condition (or multiple condition). The function will return every row where that condition is true. Just red fruit: filter(fruit_data, colour == &#39;red&#39;) ## fruit colour amount ## 1 apples red 8 Just fruit with at least 5 pieces: filter(fruit_data, amount &gt;=5) ## fruit colour amount ## 1 bananas yellow 5 ## 2 oranges orange 10 ## 3 apples red 8 5.3.4 sort(), arrange(), top_n() Another useful set of functions, often you want to sort things. The function arrange() does this very nicely. You specify the data frame, and the variable you would like to sort by. arrange(fruit_data, amount) ## fruit colour amount ## 1 apples green 2 ## 2 bananas yellow 5 ## 3 apples red 8 ## 4 oranges orange 10 Sorting is ascending by default, but you can specify descending using desc(): arrange(fruit_data, desc(amount)) ## fruit colour amount ## 1 oranges orange 10 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 apples green 2 If you `sortarrange() by a list of characters, you’ll get alphabetical order: arrange(fruit_data, fruit) ## fruit colour amount ## 1 apples green 2 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 oranges orange 10 You can sort by multiple things: arrange(fruit_data, fruit, desc(amount)) ## fruit colour amount ## 1 apples red 8 ## 2 apples green 2 ## 3 bananas yellow 5 ## 4 oranges orange 10 Notice that now red apples are first. 5.3.5 left_join(), inner_join(), anti_join() 5.3.6 Piping Another great feature of the tidyverse is that you can ‘pipe’ commands through a bunch of functions. This means that you can do one operate, and pass the result to another operation. The previous dataframe is passed as the first argument of the next function by using the pipe %&gt;% command. It works like this: fruit_data %&gt;% filter(colour != &#39;yellow&#39;) %&gt;% # remove any yellow colour fruit group_by(fruit) %&gt;% # group the fruit by type tally(amount) %&gt;% # count each group arrange(desc(n)) # arrange in descending order of the count ## # A tibble: 2 x 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 oranges 10 That code block, written in prose: “take fruit data, remove any yellow colour fruit, count the fruits by type and amount, and arrange in descending order of the total” 5.3.7 Plotting using ggplot() The tidyverse includes a pretty great plotting library called ggplot2. This can be used by piping your dataframe to a function called ggplot(). The basic idea is that you add your data, then you can add plot elements which are called geoms. Some common ones are geom_line(), geom_bar() and geom_point(). To the geom function you add aesthetics, which is basically telling the function which bits of your data should be responsible for which parts of the visualisation. These are added using aes(). I’ll explain a bit more about some of these aesthetics as I go along. As an example: Bar chart of different types of fruit (one each of bananas and oranges, two types of apple) fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit)) Counting the total amount of fruit: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount)) Charting amounts and fruit colours: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount, fill = colour)) And just because it annoys me having random colours, we can map them to the actual colours: fruit_data %&gt;% ggplot() + geom_bar(aes(x = fruit, weight = amount, fill = colour)) + scale_fill_manual(values = c(&quot;orange&quot; = &quot;orange&quot;, &quot;green&quot; = &quot;#8db600&quot;, &quot;red&quot; = &quot;#ff0800&quot;, &quot;yellow&quot; = &quot;#ffe135&quot;)) 5.3.8 Doing this with newspaper data Who cares about fruit? Nobody, that’s who. We want newspaper data! Let’s load a dataset of metadata for all the titles held by the library, and do some counting and sorting. Download from here: British Library Research Repository You would need to extract into your project folder first, if you’re following along: read_csv reads the csv from file. title_list = read_csv(&#39;BritishAndIrishNewspapersTitleList_20191118.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## title_id = col_double(), ## nid = col_double(), ## nlp = col_double(), ## first_date_held = col_double(), ## publication_date_one = col_double(), ## publication_date_two = col_double() ## ) ## See spec(...) for full column specifications. Select some particularly relevant columns: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) ## # A tibble: 24,927 x 4 ## publication_title first_date_held last_date_held country_of_publi… ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 2 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 3 &quot;Corante, or, Newes from It… 1621 1621 The Netherlands ## 4 &quot;Corante, or, Newes from It… 1621 1621 England ## 5 &quot;Courant Newes out of Italy… 1621 1621 The Netherlands ## 6 &quot;A Relation of the late Occ… 1622 1622 England ## 7 &quot;A Relation of the late Occ… 1622 1622 England ## 8 &quot;A Relation of the late Occ… 1622 1622 England ## 9 &quot;A Relation of the late Occ… 1622 1622 England ## 10 &quot;A Relation of the late Occ… 1622 1622 England ## # … with 24,917 more rows Arrange in order of the latest date of publication, and then by the first date of publication: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held), first_date_held) ## # A tibble: 24,927 x 4 ## publication_title first_date_held last_date_held country_of_publi… ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Shrewsbury chronicle 1773 Continuing England ## 2 London times|The Times|Time… 1788 Continuing England ## 3 Observer (London)|Observer … 1791 Continuing England ## 4 Limerick chronicle 1800 Continuing Ireland ## 5 Hampshire chronicle|The Ham… 1816 Continuing England ## 6 The Inverness Courier, and … 1817 Continuing Scotland ## 7 Sunday times (London)|Sunda… 1822 Continuing England ## 8 The Impartial Reporter, etc 1825 Continuing Northern Ireland ## 9 Impartial reporter and farm… 1825 Continuing Northern Ireland ## 10 Aberdeen observer 1829 Continuing Scotland ## # … with 24,917 more rows Group and count by country of publication: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() ## # A tibble: 40 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 Bermuda Islands 24 ## 2 Cayman Islands 1 ## 3 England 20465 ## 4 England|Hong Kong 1 ## 5 England|India 2 ## 6 England|Iran 2 ## 7 England|Ireland 10 ## 8 England|Ireland|Northern Ireland 10 ## 9 England|Jamaica 7 ## 10 England|Malta 2 ## # … with 30 more rows Arrange again, this time in descending order of number of titles for each country: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) ## # A tibble: 40 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 England 20465 ## 2 Scotland 1778 ## 3 Ireland 1050 ## 4 Wales 1019 ## 5 Northern Ireland 415 ## 6 England|Wales 58 ## 7 Bermuda Islands 24 ## 8 England|Scotland 13 ## 9 England|Ireland 10 ## 10 England|Ireland|Northern Ireland 10 ## # … with 30 more rows Filter only those with more than 100 titles: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% filter(n&gt;=100) ## # A tibble: 5 x 2 ## country_of_publication n ## &lt;chr&gt; &lt;int&gt; ## 1 England 20465 ## 2 Scotland 1778 ## 3 Ireland 1050 ## 4 Wales 1019 ## 5 Northern Ireland 415 Make a simple bar chart: title_list %&gt;% select(publication_title, first_date_held, last_date_held, country_of_publication) %&gt;% arrange(desc(last_date_held)) %&gt;% group_by(country_of_publication) %&gt;% tally() %&gt;% arrange(desc(n)) %&gt;% filter(n&gt;=100) %&gt;% ggplot() + geom_bar(aes(x = country_of_publication, weight = n)) So that’s a very quick introduction to R. There’s loads of places to learn more. R-studio cheat sheets The Pirate’s Guide to R, a good beginners guide to base R R for data science, which teaches the tidyverse in detail Learn how to make a book like this using Bookdown "],
["geocode-and-map-newspaper-titles.html", "6 Geocode and map newspaper titles 6.1 A map of British Newspapers by City 6.2 Drawing a background map. ` 6.3 Add some points 6.4 Choropleth map 6.5 Make the points object.", " 6 Geocode and map newspaper titles R is also really good for creating maps, both for visualisations and for spatial analysis. Using an openly available list of British Library titles, with some additional coordinate information, it’s possible to very quickly make high-quality, publishable maps. 6.1 A map of British Newspapers by City To do this we’ll need three things A background map of the UK and Ireland A count of the total titles for each city A list of coordinates for all the cities. This last one is a little trickier than the other two, as I’ll explain. 6.2 Drawing a background map. ` 6.2.1 Mapping with ggplot2 and mapdata The plotting library ggplot2, which is part of the tidyverse package, contains a function called ``map_data()which turns data from the *maps* library. This can then be used to draw a map. First you'll need to install the maps package usinginstall.packages()```. Next load ggplot2 and the maps library library(ggplot2) library(maps) First create a dataframe called ‘worldmap’ with a function called map_data(). map_data() takes an argument with the name of the map you want to load, in inverted commas. Some of the choices are ‘world’, ‘usa’, ‘france’, ‘italy’. We’ll use the ‘world’ map. worldmap = map_data(&#39;world&#39;) Take a look at the dataframe we’ve created: head(worldmap, 20) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; ## 7 -70.03511 12.61411 1 7 Aruba &lt;NA&gt; ## 8 -69.97314 12.56763 1 8 Aruba &lt;NA&gt; ## 9 -69.91181 12.48047 1 9 Aruba &lt;NA&gt; ## 10 -69.89912 12.45200 1 10 Aruba &lt;NA&gt; ## 12 74.89131 37.23164 2 12 Afghanistan &lt;NA&gt; ## 13 74.84023 37.22505 2 13 Afghanistan &lt;NA&gt; ## 14 74.76738 37.24917 2 14 Afghanistan &lt;NA&gt; ## 15 74.73896 37.28564 2 15 Afghanistan &lt;NA&gt; ## 16 74.72666 37.29072 2 16 Afghanistan &lt;NA&gt; ## 17 74.66895 37.26670 2 17 Afghanistan &lt;NA&gt; ## 18 74.55899 37.23662 2 18 Afghanistan &lt;NA&gt; ## 19 74.37217 37.15771 2 19 Afghanistan &lt;NA&gt; ## 20 74.37617 37.13735 2 20 Afghanistan &lt;NA&gt; ## 21 74.49796 37.05722 2 21 Afghanistan &lt;NA&gt; It’s a big table with about 100,000 rows. Each row has a latitude and longitude, and a group. Each region and sub-region in the dataframe has its own group number. We’ll use a function geom_polygon which tells ggplot to draw a polygon (a bunch of connected lines) for each group, and display it. With the aes(), x tells ggplot2 the longitude of each point, y the latitude, and group makes sure the polygons are grouped together correctly. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) Right, it needs a bit of tweaking. First, we only want to plot points in the UK. There’s obviously way too much map for this, so the first thing we should do is restrict it to a rectangle which includes those two countries. We can do that with coord_fixed(). coord_fixed() is used to fix the aspect ratio of a coordinate system, but can be used to specify a bounding box by using two of its arguments: xlim= and ylim=. These each take a vector (a series of numbers) with two items A vector is created using c(). Each item in the vector specifies the limits for that axis. So xlim = c(0,10) means restrict the x-axis to 0 and 10. The axes correspond to the lines of longitude (x) and latitude (y). We’ll restrict the x-axis to c(-10, 4) and the y-axis to c(50.3, 60) which should just about cover the UK and Ireland. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) + coord_fixed(xlim = c(-10,3), ylim = c(50.3, 59)) You can also change the ratio of the coordinates using coord_fixed(). The default is 1, but by specifying a different one with the argument ratio =, that can be changed. Using ratio = 1.3 results in a less squashed-looking map. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group)) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) A couple more things, which I’ll run through quickly. We can specify fill and line colors usings fill = and color = inside geom_polygon() but outside aes(). ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) We probably don’t need the grids or panels in the background. We can get rid of these with + theme_void(). ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() 6.3 Add some points 6.3.1 Get a count of the total titles for each city This next bit uses some of the functions demonstrated in the introduction to R and the tidyverse, namely group_by() and tally(). First load the rest of the tidyverse packages. library(tidyverse) Next, load the title list, which can be dowloaded from the British Library’s Open Repository title_list = read_csv(&#39;BritishAndIrishNewspapersTitleList_20191118.csv&#39;) We can quite easily make a new data frame, which will just include each location and the total number of instances in the dataset. location_counts = title_list %&gt;% group_by(country_of_publication, general_area_of_coverage, coverage_city) %&gt;% tally() Arranging these in descending order of their count shows how many of each we have: location_counts %&gt;% arrange(desc(n)) ## # A tibble: 2,189 x 4 ## # Groups: country_of_publication, general_area_of_coverage [531] ## country_of_publication general_area_of_coverage coverage_city n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 England London London 5781 ## 2 Ireland Dublin (Ireland : County) Dublin 415 ## 3 Scotland Strathclyde Glasgow 309 ## 4 England Greater Manchester Manchester 265 ## 5 England West Midlands Birmingham 260 ## 6 England Merseyside Liverpool 220 ## 7 England Avon Bristol 175 ## 8 Scotland Lothian Edinburgh 162 ## 9 England South Yorkshire Sheffield 133 ## 10 England Nottinghamshire Nottingham 127 ## # … with 2,179 more rows 6.3.2 Get a list of points. These coordinates have been produced in cooperation wiht another project with the Library, Living with Machines. We used smart annotations to quickly correct and train geocorrected = read_csv(&#39;data/geocorrected.csv&#39;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## Parsed with column specification: ## cols( ## X1 = col_double(), ## Coverage..City = col_character(), ## General.area.of.coverage = col_character(), ## Country.of.publication = col_character(), ## places = col_character(), ## wikititle = col_character(), ## wikilat = col_character(), ## wikilon = col_character(), ## confscore = col_double(), ## candidates = col_character(), ## regionCandidates = col_character(), ## status = col_character() ## ) Change the column names: library(snakecase) colnames(geocorrected) = to_snake_case(colnames(geocorrected)) Change some column names further, select just the relevant columns, change the NA values and get rid of any empty entries. colnames(geocorrected)[6:8] = c(&#39;wikititle&#39;, &#39;lat&#39;, &#39;lng&#39;) geocorrected = geocorrected %&gt;% select(-1, -9,-10, -11, -12) geocorrected = geocorrected %&gt;% mutate(country_of_publication = replace(country_of_publication, country_of_publication == &#39;na&#39;, NA)) %&gt;% mutate(general_area_of_coverage = replace(general_area_of_coverage, general_area_of_coverage == &#39;na&#39;, NA)) %&gt;% mutate(coverage_city = replace(coverage_city, coverage_city == &#39;na&#39;, NA)) geocorrected = geocorrected %&gt;% mutate(lat = as.numeric(lat)) %&gt;% mutate(lng = as.numeric(lng)) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) ## Warning: NAs introduced by coercion ## Warning: NAs introduced by coercion This is a dataframe with a set of longitude and latitude points (they come from Wikipedia, which is why they are prefixed with wiki) for every combination of city/county/country in the list of titles. These can be joined to the full title list. Using left_join() we will merge these dataframes, joining up each set of location information to its coordinates and standardised name. left_join() is a very common command in data analysis. It merges two sets of data by matching a value known as a key. Here the key is three values - city, county and country, and it matches up the two sets of data by ‘joining’ two rows together, if they share all three of these values. Store this is a new variable called lc_with_geo. lc_with_geo = location_counts %&gt;% left_join(geocorrected, by = c(&#39;coverage_city&#39; ,&#39;general_area_of_coverage&#39;, &#39;country_of_publication&#39;)) If you look at this new dataset, you’ll see that now the counts of locations have merged with the geocorrected data. Now we have an amount and coordinates for each place. head(lc_with_geo, 10) ## # A tibble: 10 x 8 ## # Groups: country_of_publication, general_area_of_coverage [9] ## country_of_publ… general_area_of… coverage_city n places wikititle lat ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bermuda Islands &lt;NA&gt; Hamilton 23 &lt;NA&gt; &lt;NA&gt; NA ## 2 Bermuda Islands &lt;NA&gt; Saint George 1 &lt;NA&gt; &lt;NA&gt; NA ## 3 Cayman Islands &lt;NA&gt; Georgetown 1 &lt;NA&gt; &lt;NA&gt; NA ## 4 England Aberdeenshire (… Peterhead 1 &lt;NA&gt; &lt;NA&gt; NA ## 5 England Acton (London, … Ealing (Lond… 1 &lt;NA&gt; &lt;NA&gt; NA ## 6 England Aintree |Merse… Maghull 3 &lt;NA&gt; &lt;NA&gt; NA ## 7 England Alford (Lincoln… Mablethorpe 1 &lt;NA&gt; &lt;NA&gt; NA ## 8 England Alford (Lincoln… Skegness 1 &lt;NA&gt; &lt;NA&gt; NA ## 9 England Alfriston |Eas… Newhaven 1 &lt;NA&gt; &lt;NA&gt; NA ## 10 England Altrincham |Gr… Sale 2 &lt;NA&gt; &lt;NA&gt; NA ## # … with 1 more variable: lng &lt;dbl&gt; Right, now we’re going to use group_by() and tally() again, this time on the the wikititle, wikilat and wikilon columns. This is because the wikititle is a standardised title, which means it will group together cities properly, rather than giving a different row for slightly different combinations of the three geographic information columns. lc_with_geo_counts = lc_with_geo %&gt;% group_by(wikititle, lat, lng) %&gt;% tally(n) Now we’ve got a dataframe with counts of total newspapers, for each standardised wikipedia title in the dataset. head(lc_with_geo_counts,20) ## # A tibble: 20 x 4 ## # Groups: wikititle, lat [20] ## wikititle lat lng n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Abbots_Langley 51.7 -0.416 1 ## 2 Aberavon_(UK_Parliament_constituency) 51.6 -3.81 1 ## 3 Aberdare 51.7 -3.44 20 ## 4 Aberdeen 57.2 -2.11 82 ## 5 Abergavenny 51.8 -3.02 9 ## 6 Abergele 53.3 -3.58 8 ## 7 Abersychan 51.7 -3.06 2 ## 8 Abertillery 51.7 -3.13 2 ## 9 Aberystwyth 52.4 -4.08 31 ## 10 Abingdon-on-Thames 51.7 -1.28 23 ## 11 Accrington 53.8 -2.36 33 ## 12 Acocks_Green 52.4 -1.82 1 ## 13 Adlington,_Lancashire 53.6 -2.60 1 ## 14 Aintree 53.5 -2.94 1 ## 15 Airdrie,_North_Lanarkshire 55.9 -3.98 13 ## 16 Alcester 52.2 -1.88 5 ## 17 Aldeburgh 52.2 1.6 5 ## 18 Alderley_Edge 53.3 -2.24 4 ## 19 Aldershot 51.2 -0.758 20 ## 20 Aldridge 52.6 -1.92 1 OK, lc_with_geo_counts is what we want to plot. This contains the city title, coordinates and counts for all the relevant places in our dataset. But first we need the map we created earlier. ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() Now we will plot the cities using geom_point() We’ll specify the lc_with_geo_counts as the argument to data = within geom_point(). The x axis position of each point is the longitude, and the y axis the latitude. We’ll also use the argument size = n within the aes(), to tell ggplot2 to size the points by the column n, which contains the counts for each of our locations, and the argument alpha = .7 outside the aes(), to make the points more transparent and slightly easier to read overlapping ones. One last thing we’ll add is +scale_size_area(). This sizes the points using their radius rather than diameter, which is a more correct way of representing numbers using circles! ggplot() + geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = &#39;gray90&#39;, color = &#39;black&#39;) + coord_fixed(ratio = 1.3, xlim = c(-10,3), ylim = c(50, 59)) + theme_void() + geom_point(data = lc_with_geo_counts, aes(x = as.numeric(lng), y = as.numeric(lat), size = n), alpha = .7) + scale_size_area() 6.4 Choropleth map Another type of map is a choropleth. This is where the data is visualised by a certain polygon area rather than a point. Typically these represent areas like parishes, counties or countries. Using the library sf, which stands for Simple Features, a choropleth map can be made quite quickly. A choropleth map uses a shapefile, which is a list of polygons and a projection. The trick here is to use the coordinates to correctly situate each set of points within the correct county, as found in the shapefile. Then, count up the titles by this corrected county, and use this total to color or shade the map. The good thing about this method is that once the points are correct, they can be situated within any shapefile - a historic map, for example. This relies on data from (www.visionofbritain.ac.uk). Download shapefiles for england and scotland from here Turn into sf object Download list of points, turn into sf object Use st join to get county information Join to the title list and deselect everything except county and titles - maybe 19th century only.. Join that to the sf object Plot using geom_sf() Load libraries library(tidyverse) library(sf) ## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0 Next, download (if you haven’t already) the title list from the British Library open repository. title_df = read_csv(&#39;BritishAndIrishNewspapersTitleList_20191118.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## title_id = col_double(), ## nid = col_double(), ## nlp = col_double(), ## first_date_held = col_double(), ## publication_date_one = col_double(), ## publication_date_two = col_double() ## ) ## See spec(...) for full column specifications. 6.5 Make the points object. Make the points sf object First, download the relevant shapefiles. These don’t necessarily have to be historic ones. Use st_read() to read the file, specifying its path. Do this for England, Wales and Scotland (we don’t have points for Ireland). eng_1851 = st_read(&quot;/Users/Yann/Documents/non-Github/sf_experiments/EW1851_regcounties/EW1851_regcounties.shp&quot;) ## Reading layer `EW1851_regcounties&#39; from data source `/Users/Yann/Documents/non-Github/sf_experiments/EW1851_regcounties/EW1851_regcounties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 55 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 87019.07 ymin: 7067.26 xmax: 655838 ymax: 657543.5 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs scot_1851 = st_read(&quot;/Users/Yann/Documents/non-Github/sf_experiments/Spre1890_scocounties/Spre1890_scocounties.shp&quot;) ## Reading layer `Spre1890_scocounties&#39; from data source `/Users/Yann/Documents/non-Github/sf_experiments/Spre1890_scocounties/Spre1890_scocounties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 33 features and 7 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 53033.88 ymin: 530297 xmax: 469817 ymax: 1219574 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs These shapefiles use points system known as UTM, which stands for ‘Universal Transverse Mercator’. According to wikipedia, it differs from global latitude/longitude in that it divides earth into 60 zones and projects each to the plane as a basis for its coordinates. It needs to be transformed into lat/long coordinates, because the coordinates we have are in that format. This is easy with st_transform(). To transform correctly, the correct crs is needed. This is the code for which of the 60 zones this UTM comes from. Britain is 4326. eng_1851 = st_transform(eng_1851, crs = 4326) scot_1851 = st_transform(scot_1851, crs = 4326) Bind them both together, using rbind() to make one big shapefile for Great Britain. gb1851 = rbind(eng_1851, scot_1851 %&gt;% select(-UL_AUTH)) Next, load and pre-process the set of coordinates: geocorrected = read_csv(&#39;data/geocorrected.csv&#39;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## Parsed with column specification: ## cols( ## X1 = col_double(), ## Coverage..City = col_character(), ## General.area.of.coverage = col_character(), ## Country.of.publication = col_character(), ## places = col_character(), ## wikititle = col_character(), ## wikilat = col_character(), ## wikilon = col_character(), ## confscore = col_double(), ## candidates = col_character(), ## regionCandidates = col_character(), ## status = col_character() ## ) Change the column names: library(snakecase) colnames(geocorrected) = to_snake_case(colnames(geocorrected)) Change some column names further, select just the relevant columns, change the NA values and get rid of any empty entries. colnames(geocorrected)[6:8] = c(&#39;wikititle&#39;, &#39;lat&#39;, &#39;lng&#39;) geocorrected = geocorrected %&gt;% select(-1, -9,-10, -11, -12) geocorrected = geocorrected %&gt;% mutate(country_of_publication = replace(country_of_publication, country_of_publication == &#39;na&#39;, NA)) %&gt;% mutate(general_area_of_coverage = replace(general_area_of_coverage, general_area_of_coverage == &#39;na&#39;, NA)) %&gt;% mutate(coverage_city = replace(coverage_city, coverage_city == &#39;na&#39;, NA)) geocorrected = geocorrected %&gt;% mutate(lat = as.numeric(lat)) %&gt;% mutate(lng = as.numeric(lng)) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) ## Warning: NAs introduced by coercion ## Warning: NAs introduced by coercion Next, join these points to the title list, so that every title now has a set of lat/long coordinates. title_df = title_df %&gt;% left_join(geocorrected) %&gt;% filter(!is.na(lat)) %&gt;% filter(!is.na(lng)) ## Joining, by = c(&quot;country_of_publication&quot;, &quot;general_area_of_coverage&quot;, &quot;coverage_city&quot;) To join this to the shapefile, we need to turn it in to an simple features item. To do this we need to specify the coordinates and the CRS. The resulting file will contain a new column called ‘geometry’, containing the lat/long coordaintes in the correct simple features format. st_title = st_as_sf(title_df, coords = c(&#39;lng&#39;, &#39;lat&#39;)) st_title = st_title %&gt;% st_set_crs(4326) Now, we can use a special kind of join, which will join the points in the title list, if they are within a particular polygon. The resulting dataset now has the relevant county, as found in the shapefile. st_counties = st_join(st_title, gb1851) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar Make a new dataframe, containing just the counties and their counts. county_tally = st_counties %&gt;% select(G_NAME) %&gt;% group_by(G_NAME) %&gt;% tally() %&gt;% st_drop_geometry() ## Warning: Factor `G_NAME` contains implicit NA, consider using ## `forcats::fct_explicit_na` Join this to the shapefile we made earlier, which gives a dataset with the relevant counts attached to each polygon. This can then be visualised using the geom_sf() function from ggplot2, and all of ggplot2’s other features can be used. gb1851 %&gt;% left_join(county_tally) %&gt;% ggplot() + geom_sf(lwd = .2,color = &#39;black&#39;, aes(fill = n)) + theme_void() + lims(fill = c(10,4000)) + scale_fill_viridis_c(option = &#39;plasma&#39;) ## Joining, by = &quot;G_NAME&quot; ## Scale for &#39;fill&#39; is already present. Adding another scale for &#39;fill&#39;, which ## will replace the existing scale. "],
["text-mining.html", "7 Text mining 7.1 What were the most common words used in newspaper titles in the nineteenth century?_", " 7 Text mining 7.1 What were the most common words used in newspaper titles in the nineteenth century?_ title 7.1.1 Titles don’t just help you identity a newspaper, but they might tell you a little bit about the time in which they were established. With a bibliographic list of all our UK and Irish titles, we can count the most frequent words and track them over time and place, using text mining and data analysis. 7.1.2 What is this document? This is a markdown file, made from a Jupyter notebook. A jupyter notebook is usually an interactive document which can contain code, images and text, and a markdown file is a static version of that. Each bit of code runs in a cell, and the output is displayed directly below. The code I’ve used is R, which is a language particularly good for data analysis, but another language, Python, is probably used in Jupyter more frequently. If you’re going to work in R, I would recommend downloading R-Studio to do all your work, which could then be copied-and-pasted over the Jupyter notebook if needed, like I’ve done here. There are tonnes of places to get started working with R, Python, Jupyter notebooks and so forth, and we would recommend looking here in the first instance: https://programminghistorian.org/ https://software-carpentry.org/ First we need to load some libraries which we’ll use. A library is just a bunch of functions* grouped together, usually with a particular overall purpose or theme. ‘tidyverse’ is actually a number of libraries with hundreds of useful functions to make lots of data analysis easier. It includes a very powerful plotting library called ‘ggplot2’. It’s usually the first thing I load, before even deciding what I’m going to do with my data. ‘readxl’ is a library which.. reads excel files.. Lots of this code uses something called piping. This is a function in one of our tidyverse libraries which allows you to do something to your data, and then pass it along to another function using this symbol: %&gt;% It allows you to string lots of changes to your data together in one block of code, so you might filter it, then pass the filtered data to another function which summarises it, and pass it on to another function which plots it as a graph. * You might say a function is a pre-made block of code which does something to some data. It has a name and often one or more arguments. The first argument is often a space for you to specify the thing you want to do the function on, and subsequent arguments might be additional parameters. library(tidyverse) library(readxl) The first thing we do is load the whole title list as a variable called ‘working_list’, specifying the sheet of the excel file we’d like to use. We’ll dive a little deeper into the structure and how we might filter in another notebook. working_list &lt;- read_csv( &quot;BritishAndIrishNewspapersTitleList_20191118.csv&quot;, local = locale(encoding = &quot;latin1&quot;)) ## Parsed with column specification: ## cols( ## .default = col_character(), ## title_id = col_double(), ## nid = col_double(), ## nlp = col_double(), ## first_date_held = col_double(), ## publication_date_one = col_double(), ## publication_date_two = col_double() ## ) ## See spec(...) for full column specifications. Let’s just look at the nineteenth century - we’ll use the filter() function from dplyr, which is one of the libraries in the tidyverse. We then use %&gt;% to pipe the filtered data to a function called head(), which displays a set number of rows of your data frama - useful for taking a peek at the structure. working_list %&gt;% filter(last_date_held&gt;1799) %&gt;% filter(first_date_held&lt;1900) %&gt;% head(2) ## # A tibble: 2 x 24 ## title_id nid nlp publication_tit… edition preceding_titles ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 13774146 NA NA London gazette &lt;NA&gt; Continues: Oxfo… ## 2 13943676 32666 537 The Newcastle C… &lt;NA&gt; &lt;NA&gt; ## # … with 18 more variables: succeeding_titles &lt;chr&gt;, ## # place_of_publication &lt;chr&gt;, country_of_publication &lt;chr&gt;, ## # general_area_of_coverage &lt;chr&gt;, coverage_city &lt;chr&gt;, ## # first_geographical_subject_heading &lt;chr&gt;, ## # subsequent_geographical_subject_headings &lt;chr&gt;, first_date_held &lt;dbl&gt;, ## # last_date_held &lt;chr&gt;, publication_date_one &lt;dbl&gt;, ## # publication_date_two &lt;dbl&gt;, current_publication_frequency &lt;chr&gt;, ## # publisher &lt;chr&gt;, holdings_more_information &lt;chr&gt;, ## # free_text_information_about_dates_of_publication &lt;chr&gt;, ## # online_status &lt;chr&gt;, link_to_british_newspaper_archive &lt;chr&gt;, ## # explore_link &lt;chr&gt; We have some duplicated newspaper titles in this dataframe. We can get rid of these for this analysis, though there are reasons we left them in which I won’t go into now. We can delete any duplicated titles using a function called distinct() on NID id field. working_list %&gt;% filter(last_date_held&gt;1799) %&gt;% filter(first_date_held&lt;1900) %&gt;% distinct(NID, .keep_all = TRUE) %&gt;% head(2) ## Warning: Trying to compute distinct() for variables not found in the data: ## - `NID` ## This is an error, but only a warning is raised for compatibility reasons. ## The operation will return the input unchanged. ## # A tibble: 2 x 24 ## title_id nid nlp publication_tit… edition preceding_titles ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 13774146 NA NA London gazette &lt;NA&gt; Continues: Oxfo… ## 2 13943676 32666 537 The Newcastle C… &lt;NA&gt; &lt;NA&gt; ## # … with 18 more variables: succeeding_titles &lt;chr&gt;, ## # place_of_publication &lt;chr&gt;, country_of_publication &lt;chr&gt;, ## # general_area_of_coverage &lt;chr&gt;, coverage_city &lt;chr&gt;, ## # first_geographical_subject_heading &lt;chr&gt;, ## # subsequent_geographical_subject_headings &lt;chr&gt;, first_date_held &lt;dbl&gt;, ## # last_date_held &lt;chr&gt;, publication_date_one &lt;dbl&gt;, ## # publication_date_two &lt;dbl&gt;, current_publication_frequency &lt;chr&gt;, ## # publisher &lt;chr&gt;, holdings_more_information &lt;chr&gt;, ## # free_text_information_about_dates_of_publication &lt;chr&gt;, ## # online_status &lt;chr&gt;, link_to_british_newspaper_archive &lt;chr&gt;, ## # explore_link &lt;chr&gt; Select only the information we need. Let’s create a new dataframe, run the filter and distinct functions, and select some date information along with the titles, Afterwards we might want to choose geographic information instead, so we’ll keep our original data frame: titles_dates = working_list %&gt;% filter(last_date_held&gt;1799) %&gt;% filter(first_date_held&lt;1900) %&gt;% distinct(NID, .keep_all = TRUE) %&gt;% select(publication_title, first_date_held) ## Warning: Trying to compute distinct() for variables not found in the data: ## - `NID` ## This is an error, but only a warning is raised for compatibility reasons. ## The operation will return the input unchanged. To count the title keywords, we can tokenise our data. This splits everything into individual words. For this we need to load a library called ‘tidytext’, which contains lots of functions for text mining. library(tidytext) tokenised_titles_dates = titles_dates %&gt;% unnest_tokens(word, publication_title) Now we’ll get rid of stopwords - the very frequently-used words like ‘the’ or ‘an’ and so forth. It’s not always appropriate to remove stopwords, and in fact sometimes they are the most interesting, but I think here it will make things easier to manage. data(stop_words) # this loads a dataset of stopwords tokenised_titles_dates = tokenised_titles_dates %&gt;% anti_join(stop_words) # this does an &#39;anti-join&#39; which removes any word in one list which also appears in another. ## Joining, by = &quot;word&quot; Let’s do some simple analysis first. We can count the most common words overall: tokenised_titles_dates %&gt;% count(word, sort = TRUE) %&gt;% head(20) ## # A tibble: 20 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 advertiser 2343 ## 2 news 1161 ## 3 weekly 918 ## 4 gazette 910 ## 5 times 890 ## 6 chronicle 834 ## 7 journal 748 ## 8 herald 658 ## 9 express 430 ## 10 press 387 ## 11 north 375 ## 12 west 372 ## 13 south 367 ## 14 london 361 ## 15 observer 354 ## 16 daily 319 ## 17 evening 298 ## 18 county 296 ## 19 guardian 289 ## 20 free 258 We can turn it into a bar chart. tokenised_titles_dates %&gt;% count(word, sort = TRUE) %&gt;% mutate(word = reorder(word, n)) %&gt;% head(20) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() Advertiser is the most popular word in a title! Lots of other words are at the top which might be expected, like ‘news’, ‘daily’, ‘evening’ and so forth. It might be more interesting to look at the changes in the top words over time. This adds a new column with the date ‘floored’ to the previous 20. When we group and count again, everything between 1800 and 1819 will become 1800, everything between 1820 and 1839 will become 1820 and so forth. tokenised_titles_dates = tokenised_titles_dates %&gt;% mutate(timespan = first_date_held - first_date_held %% 20) Now look at the top ten title words for each of these twenty-year periods: 1800 - 1819: tokenised_titles_dates %&gt;% filter(timespan == &#39;1800&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(10) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() 1820 - 1839: tokenised_titles_dates %&gt;% filter(timespan == &#39;1820&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(10) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() 1840 - 1859 (it’s interesting how ‘British’ has fallen out of the top ten, and ‘north’ and ‘south’ have been bumped up. Even something this simple can confirming interesting things about the growth of the regional press! tokenised_titles_dates %&gt;% filter(timespan == &#39;1840&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(10) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() 1860 - 1879: tokenised_titles_dates %&gt;% filter(timespan == &#39;1860&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(10) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() 1880 - 1899 tokenised_titles_dates %&gt;% filter(timespan == &#39;1880&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% filter(n &gt; 100) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() png There might be better ways of counting. How about a line chart which tracks a certain number of keywords over time? We know that the number of titles per year increases a lot over the century, so we’ll need to make a relative rather than absolute values. This next bit adds a count of all the words per decade, and puts it beside each word. Then we can divide one by the other and get a fraction. I’ll show the first few lines of each dataframe to show what I mean. title_words = tokenised_titles_dates %&gt;% mutate(decade = first_date_held - first_date_held %% 10) %&gt;% count(decade, word, sort = TRUE) head(title_words, 5) ## # A tibble: 5 x 3 ## decade word n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1850 advertiser 506 ## 2 1860 advertiser 425 ## 3 1870 advertiser 322 ## 4 1880 advertiser 285 ## 5 1890 news 276 total_words &lt;- title_words %&gt;% group_by(decade) %&gt;% summarize(total = sum(n)) head(total_words, 5) ## # A tibble: 5 x 2 ## decade total ## &lt;dbl&gt; &lt;int&gt; ## 1 1660 2 ## 2 1710 2 ## 3 1720 26 ## 4 1730 17 ## 5 1740 21 title_words &lt;- left_join(title_words, total_words) ## Joining, by = &quot;decade&quot; head(title_words, 5) ## # A tibble: 5 x 4 ## decade word n total ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1850 advertiser 506 5416 ## 2 1860 advertiser 425 5835 ## 3 1870 advertiser 322 6581 ## 4 1880 advertiser 285 6396 ## 5 1890 news 276 6304 We’ll pick a list of terms to plot, because otherwise it’ll be unreadable. top_terms = c(&#39;advertiser&#39;, &#39;news&#39;, &#39;weekly&#39;, &#39;times&#39;, &#39;gazette&#39;, &#39;chronicle&#39;, &#39;herald&#39;, &#39;journal&#39;, &#39;express&#39;, &#39;bulletin&#39;, &#39;record&#39;, &#39;guardian&#39;, &#39;express&#39;) Now we draw a plot, using n/total as the y-axis variable to plot the fraction of the total: title_words %&gt;% filter(word %in% top_terms) %&gt;% filter(decade &gt;1800) %&gt;% ggplot(aes(x = decade, y = n/total, color = word)) + geom_line(size = 1, alpha = .7,stat = &#39;identity&#39;) + theme_minimal() + ylab(label = &quot;Percentage of total words&quot;) + theme(legend.position = &#39;bottom&#39;) Well, that’s quite interesting. Advertiser declines in the second half of the century, and is overtaken by ‘news’ right at the end. 7.1.3 How about differences by country? Go back to the working list and make a version with country information: titles_countries = working_list %&gt;% select(publication_title, country_of_publication) tokenised_titles_countries = titles_countries %&gt;% unnest_tokens(word, publication_title) tokenised_titles_countries = tokenised_titles_countries %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; Let’s make an overall chart for each country: England: tokenised_titles_countries %&gt;% filter(country_of_publication == &#39;England&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(15) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() Scotland: tokenised_titles_countries %&gt;% filter(country_of_publication == &#39;Scotland&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% filter(n &gt; 40) %&gt;% mutate(word = reorder(word, n)) %&gt;% head(15) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() Ireland: tokenised_titles_countries %&gt;% filter(country_of_publication %in% c(&#39;Ireland&#39;, &#39;Northern Ireland&#39;)) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(15) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() Wales: tokenised_titles_countries %&gt;% filter(country_of_publication == &#39;Wales&#39;) %&gt;% count(word, sort = TRUE) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% head(15) %&gt;% ggplot(aes(word, n)) + geom_col(fill = &#39;lightblue&#39;, color = &#39;black&#39;, alpha = .7) + xlab(NULL) + theme_minimal() + coord_flip() How about looking for the most unique terms for each country? That might tell us something interesting. First we’ll select just the key countries: countryList = c(&#39;England&#39;, &#39;Ireland&#39;, &#39;Wales&#39;, &#39;Scotland&#39;, &#39;Northern Ireland&#39;) Next we’ll use a function which gives the ‘tf-idf’ score for each word. This measures the frequency of the word in comparison to its frequency in all other countries, giving us words that are more unique to titles from that country. total_by_country = tokenised_titles_countries %&gt;% filter(country_of_publication %in% countryList) %&gt;% count(country_of_publication, word, sort = TRUE) total_by_country &lt;- total_by_country %&gt;% bind_tf_idf(word, country_of_publication, n) total_by_country %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(country_of_publication) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = country_of_publication)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~country_of_publication, ncol = 2, scales = &quot;free&quot;) + coord_flip() ## Selecting by tf_idf Unsurprisingly, this mostly gives us placenames, as used in the titles, which are obviously only going to be used in one country. There’s a couple of interesting things: ‘Wales’ and ‘Scotland’ have a high score, but not ‘Ireland’. Why would Ireland not be used in a newspaper title in the same way as Welsh or Scottish newspapers? We need a way to try and filter out geographic places as they’re drowning out other potentially interesting terms. We can make a list of places from our original title list which would be a good start. all_places = read_csv( &quot;BritishAndIrishNewspapersTitleList_20191118.csv&quot;, local = locale(encoding = &quot;latin1&quot;)) ## Parsed with column specification: ## cols( ## .default = col_character(), ## title_id = col_double(), ## nid = col_double(), ## nlp = col_double(), ## first_date_held = col_double(), ## publication_date_one = col_double(), ## publication_date_two = col_double() ## ) ## See spec(...) for full column specifications. list_of_places = c(all_places$first_geographical_subject_heading, all_places$subsequent_geographical_subject_headings, all_places$general_area_of_coverage, all_places$coverage_city, all_places$place_of_publication, all_places$country_of_publication) list_of_places = as.data.frame(list_of_places) %&gt;% group_by(list_of_places) %&gt;% count() %&gt;% select(list_of_places) ## Warning: Factor `list_of_places` contains implicit NA, consider using ## `forcats::fct_explicit_na` ## Warning: Factor `list_of_places` contains implicit NA, consider using ## `forcats::fct_explicit_na` list_of_places = tolower(list_of_places$list_of_places) It’s a bit crude but it’s given us a list of 5,000 or so places which we can use to filter our word list. Plotting the filtered list: total_by_country %&gt;% filter(!word %in% list_of_places) %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(country_of_publication) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = country_of_publication)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~country_of_publication, ncol = 2, scales = &quot;free&quot;) + coord_flip() ## Selecting by tf_idf There are still lots of geographic terms, but there are some other words ‘unique’ to each country. Ireland and Northern Ireland high-scoring words are particularly interesting (of course we’re looking at 19th century titles so the division is meaningless, but it still represents some kind of regionality). Only Irish titles tend to have ideological terms like ‘nationalist’, ‘democrat’, ‘loyalty’, ‘impartial’ and so forth. Newspapers with ‘illustrated’ or ‘Sunday’ in the title are unique to England, and, intriguingly, ‘visitors’. "],
["extract-text.html", "8 Make a Text Corpus 8.1 Where is this data? 8.2 Folder structure", " 8 Make a Text Corpus 8.1 Where is this data? Go to repository Describe folder structure Naming convention Multiple titles for one newspaper How to then unzip and extract in bulk, using R Then how to extract text And iterate through all the folders save to a dataframe Inspired by the brilliant posts by https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/ I’ve written some code to extract the full text of articles from our newspapers, and then do some basic text mining and topic modelling. Unfortunately for me, the METS/ALTO flavour produced by the British Library is very different to the BnL. I’ve taken the basic principle of the the tutorial above and tailored it heavily to the British Library’s METS/ALTO. In the BL, the METS file contains information on textblocks. Each textblock has a code, which can be found in one of the ALTO files - of which there are one per page. The ALTO files list each individual word in each textblock. The METS file also contains information on which textblocks make up each article. the output will be a csv for each issue - these can be combined into a single dataframe afterwards, but it’s nice to have the .csv files themselves first of all. 8.2 Folder structure We’ll do this on a small sample of newspapers prepared for this notebook. You can find them here, and you should put them in the same folder as this notebook. The folder structure of the newspapers is [nlp]-&gt;year-&gt;issue month and day-&gt; xml files. The nlp is a unique code given to each digitised newspaper. This makes it easy to find an individual issue of a newspaper. Load some libraries: all the text extraction is done using tidyverse and furrr for some parallel programming. require(furrr) ## Loading required package: furrr ## Loading required package: future require(tidyverse) library(tidytext) Basically there’s two main functions: get_page(), which extracts words and their corresponding textblock, and make_articles(), which extracts a table of the textblocks and corresponding articles, and joins them to the words from get_page(). Here’s get_page(): get_page = function(alto){ page = alto %&gt;% read_file() %&gt;% str_split(&quot;\\n&quot;, simplify = TRUE) %&gt;% keep(str_detect(., &quot;CONTENT|&lt;TextBlock ID=&quot;)) %&gt;% str_extract(&quot;(?&lt;=CONTENT=\\&quot;)(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)&quot;)%&gt;% discard(is.na) %&gt;% as.tibble() %&gt;% mutate(pa = ifelse(str_detect(value, &quot;pa[0-9]{7}&quot;), str_extract(value, &quot;pa[0-9]{7}&quot;), NA)) %&gt;% fill(pa) %&gt;% filter(str_detect(pa, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;STYLE=\\&quot;subscript\\&quot; &quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;\\&quot;&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart2 SUBS_CONTENT=&quot;)) } I’ll break it down in stages: First read the alto page, which should be an argument to the function. Here’s one page to use as an example: alto = &quot;0002644/1809/0101/0002644_18090101_0001.xml&quot; altofile = alto %&gt;% read_file() Split the file on each new line: altofile = altofile %&gt;% str_split(&quot;\\n&quot;, simplify = TRUE) altofile %&gt;% glimpse() ## chr [1, 1:2650] &quot;&lt;?xml version=\\&quot;1.0\\&quot; encoding=\\&quot;UTF-8\\&quot;?&gt;\\r&quot; ... Just keep lines which contain either a CONTENT or TextBlock tag: altofile = altofile %&gt;% keep(str_detect(., &quot;CONTENT|&lt;TextBlock ID=&quot;)) altofile %&gt;% glimpse() ## chr [1:1173] &quot;\\t\\t\\t\\t&lt;TextBlock ID=\\&quot;pa0001001\\&quot; HPOS=\\&quot;304\\&quot; VPOS=\\&quot;58\\&quot; WIDTH=\\&quot;2152\\&quot; HEIGHT=\\&quot;154\\&quot; STYLEREFS=\\&quot;TXT_0 PAR_LEFT\\&quot;&gt;\\r&quot; ... This was the bit I never would have figured out: it extracts the words and the textblock ID for each line. altofile = altofile %&gt;% str_extract(&quot;(?&lt;=CONTENT=\\&quot;)(.*?)(?=WC)|(?&lt;=&lt;TextBlock ID=)(.*?)(?= HPOS=)&quot;)%&gt;% discard(is.na) %&gt;% as_tibble() ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. Now I have a dataframe with a single column, which is every textblock, textline and word in the ALTO file. Now we need to extract the textblock IDs, put them in a separate column, and then fill() each textblock ID down until it reaches the next one. altofile = altofile %&gt;% mutate(pa = ifelse(str_detect(value, &quot;pa[0-9]{7}&quot;), str_extract(value, &quot;pa[0-9]{7}&quot;), NA)) %&gt;% fill(pa) Now we get rid of the textblock lines in the column which should contain words, and get rid of some other tags which have come through: altofile = altofile %&gt;% filter(str_detect(pa, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;pa[0-9]{7}&quot;)) %&gt;% filter(!str_detect(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;STYLE=\\&quot;subscript\\&quot; &quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;\\&quot;&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart1 SUBS_CONTENT=&quot;)) %&gt;% mutate(value = str_remove_all(value, &quot;SUBS_TYPE=HypPart2 SUBS_CONTENT=&quot;)) Ta-da! A dataframe with individual words on one side and the text block on the other. head(altofile) ## # A tibble: 6 x 2 ## value pa ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;:THE &quot; pa0001001 ## 2 &quot;NATIONAL &quot; pa0001001 ## 3 &quot;REGIS &quot; pa0001001 ## 4 &quot;.. &quot; pa0001002 ## 5 &quot;_. &quot; pa0001002 ## 6 &quot;•f.4r)i &quot; pa0001003 This is the second function: make_articles &lt;- function(foldername){ metsfilename = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*mets.xml&quot;) %&gt;% discard(is.na) csvfoldername = metsfilename %&gt;% str_remove(&quot;_mets.xml&quot;) metsfile = read_file(metsfilename) page_list = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*[0-9]{4}.xml&quot;) %&gt;% discard(is.na) metspagegroups = metsfile %&gt;% str_split(&quot;&lt;mets:smLinkGrp&gt;&quot;)%&gt;% flatten_chr() %&gt;% as_tibble() %&gt;% filter(str_detect(value, &#39;#art[0-9]{4}&#39;)) %&gt;% mutate(articleid = str_extract(value,&quot;[0-9]{4}&quot;)) future_map(page_list, get_page) %&gt;% bind_rows() %&gt;% left_join(extract_page_groups(metspagegroups$value) %&gt;% unnest() %&gt;% mutate(art = ifelse(str_detect(id, &quot;art&quot;), str_extract(id, &quot;[0-9]{4}&quot;), NA)) %&gt;% fill(art) %&gt;% filter(!str_detect(id, &quot;art[0-9]{4}&quot;)), by = c(&#39;pa&#39; = &#39;id&#39;)) %&gt;% group_by(art) %&gt;% summarise(text = paste0(value, collapse = &#39; &#39;)) %&gt;% mutate(issue_name = metsfilename ) %&gt;% write_csv(path = paste0(csvfoldername, &quot;.csv&quot;)) } It’s a bit more complicated, and a bit of a fudge. Because there are multiple ALTO pages for one METS file, we need to read in all the ALTO files, run our get_pages() function on them within this function, bind them altogether, and then join that to a METS file which contains an article ID and all the corresponding textBlocks. I’ll try to break it down into components: It takes an argument called ‘foldername’. This should be a list of issue folders - which is that final folder, in the format mmdd, which contains a single issue. we can pass a list of folder names using furrr, and it will run the function of each folder in turn. To break it down, with a single folder: foldername = &quot;0002644/1809/0101/&quot; Using the folder name as the last part of the file path, and then a regular expression to get only a file ending in mets.xml, this will get the correct METS file name and read it into memory: metsfilename = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*mets.xml&quot;) %&gt;% discard(is.na) metsfile = read_file(metsfilename) We also need to call the .csv (which we’re going to have as an output) a unique name: csvfoldername = metsfilename %&gt;% str_remove(&quot;_mets.xml&quot;) Next we have to grab all the ALTO files in the same folder, using the same method: page_list = str_match(list.files(path = foldername, all.files = TRUE, recursive = TRUE, full.names = TRUE), &quot;.*[0-9]{4}.xml&quot;) %&gt;% discard(is.na) Next we need the file which lists all the pagegroups and corresponding articles. metspagegroups = metsfile %&gt;% str_split(&quot;&lt;mets:smLinkGrp&gt;&quot;) %&gt;% flatten_chr() %&gt;% as_tibble() %&gt;% filter(str_detect(value, &#39;#art[0-9]{4}&#39;)) %&gt;% mutate(articleid = str_extract(value,&quot;[0-9]{4}&quot;)) The next bit uses a function written by brodrigues called extractor() extractor &lt;- function(string, regex, all = FALSE){ if(all) { string %&gt;% str_extract_all(regex) %&gt;% flatten_chr() %&gt;% str_extract_all(&quot;[:alnum:]+&quot;, simplify = FALSE) %&gt;% map(paste, collapse = &quot;_&quot;) %&gt;% flatten_chr() } else { string %&gt;% str_extract(regex) %&gt;% str_extract_all(&quot;[:alnum:]+&quot;, simplify = TRUE) %&gt;% paste(collapse = &quot; &quot;) %&gt;% tolower() } } We also need another function which extracts the correct pagegroups: extract_page_groups &lt;- function(article){ id &lt;- article %&gt;% extractor(&quot;(?&lt;=&lt;mets:smLocatorLink xlink:href=\\&quot;#)(.*?)(?=\\&quot; xlink:label=\\&quot;)&quot;, all = TRUE) type = tibble::tribble(~id, id) } Next this takes the list of ALTO files, and applies the get_page() function to each item, then binds the four files together vertically. I’ll give it a random variable name, even though it doesn’t need one in the function because we just pipe it along to the csv. t = future_map(page_list, get_page) %&gt;% bind_rows() head(t) This extracts the page groups from the mets dataframe we made, and turns it into a dataframe with the article ID as a number, again extracting and filtering using regular expressions, and using fill(). The result is a dataframe of every word, plus their article and text block. t = t %&gt;% left_join(extract_page_groups(metspagegroups$value) %&gt;% unnest() %&gt;% mutate(art = ifelse(str_detect(id, &quot;art&quot;), str_extract(id, &quot;[0-9]{4}&quot;), NA))%&gt;% fill(art), by = c(&#39;pa&#39; = &#39;id&#39;)) %&gt;% fill(art) head(t, 50) Next we use summarise() and paste() to group the words into the individual articles, and add the mets filename so that we also can extract the issue date afterwards. t = t %&gt;% group_by(art) %&gt;% summarise(text = paste0(value, collapse = &#39; &#39;)) %&gt;% mutate(issue_name = metsfilename ) head(t, 10) And finally write to .csv using the csvfoldername we created: t %&gt;% write_csv(path = paste0(csvfoldername, &quot;.csv&quot;)) To run it on a bunch of folders, you’ll need to make a list of paths to all the issue folders you want to process. You can do this using list_dirs. You only want these final-level issue folders, otherwise it will try to work on an empty folder and give an error. This means that if you want to work on multiple years or issues, you’ll need to figure out how to pass a list of just the isuse level folder paths. folderlist = list.dirs(&quot;0002644/1809/&quot;, full.names = TRUE, recursive = TRUE)[-1] Finally, this applies the function make_articles() to everything in the folderslist vector. It will write a new .csv file into each of the folders, containing the article text and codes. future_map(folderlist, make_articles) It’s not very fast, but you should now have a .csv file in each of the issue folders, with a row per line. We’ll also make a dataframe containing all the .csv files, which is easier to work with, add a unique code for each article, and extract date and title information from the file path. I’ll save this to R’s data format using save() news_sample_dataframe = list.files(&quot;0002644/1809/&quot;, pattern = &quot;csv&quot;, recursive = TRUE, full.names = TRUE) %&gt;% map_df(~read_csv(.)) %&gt;% separate(issue_name, into = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;), sep = &quot;/&quot;) %&gt;% select(art, text, a, b, d) %&gt;% select(art, text, title = a, year = b, date = d) %&gt;% mutate(full_date = as.Date(paste0(year,date), &#39;%Y%m%d&#39;)) %&gt;% mutate(article_code = 1:n()) %&gt;% select(article_code, everything()) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) ## Parsed with column specification: ## cols( ## art = col_character(), ## text = col_character(), ## issue_name = col_character() ## ) save(news_sample_dataframe, file = &#39;news_sample_dataframe&#39;) This news_sample_dataframe will be used for some basic text mining examples: word frequency count tf-idf scores sentiment analysis topic modelling text reuse "],
["term-frequencies.html", "9 Term Frequencies", " 9 Term Frequencies Use the dataframe to get lists of top words, separated by date/issue etc. load(&#39;news_sample_dataframe&#39;) library(tidyverse) library(tidytext) The data frame has a row per article. This is a really easy format to do text mining with, using the techniques from here: https://www.tidytextmining.com/ glimpse(news_sample_dataframe) ## Observations: 621 ## Variables: 7 ## $ article_code &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ art &lt;chr&gt; &quot;0001&quot;, &quot;0002&quot;, &quot;0003&quot;, &quot;0005&quot;, &quot;0007&quot;, &quot;0008&quot;, &quot;0009&quot;, … ## $ text &lt;chr&gt; &quot;:THE NATIONAL REGIS .. _. •f.4r)i ,&amp;#34;•)&#39;; 1 S… ## $ title &lt;chr&gt; &quot;0002644&quot;, &quot;0002644&quot;, &quot;0002644&quot;, &quot;0002644&quot;, &quot;0002644&quot;, &quot;… ## $ year &lt;chr&gt; &quot;1809&quot;, &quot;1809&quot;, &quot;1809&quot;, &quot;1809&quot;, &quot;1809&quot;, &quot;1809&quot;, &quot;1809&quot;, … ## $ date &lt;chr&gt; &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, … ## $ full_date &lt;date&gt; 1809-01-01, 1809-01-01, 1809-01-01, 1809-01-01, 1809-01… Most analysis involves tokenising the text. This divides the text into ‘tokens’ - representing one unit. A unit is often a word, but could be a bigram - a sequence of two consecutive words, or a trigram, a sequence of three consecutive words. With the library tidytext, this is done using a function called unnest_tokens(). This will split the column containing the text of the article into a long dataframe, with one word per row. The two most important arguments to ``unnest_tokensareoutputandinput```. This is fairly self explanatory. Just pass it the name you would like to give the new column of words (or n-grams) and the column you’d like to split up: in this case the original column is called ‘text’, and we’d like our column of words to be called words. news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) ## # A tibble: 467,021 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002644 1809 0101 1809-01-01 the ## 2 1 0001 0002644 1809 0101 1809-01-01 national ## 3 1 0001 0002644 1809 0101 1809-01-01 regis ## 4 1 0001 0002644 1809 0101 1809-01-01 f ## 5 1 0001 0002644 1809 0101 1809-01-01 4r ## 6 1 0001 0002644 1809 0101 1809-01-01 i ## 7 1 0001 0002644 1809 0101 1809-01-01 34 ## 8 1 0001 0002644 1809 0101 1809-01-01 1 ## 9 1 0001 0002644 1809 0101 1809-01-01 style ## 10 1 0001 0002644 1809 0101 1809-01-01 superscript ## # … with 467,011 more rows I can also specify an argument for token, allowing me to split the text into sentences, characters, lines, or n-grams.If I split into n-grams, I used the argument n to specify how many consecutive words I’d like to use. Like this: news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text, token = &#39;ngrams&#39;, n =3) ## # A tibble: 465,872 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002644 1809 0101 1809-01-01 the national regis ## 2 1 0001 0002644 1809 0101 1809-01-01 national regis f ## 3 1 0001 0002644 1809 0101 1809-01-01 regis f 4r ## 4 1 0001 0002644 1809 0101 1809-01-01 f 4r i ## 5 1 0001 0002644 1809 0101 1809-01-01 4r i 34 ## 6 1 0001 0002644 1809 0101 1809-01-01 i 34 1 ## 7 1 0001 0002644 1809 0101 1809-01-01 34 1 style ## 8 1 0001 0002644 1809 0101 1809-01-01 1 style superscript ## 9 1 0001 0002644 1809 0101 1809-01-01 style superscript l ## 10 1 0001 0002644 1809 0101 1809-01-01 superscript l style ## # … with 465,862 more rows Before we do any counting, there’s a couple more processing steps. I’m going to remove ‘stop words’. Stop words are very frequently-used words which often crowd out more interesting results. This isn’t always the case, and you shoudln’t just automatically get rid of them, but rather think about what it is yo uare looking for. For this tutorial, though, the results will be more interesting if it’s not just a bunch of ‘the’ and ‘at’ and so forth. This is really easy. We load a dataframe of stopwords, which is included in the tidytext package. data(&quot;stop_words&quot;) Next use the function anti_join(). This bascially removes any word in our word list which is also in the stop words list news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; ## # A tibble: 222,078 x 7 ## article_code art title year date full_date word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 0001 0002644 1809 0101 1809-01-01 national ## 2 1 0001 0002644 1809 0101 1809-01-01 regis ## 3 1 0001 0002644 1809 0101 1809-01-01 4r ## 4 1 0001 0002644 1809 0101 1809-01-01 34 ## 5 1 0001 0002644 1809 0101 1809-01-01 1 ## 6 1 0001 0002644 1809 0101 1809-01-01 style ## 7 1 0001 0002644 1809 0101 1809-01-01 superscript ## 8 1 0001 0002644 1809 0101 1809-01-01 style ## 9 1 0001 0002644 1809 0101 1809-01-01 superscript ## 10 1 0001 0002644 1809 0101 1809-01-01 l2o ## # … with 222,068 more rows A couple of words from the .xml have managed to sneak through our text processing: ‘style’ and ‘superscript’. I’m also going to remove these, plus a few more common OCR errors for the word ‘the’. I’m also going to remove any word with two or less characters, and any numbers. Again, these are optional steps. I’ll store the dataframe as a variable called ‘tokenised_news_sample’. I’ll also save it using save(), which turns it into an .rdata file, which can be used later. tokenised_news_sample = news_sample_dataframe %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% filter(!word %in% c(&#39;superscript&#39;, &#39;style&#39;, &#39;de&#39;, &#39;thle&#39;, &#39;tile&#39;, &#39;tie&#39;, &#39;tire&#39;, &#39;tiie&#39;, &#39;tue&#39;)) %&gt;% filter(!str_detect(word, &#39;[0-9]{1,}&#39;)) %&gt;% filter(nchar(word) &gt; 2) ## Joining, by = &quot;word&quot; save(tokenised_news_sample, file = &#39;tokenised_news_sample&#39;) Now I can use all the tidyverse commands like filter, count, tally and so forth on the data, making it really easy to do basic analysis like word frequency counting. A couple of examples: The top words overall: tokenised_news_sample %&gt;% group_by(word) %&gt;% tally() %&gt;% arrange(desc(n)) ## # A tibble: 45,335 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 house 666 ## 2 country 621 ## 3 duke 577 ## 4 street 533 ## 5 march 515 ## 6 time 504 ## 7 clarke 497 ## 8 sir 495 ## 9 april 484 ## 10 army 470 ## # … with 45,325 more rows The top five words for each day in the dataset: tokenised_news_sample %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% arrange(full_date, desc(n)) %&gt;% group_by(full_date) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 90 x 3 ## # Groups: full_date [17] ## full_date word n ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 1809-01-01 jan 44 ## 2 1809-01-01 street 35 ## 3 1809-01-01 country 32 ## 4 1809-01-01 guildhall 27 ## 5 1809-01-01 madrid 27 ## 6 1809-01-08 army 43 ## 7 1809-01-08 enemy 36 ## 8 1809-01-08 time 31 ## 9 1809-01-08 lord 30 ## 10 1809-01-08 french 29 ## # … with 80 more rows If we had more than one title, we could look at the top words per title like this: tokenised_news_sample %&gt;% group_by(title, full_date, word) %&gt;% tally() %&gt;% arrange(full_date, desc(n)) %&gt;% group_by(full_date) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 90 x 4 ## # Groups: full_date [17] ## title full_date word n ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 0002644 1809-01-01 jan 44 ## 2 0002644 1809-01-01 street 35 ## 3 0002644 1809-01-01 country 32 ## 4 0002644 1809-01-01 guildhall 27 ## 5 0002644 1809-01-01 madrid 27 ## 6 0002644 1809-01-08 army 43 ## 7 0002644 1809-01-08 enemy 36 ## 8 0002644 1809-01-08 time 31 ## 9 0002644 1809-01-08 lord 30 ## 10 0002644 1809-01-08 french 29 ## # … with 80 more rows We could also summarise by month, using cut(). This rounds the date down to the nearest day, year or month. Once it’s been rounded down, we can count by this new value. tokenised_news_sample %&gt;% mutate(month = cut(full_date, &#39;month&#39;)) %&gt;% group_by(month, word) %&gt;% tally() %&gt;% arrange(month, desc(n)) %&gt;% group_by(month) %&gt;% top_n(5) ## Selecting by n ## # A tibble: 20 x 3 ## # Groups: month [4] ## month word n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 1809-01-01 army 199 ## 2 1809-01-01 jan 168 ## 3 1809-01-01 country 166 ## 4 1809-01-01 enemy 163 ## 5 1809-01-01 feb 153 ## 6 1809-02-01 clarke 300 ## 7 1809-02-01 duke 271 ## 8 1809-02-01 york 194 ## 9 1809-02-01 house 182 ## 10 1809-02-01 march 171 ## 11 1809-03-01 house 238 ## 12 1809-03-01 duke 220 ## 13 1809-03-01 march 216 ## 14 1809-03-01 april 204 ## 15 1809-03-01 clarke 188 ## 16 1809-04-01 april 200 ## 17 1809-04-01 country 157 ## 18 1809-04-01 lord 131 ## 19 1809-04-01 house 130 ## 20 1809-04-01 guildhall 120 We can also pipe everything directly to a plot. Enemy is a common word: I wonder how many times it was used in each day? Here we use filter() to filter out everything except the word (or words) we’re interested in. tokenised_news_sample %&gt;% filter(word == &#39;enemy&#39;) %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% ggplot() + geom_col(aes(x = full_date, y = n)) Charting a couple of words might be more interesting: tokenised_news_sample %&gt;% filter(word %in% c(&#39;enemy&#39;, &#39;france&#39;, &#39;spain&#39;)) %&gt;% group_by(full_date, word) %&gt;% tally() %&gt;% ggplot() + geom_line(aes(x = full_date, y = n, color = word)) issue_words = tokenised_news_sample %&gt;% count(full_date, word, sort = TRUE) total_words &lt;- issue_words %&gt;% group_by(full_date) %&gt;% summarize(total = sum(n)) issue_words &lt;- left_join(issue_words, total_words) ## Joining, by = &quot;full_date&quot; issue_words ## # A tibble: 97,885 x 4 ## full_date word n total ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1809-03-19 duke 129 14883 ## 2 1809-01-22 majesty 107 10493 ## 3 1809-02-12 clarke 103 10924 ## 4 1809-03-12 clarke 100 13465 ## 5 1809-03-19 evidence 100 14883 ## 6 1809-03-19 house 98 14883 ## 7 1809-02-19 duke 92 10440 ## 8 1809-02-19 clarke 88 10440 ## 9 1809-02-05 clarke 83 11337 ## 10 1809-03-19 clarke 80 14883 ## # … with 97,875 more rows issue_words %&gt;% ggplot() + geom_histogram(aes(x = n/total, fill = as.factor(full_date))) + facet_wrap(~full_date, scales = &#39;free&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. "],
["tf-idf.html", "10 Tf_idf:", " 10 Tf_idf: Another common analysis of text uses a metric known as ‘tf-idf’. This stands for term frequency-inverse document frequency. Take a corpus with a bunch of documents (here we’re using articles as individual documents). TF-idf scores the words in each document, normalised by how often they are found in the other documents. It’s a good measure of the ‘importance’ of a particular word for a given document, and it’s particularly useful in getting good search results from keywords. It’s also a way of understanding the way language is used in newspapers, and how it changed over time. The function in the tidytext library bind_tf_idf takes care of all this. First you need to get a frequency count for each issue in the dataframe. We’ll make a unique issue code by pasting together the date and the nlp into one string, using the function paste0. First load the necessary libraries and tokenised data we created in the last notebook: library(tidytext) library(tidyverse) library(rmarkdown) ## ## Attaching package: &#39;rmarkdown&#39; ## The following object is masked from &#39;package:future&#39;: ## ## run load(&#39;tokenised_news_sample&#39;) issue_words = tokenised_news_sample %&gt;% mutate(issue_code = paste0(title, full_date)) %&gt;% group_by(issue_code, word) %&gt;% tally() %&gt;% arrange(desc(n)) Next use bind_tf_idf() to calculate the measurement for each word in the dataframe. issue_words %&gt;% bind_tf_idf(word, issue_code, n) ## # A tibble: 97,885 x 6 ## # Groups: issue_code [17] ## issue_code word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00026441809-03-19 duke 129 0.00867 0 0 ## 2 00026441809-01-22 majesty 107 0.0102 0 0 ## 3 00026441809-02-12 clarke 103 0.00943 0.0606 0.000572 ## 4 00026441809-03-12 clarke 100 0.00743 0.0606 0.000450 ## 5 00026441809-03-19 evidence 100 0.00672 0 0 ## 6 00026441809-03-19 house 98 0.00658 0 0 ## 7 00026441809-02-19 duke 92 0.00881 0 0 ## 8 00026441809-02-19 clarke 88 0.00843 0.0606 0.000511 ## 9 00026441809-02-05 clarke 83 0.00732 0.0606 0.000444 ## 10 00026441809-03-19 clarke 80 0.00538 0.0606 0.000326 ## # … with 97,875 more rows Now we can sort it in descending order of the issue code, to find the most ‘unusual’ words: issue_words %&gt;% bind_tf_idf(word, issue_code, n) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 97,885 x 6 ## # Groups: issue_code [17] ## issue_code word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 00026441809-01-22 petticoat 14 0.00133 2.83 0.00378 ## 2 00026441809-02-12 deanery 11 0.00101 2.83 0.00285 ## 3 00026441809-02-19 dowler 17 0.00163 1.73 0.00282 ## 4 00026441809-04-09 prayer 21 0.00191 1.45 0.00277 ## 5 00026441809-02-19 shawe 10 0.000958 2.83 0.00271 ## 6 00026441809-02-19 kennett 12 0.00115 2.14 0.00246 ## 7 00026441809-01-15 yates 12 0.00114 2.14 0.00244 ## 8 00026441809-02-05 adam 31 0.00273 0.887 0.00243 ## 9 00026441809-01-01 jan 44 0.00454 0.531 0.00241 ## 10 00026441809-01-08 hodgson 15 0.00139 1.73 0.00240 ## # … with 97,875 more rows What does this tell us? Take the top word as an example. For some reason, it’s was very frequent in the issue for 22 January 1809, but not very frequent in any of the other issues. This might point to particular topics, and it might point, in particular, point to topics which had a very short lifespan. If we had a bigger dataset, or one arranged in another way, these words might point to linguistic differences between regions. Let’s find the petticoats articles. We can use a function called str_detect() with filter() to filter to just articles containing a given word. So we’ll go back to the untokenised dataframe. load(&#39;news_sample_dataframe&#39;) news_sample_dataframe %&gt;% filter(str_detect(text, &quot;petticoat&quot;)) ## # A tibble: 3 x 7 ## article_code art text title year date full_date ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 126 0030 LADIES&#39; DRESSES. HER. MAJE… 0002… 1809 0122 1809-01-22 ## 2 127 0031 PRINCESS AUGUSTA. Spanish … 0002… 1809 0122 1809-01-22 ## 3 140 &lt;NA&gt; THE NATIONAL REGISTER. JAN… 0002… 1809 0122 1809-01-22 In this case, there’s an article or two about women’s fashion. How about some other high-scoring terms? news_sample_dataframe %&gt;% filter(str_detect(text, &quot;prayer&quot;)) ## # A tibble: 9 x 7 ## article_code art text title year date full_date ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 25 0027 &quot;THE NATIONAL REGISTER. &#39;l… 0002… 1809 0101 1809-01-01 ## 2 27 0030 &quot;THE NATIONAL REGISTER. 13… 0002… 1809 0101 1809-01-01 ## 3 128 0034 &quot;6V Itispersonal appearance… 0002… 1809 0122 1809-01-22 ## 4 168 0035 &quot;THE NATIONAL, REGISTER. g… 0002… 1809 0129 1809-01-29 ## 5 317 0019 &quot;THE NATIONAL REGISTER. 13… 0002… 1809 0226 1809-02-26 ## 6 504 0035 &quot;ACCIDENT&#39;S, OFFENCES, Ec. … 0002… 1809 0402 1809-04-02 ## 7 529 0025 &quot;ebe pulpit. The Pulpit. … 0002… 1809 0409 1809-04-09 ## 8 530 0026 &quot;THE NATIONAL REGISTER, Co… 0002… 1809 0409 1809-04-09 ## 9 581 &lt;NA&gt; &quot;THE NATIONAL REGISTER. Is… 0002… 1809 0416 1809-04-16 "],
["sentiment-analysis.html", "11 Sentiment analysis 11.1 Install and load relevant packages 11.2 Fetch sentiment data 11.3 Load the tokenised news sample", " 11 Sentiment analysis A surprisingly easy text mining task, once your documents have been turned into a tokenised dataframe, is sentiment analysis. Sentiment analysis is the name for a range of techniques which attempt to measure emotion in a text. There are lots of ways of doing this, which become more and more sophisticated. One fairly simple but robust method is to take a dataset of words with corresponding sentiment scores (this could be a simple negative or positive score, or a score for each of a range of emotions). Then you join these scores to your tokenised dataframe, and count them. The tricky bit is working out what it all means: You could argue that it’s reductive to reduce a text to the sum of its positive and negative scores for each word - this is obviously not the way that language works. Also, if you’re summing the scores, you need to think about the unit you’re summarising by. Can you measure the emotions of a newspaper? or does it have to be per article? And of course it goes without saying that this was created by modern readers for use on modern text. Despite these questions, it can throw up some interesting patterns. Perhaps, if used correctly, one might be able to understand something of the way an event was reported, though it may not actually help with the ‘sentiment’ of the article, but rather reporting style or focus. I think with the right use, sentiment shows some promise when specifically applied to newspaper data, but thinking of it as sentiment may be a fool’s errand: it tells us something about the focus or style of an article, and over time and in bulk, something of a newspaper’s style or change in style. The tidytext library has a few built-in sentiment score datasets (or lexicons). To load them first install the textdata and tidytext packages, if they’re not installed already (using install.packages()) 11.1 Install and load relevant packages library(textdata) library(tidytext) library(tidyverse) 11.2 Fetch sentiment data Next use a function in the tidytext library called get_sentiments(). All this does is retrieve a dataset of sentiment scores and store them as a dataframe. There are four to choose from - I’ll quickly explain each one. 11.2.1 Afinn dataset afinnsentiments = get_sentiments(&#39;afinn&#39;) head(afinnsentiments,10) ## # A tibble: 10 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 The Afinn dataset has two colums: words in one column, and a value between -5 and +5 in the other. The value is a numeric score of the word’s perceived positivity or negativity. More information is available on the official project GitHub page 11.2.2 Bing dataset The second, the Bing dataset, was compiled by the researchers Minqing Hu and Bing Liu. It is also a list of words, with each classified as either positive or negative. bingsentiments = get_sentiments(&#39;bing&#39;) head(bingsentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative 11.2.3 Loughran dataset I’ve never used it, but it’s clearly similar to the Bing dataset, with a column of words and a classification of either negative or positive. More information and the original files can be found on the creator’s website loughransentiments = get_sentiments(&#39;loughran&#39;) head(loughransentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abandon negative ## 2 abandoned negative ## 3 abandoning negative ## 4 abandonment negative ## 5 abandonments negative ## 6 abandons negative ## 7 abdicated negative ## 8 abdicates negative ## 9 abdicating negative ## 10 abdication negative 11.2.4 NRC dataset nrcsentiments = get_sentiments(&#39;nrc&#39;) head(nrcsentiments,10) ## # A tibble: 10 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing. (https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) The NRC dataset is a bit different to the other ones. This time, there’s a list of words, and an emotion associated with that word. A word can have multiple entries, with different emotions attached to them. 11.3 Load the tokenised news sample load(&#39;tokenised_news_sample&#39;) This has two colums, ‘word’ and ‘value’. inner_join() will allow you to merge this with the tokenised dataframe. tokenised_news_sample %&gt;% inner_join(afinnsentiments) ## Joining, by = &quot;word&quot; ## # A tibble: 15,712 x 8 ## article_code art title year date full_date word value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 0001 0002644 1809 0101 1809-01-01 grateful 3 ## 2 1 0001 0002644 1809 0101 1809-01-01 encouragement 2 ## 3 1 0001 0002644 1809 0101 1809-01-01 encouragement 2 ## 4 1 0001 0002644 1809 0101 1809-01-01 promised 1 ## 5 1 0001 0002644 1809 0101 1809-01-01 increase 1 ## 6 1 0001 0002644 1809 0101 1809-01-01 negative -2 ## 7 1 0001 0002644 1809 0101 1809-01-01 worth 2 ## 8 1 0001 0002644 1809 0101 1809-01-01 gift 2 ## 9 1 0001 0002644 1809 0101 1809-01-01 inconvenience -2 ## 10 1 0001 0002644 1809 0101 1809-01-01 determined 2 ## # … with 15,702 more rows Now we have a list of all the words, one per line, which occurred in the afinn list, and their individual score. To make this in any way useful, we need to summarise the scores. The article seems by far the most logical start. We can get the average score for each article, which will tell us whether the article contained more positive or negative words. For this we use tally() and mean() I’m also using add_tally() to filter out only articles which contain at least 20 of these words from the lexicon, because I think it will make the score more meaningful. Let’s look at the most ‘positive’ article tokenised_news_sample %&gt;% inner_join(afinnsentiments) %&gt;% group_by(article_code) %&gt;% add_tally() %&gt;% filter(n&gt;20) %&gt;% tally(mean(value)) %&gt;% arrange(desc(n)) ## Joining, by = &quot;word&quot; ## # A tibble: 256 x 2 ## article_code n ## &lt;int&gt; &lt;dbl&gt; ## 1 262 1.39 ## 2 111 1.17 ## 3 16 1.11 ## 4 86 1.11 ## 5 85 1.06 ## 6 531 1.04 ## 7 52 0.971 ## 8 225 0.92 ## 9 563 0.877 ## 10 301 0.868 ## # … with 246 more rows Let’s take a look at the most ‘positive’ article. news_sample_dataframe %&gt;% filter(article_code == 262) %&gt;% pull(text) ## [1] &quot;THE NATIONAL REGISTER. the River St. Lawrence being frozen, could not probably be before June next. Much had been said on the advantages which this country would derive front the Embargo being raised with respect to England, and continued with res i t ...et to France. But this was a chimerical measure. The moment the French decrees were repealed, ours would he so too. But his Majesty had pledged himself not to seem to purchase the removal of the Embargo, by an act of yielding weakness beneath the dignity of this country. Lord GREN V ILLE having made an able reply, the House divided, when the members stood thus :—Ayes, 70—Noes, 115—Majority against the Motion, .15.—Adjourned. 1101 - STYLE=superscript SE OF COMMONS. Monda c y, Fe). 1 3. The Ilfonqe resolved into a Committee of Supply, in which the Bill for raking 1,500,000/. by Exchequer Bilk, and for paying ox 1,500.00 tn. oft he outstanding Exchequer Bills of last yii.ar, was read fur the first time. Tim Di; N.E OF YORK. Mr. MALTBY examined by the CHANCELLOR of the Ex- C E r R.. —Q. Are you acquainted with Mrs. Clarke? A. Yes; since July, 1806.—Q. flow did you become so acquainted with • her A. By means of Mr. Russel Manners, who was a Member of last Parliament, and who married my wife&#39;s sister. He in introduced troduced introduced me to Mrs. Clarice, whom he stated to have great in influence fluence influence with the Duke of York. I therefore went with him to call upon her. This might be three or four years since, about July or August 1506. Mr. Manners, by his relation Gen. Manners, Last a reg . imental account to settle, which could only be done through the Duke of York. He wished therefore to be well widl Mfrs. Clarke.—Q. What had you to do in this transaction? A. I had accommodated Mr. Manners with various sums of money, to the amouat of about one thousand pounds, in conse consequence quence consequence of which he a , STYLE=superscript signed over to me this regimental account, in which he was the creditor against government. I wished to get it settled if I could, and therefore kept well with Mrs. Clarke. —Q. What do you know of a Mr. Lodovick, or do you Ino w any thin; of this person? A. I understood that Mr. Lodo Lodovick vick Lodovick wanted a place, a Commissary&#39;s place, or something of that kind.—Q. Who seat Lodovick to you ?A. Mrs. Clarke.— She sent him to you to procure him a place? A. Yes, to ptit him in the way of And was any money deposited in consequence? A. Yes, 750/. —Q. Who were the agents to wham you were to have given this money, in the event ? A. Mr. Tindal was to have had the 1501. the other 6001. I don&#39;t know who was to have had, except Mr. Lloyd.— Q. And what were you to get if the appointment took place? A. No Nothing thing Nothing at You had no other purpose than merely to oblige. Mrs. Clarke? A. No, I had no other purpose.—Q. Had you ever represented yourself to Z? rs . Clarke as having any connec connections, tions, connections, as knowing any person, through whom you might procure any appointment? A. Never, I am sure of . STYLE=superscript Did Mr!. Clarke ever apply to you on any other cases of this kind? A. Yes, for a gentleman of the name of Thomson, a relation of per&#39;s. Q. When were all these applications made? A. Since Septem September ber September last.---Q. Did you not know that she had separated from the Duke at that time ? A. No; she never told me that she had. —Q. Do you not know that Mrs. Clarke used to represent you as the Duke of Portland, used to say that you were her Duke of Portland ? A. I never knew it till I heard so the other day. Lard ro Lts sTo N e.—Q. When you found that Mrs. Clarke had so little interest to obtain the situations, for which she received your assistance, how came you to imagine that she had sufficient interest with the Duke of York to obtain your object I believed she had still considerable influence over the Duke, and was under his protection.—Q. How many interviews had you, on various occasions, with Mr. Tin Tindal dal Tindal ? A. I cannot tell, but a great many.—Q. In any one of those interviews did you a,ic Mr. Tindal through what channel he teas to obtain those appointments ?—A. On one occasion, that of Mr. Lodovick&#39;s, I asked Mr. Tindal through what me medium dium medium he was likely to obtain the appointments ; and he an anowered owered anowered me thrcugh the Wellesiev interest, but he did not men mention tion mention any particular person.—Q. What first led you to \\\\lr. Tin Tindal dal Tindal ? A. I had been acquainted with him as a kind of agent, and understood he could obtain appointments under government. I was about to state, that I had understood Mr. Manners had received a letter from the Duke of York., in which he was pro nited a place suitable to his name and family. Q. Did that bare had lepoken to the Her. Clarke, in your hearing, ray !B. 19, Duke of York on the affairs of Mr. Manners ? A. Did she ever mention Mr. Manners&#39;s business to ion ? A. I do not re recollect.—Q. collect.—Q. recollect.—Q. Did you transact the business fur Mrs. Clarke gratuitously, or did you expect to derive any emolument ? A. I hoped to get my account settled.—Q. How long is it since you have given up the hopes of the settlement ? A. About a mouth. Mrs. Clarke was then called in, who produced a packet of letters, which she said she had received from Mr. Maltby, which she presumed to he his hand-writing, from his name being signed to them all. She also produced two letters from Mr. Barber, whose hand-writing she. also professed a•knowledge of. She was then asked if she had any other letters or papers replied she had; three from Col. M`Mahott. Lord FOLKSTON E asked her if she had any other papers to produce ? A. Yes; she then presented a letter from the Arch Archbishop bishop Archbishop Tuam, in recommendation of Dr. O&#39;Mara. This letter, she stated, was not directed to her, but to the Doctor him. self. She had also a letter from the Duke of York, on the subject of General Claveriag ; and the Duke of York&#39;s answer to her respecting Dr. O&#39;Mara; and a letter from Cu!. Shaw, dated from the Downs, previous to his - STYLE=superscript sailing fur the Cape of Good Hope, and soliciting to be placed on the half-pay list. Q. Have you seen the Duke of York write, and is the letter now •hewn to you his writiig;—A. Yes, I have seen his Royal Highness write. This letter to George Farquhar is his hand handwriting. writing. handwriting. Every letter his Royal Highness addressed to me, the superscription was in a fictitious hand, and directed to Mrs. Clarke, at the Post-Office, Worthing, to be left till called for. The inside of those letters were in his Highness&#39;s usual hand.— Q. How did you come to the knowledge of the Archbishop of Tuain&#39;s hand-writing T. A. I found the letter by accident among, the documents of Dr. O&#39;Mara. Here Mrs. Clarke was ordered to withdraw, when several letters of Mr. Maltby were real by the Clerk. They related to appointments which Mrs. Clarke undertook to obtain for dttkrent officers, urging her to persevere in their behalf K itb the Duke of York. The remainder of Mr. Malthy&#39;s letters had reference to Mr. Barber and Mr. Lloyd, in tire procuration of a situation in the Commissariat Department, for Mr. Lodovick. Two or three letters from Col. M`Mahon to Mrs. Clarke were then read. The laat of Colonel 11‘11ahon&#39;s letters declared, his total in inability ability inability to serve Mrs. Clarke ; but, at the same time, he ex expressed pressed expressed the high sense he entertained of her manners and conduct, which demanded nothing but his respect, and an assurance of the good wishes be entertained for her. AUTHENTIC LETTERS, FROM THAI DUE OF ronx TO MRS. CLIRNR. August 4, 1805. How can I sufficiently express to my sweetest, my darling Love, the delight which her dear, her pretty letter gave me, or how much I feel all the kind things she says to me in it ! millions and millions of thanks for it, my Angel ! and be assured that my heart is fully sensible of your affection, and that upon it alone its whole happiness depends. I am, however, quite hurt that my Love did not go to the Lewes Races; how kind of her to think of me upon the occasion ; but I trust that she knows me too well not to be convinced that I cannot bear the idea of adding to those sacrifices which I um but too sensible that she has made to me. News, my Angel cannot expect from me from hence ; though the life led here, at least in the family I am in, is very hurrying, there is a sameness in it which affords little subject for a letter; except Lord Chesterfield&#39;s family, there is not a single person except ourselves that I know. Last night, we were at the play which went off better than the first night. Dr. O&#39;Meara called upon me yesterday morning, and delivered me your letter ; he wishes much to preach before Royalty, and if I can put him in the way of it I will. What a time it appears to sae al already, ready, already, my darling, since we parted ; How impatiently 1 look forward to next Wednesday se nnight ! God bless you, my own dear, dear Love ! I shall miss the post if I add more; Oh believe me ever, to my last hour, your&#39;s and your&#39;s Addressed : Mrs. Clarke, to be left at the Post-Oeice, Worthing. 66 STYLE=superscript Sandgate, 24, 1804. How can I sufficiently express to my darling love my thanks for her dear, dear letter, or the . STYLE=superscript delight which the assurances of her love give me ? Oh, my Angel ! do me justice and be con convinced vinced convinced that there never was a woman adored as you are. Every day, every hour convinces me more and more that my whol happiness depends upon you alone. What a time it appears t o STYLE=superscript bu since we pitrted, and with what Impatience do I look ft,r&quot; news_sample_dataframe %&gt;% filter(article_code == 482) %&gt;% pull(text) ## [1] &quot;sent separate article, and have sealed, it with the seals of our arms. Donent London this 14th day of January, 1809. (L.S.) GEORGE CANNING.: Art. 11. Separate. A treaty shall forthwith be negociatNl, stipulating the amount and deseriptioir of succours to be afforded by his Britannic Majesty, agreeably to the third article of the present treaty. In witness w hereof, we, the under - STYLE=superscript signed Plenipotentiaries, have signed, in virtue of our respectite full powers, the present teparatearticle, mid have sealed it with the seals of our arms. Done at London this 11th day of January, 1809. (t.. S.) GEORGI; CANNING. Additional Art.--I . STYLE=superscript llc present circumstances not admitting of the regular negotiation ef a treaty of commerce between the two countries, with all the care and consideration tine to so im important portant important a subjt-et, the. high contracting parties mutually engage to proceed to such negotiation as soon as it shall be practicable so to do, affording, in t he mean time, mutual fat ilitics to the commerce of the subjects of each other, by temporary regula regulations tions regulations founded on principles of reciprocal utility. In witne. , STYLE=superscript s whereof, we, the undersigned Plenipotentiaries,. having sig ned, in virtue, of our respective full powers, the. present ad ditional article, and have sealed it with the seals of our arms. Dune at London this 21st day of March, ISM. (L. S.) GEORGE CANNING. TUESDAY&#39;S LOXDOY GAZETTE., Downing-street, .March &#39;27, 1809. Dispatches, of which the following are copies, have this day Invo received from Lieut -Gen- Beckwith, Commander of his Majestyerforces in the Leeward Islands, addressed to Viscount Castlereagh, one of his Majesty&#39;s Principal Secretaries of State. LThe first dispatch, dated Feb. 1, announces the landing of the , STYLE=superscript t,ops tho.ithh of January, in two divisions. The second coot: s nr , STYLE=superscript :lecount of the taking of t he heights of Surirey, 014 eon island, and other operations, which we have already minutely detailed from the Barbadoes Papers.] Camp, lhights of Surirey, Martinique, Feb. 10, 1809. llv Lo kb—Having, in my communications of the lst and sth submitted to your Lordship&#39;s consideration general reports orate operations of the army I have the !loliour to command, I now beg leave to inclose the special reports of the General Offi Officers cers Officers commanding divisions, and of Brigwiier-General Iloghton, y:hose brigade wac in action upon the Ist. with separate returns t4f our loss upon the Ist and 2d, which, I am inclined to believe. terminate our operations in th e field. The lower fort, formerly Fort Edward, was taken pOSSCSSiOn of before day-break in the morning of the bill, by Major Ilen Ileniler*on, iler*on, Ileniler*on, commanding the Royal York Rangers, with that regi regiownt, ownt, regiownt, tvit hoot istance, and we now occupy that work. St. Pierre surrendered to Lieutenant-Colonel Barne , STYLE=superscript ,, of the -I;ith regiment, the day before yesterday, and I have not yet re reved ved reved the details. Jut he course of all these services, where the co-operation ►►t&#39; Cie navy wa practicable, the greatest exertions have been rts;v!e 11% the Rear-Admiral ; and the iinportant advantages ret,ot.i . STYLE=superscript til e&#39;u shore by that excellent (dicer Cotan►odore Cm:idiom, in the rAttet ion of Pigeon Island, and tht. landing eantoot, mortars, and .cmuunition, at Point Negroe, aid conveying titttn to the sever; bit teries on that side, hare been of the highest importance to t:►c King&#39;s service. _ I have, &amp;amp;c. (co. BECKWITH ) COM. Forces. To Lord Viscount Castlereagh, .i&#39;r. [The special reports which fol:ow, by Generals Prevost, lloghtott, and Maitland, give an account of the operations in V.:licit they were respectively engage;:, but which have all been already fully described in our Paper.] R !tarn of killed, wounded, and rni:;.7;ns , STYLE=superscript of the DirLionnndcr the Command of Lieut.-Geprot Sir George Peccost, Lail. in the miters of thelst cf Feb. I 809. Zth or Ito:.111 Fusileers, Int battalion, 1 captain, 9 rank and i killed; 2 seijoants, 1 drummer, 56 rank and tile, wounded; r.ink and tile, mi..ising.-23:1 or It oyal leers, I st Laalion ) 1 serjeant ) I-1 rank and We, kizlcd ; zedeantt, &#39;Z9 rank and file, wounded; h serjeant, 5 rank and tile, missing,— Detachment of the Ist West India Regiment, 1 rank and file t killed; 1 drummer, 18 rank and file, wounded; . STYLE=superscript 1 subaltern, ta:•sing.—Light battalion, 11 rank and file, killed ; 1 captain, 2 Subalterns, 1- serjeant, 2 buglei, SO rank and file, wounded; 3 rank and file, missing.- 7 -Total : 1 captain, 1 serjeant, 35 rank and file, killed ; 1 captain,.2 subalterns, 5 serjeants, 4 drummers, 2 bugles, 183 rank and tile, Wounded ; 1 subaltern, 1 serjeant, 12 rank. and file, missing. Officer killed.—Capt. Taylor, of the Royal Tusileers, Acting Deputy Quarter-Master-General. Officers Wounded.—Captain Gledstanes, of the 3d West India light infantry; Lieut. Johnson of the 4th ditto; Lieut. Jackson, of the rifle company Royal West India Rangers. Officer Missing.—Lieut. Gilmour, of the Ist West India Regi Regi, , Regi, went, taken prisoner.• (Signed) A. LIGERTw 00D. Acting Deputy Adjutant-General to the Forces under the Command of Lieut..-Gen. Sir G. Prevost. Return of killed. wounded, and missing, of the First Division of the it ring upon the ileight of Surirey,-Peb. 2,181.9. 7th Royal Fusileers, Ist battalion, 1 serjeant, 20 rank and tile, killed ; 1 field-officer, 2 eaptaita;, 1 serjeant, 1 drummer, 58 rank and file, w ounded ;• 3 rank and tile, missing. -Bth King&#39;s Regi Regiment, ment, Regiment, Ist battalion. 1 field-otficer, 4 rank and file, killed; 13 rank and file, wounded. —23 cl Royal Welch Fusileers,. Ist bat battalion, talion, battalion, 1 serjeant, 3 rank and file, killed ; 1 subaltern, 1 staff, Ifa rank and file, wounded . STYLE=superscript ; 1 serjeant, inissing.—Dtstachmein, 13t West India Regintzat, 1. rank and tile, killed.—Light battalion, 1 captain, 1 serjeant, 14 rank and file, killed; .1 field-officer, 1 subaltetn,2 serjeants,36 rank aed file, wounded.—Total ; I field fieldofficer, officer, fieldofficer, 1 captain, 3 serji!ant!z, 42 rank and file, killed; 2 field• officers, 2 captains, 23ubalterns, I staff, 3 serjeants, 1 drummer, 126 rank and file, w minded; 1 serjeant, 3 rank and file, missing. Officers Killed.— kith Regiment, Major Maxwell, light bat battalion; talion; battalion; Capt. Sinclair, 23th regiment. Officers . Wounded.-7th Regiment, Hon. Lieut.-Colonel Pakenhant; Captains Rowe and Cholwick.-23d Regiment, Lieut. Roskelly; Surgeon Power.—Light battalion, Major Campbell of the Royal West India Rangers; Lieut. Hopwood of ditto.—Staff, Captain Coore, Aide-de-Camp to Lieut.-Gen. Sir G. Prevost, slightly. G. W. R a Stsi Y., Brig. and Adj.-Gen. A Letter from Rear-Ad►niral Sir Alexander Cochrane gives an account of the lauding of the troops under Lieut.-General Beckwith, at Martinique, and incloses a letter from Capt. Beaver, of the Acasta, respecting the dbembarkatiou of the troops under his charge at Bay Robert. Copy of a Letter to Rear-Admiral Sir Alex. Cochrane. Cleopatra, oft&#39; Basseterre, Guadaloupc, Jan 23, 1801. Slit, In consequence ofseparatiag from hi= Majesty&#39;s ship J ason, and there being no probability of communication either with Capt. Mande, or Capt. Pigot of the &#39;,atona l and senior officer of the blockading squadron, I beg to inform you, that yester yesterday, day, yesterday, in obedience to the signals made to me by Capt. Maude, I chaced a ship in the N. N. W. which I shortly afterwards made out to be a French frigate, who, on seeing us, hauled close in shore, and anchored under a small battery a little to the S. of Point Noir. Having ascertained that they were securing her (by springs on her cables, and others fast to the trees on shore) as well as her situation would permit, I made every preparation for attacking her, the witid being at this time from the s. but very light aid variable. At half past 2P.M. we got the true breeze, and turned up to windward till within a ca cable&#39;s ble&#39;s cable&#39;s lenOt oi . STYLE=superscript the shore, and half musquet shot from the ene ene, , ene, « h ich was eiThrted at :t o&#39;clock, when his firing commenced. taw from the shape of the land aml the shoal of water between us, that I could not close without danger of being raked, I was therefore obliged to anchor in six fathoms and a half, and re returned turned returned his tile, which fortunately cut away his outside spring, when he swtmg in shore with his head towards us, giving us the advantage I refosed hint before ; this 1 so effectually preserved, that he never afterwards got more than. half his broadside to hear; we thus engaged for 40 minutes, when the Jason and Ha Hazard zard Hazard came up, the former having taken a position on her star starboard board starboard quarter, and firing her bow guns, the Hazard at the same time directing hers to the fort, the enemy hauled down his colours, finding he was not able to sustain so unequal a combat. She proves to be the French national frigate Topaze, carrying 48 guns, eighteen 24 and 56-pounders, commanded by MOM, La Lahall hall Lahall e, Capitnine dc frigate, «ith a complement of 330 men; she P6_TB00016 . Zl4 P6_TB00017 TrrliE NATIONAL REGISTER. APRIL 2.&quot; The most negative is perhaps more relevant: it includes a casualty list, and so repeats words like ‘shot’ and ‘wounded’ and so forth. Sentiment analysis should be uesd with caution, but it’s potentially a useful tool, particularly to look at changes over time, or differences between newspapers or authors. We can plot the average of all the average article scores. If we had them, this could be segmented by title. tokenised_news_sample %&gt;% inner_join(afinnsentiments) %&gt;% group_by(full_date,article_code) %&gt;% add_tally() %&gt;% filter(n&gt;20) %&gt;% tally(mean(value)) %&gt;% group_by(full_date) %&gt;% tally(mean(n)) %&gt;% arrange(desc(n)) %&gt;% ggplot() + geom_col(aes(x = full_date, y = n)) ## Joining, by = &quot;word&quot; "],
["topic-modelling.html", "12 Topic modelling", " 12 Topic modelling Another text mining tool which can be useful in exploring newspaper data is topic modelling. Topic modelling tried to discern a bunch of ‘topics’, expressed as a set of important keywords, from a group of documents. It works best if each document covers one clear subject. Load relevant libraries. If you’ve loaded the notebooks into Rstudio, it should detect and ask if you want to install any missing ones, but you might need to use install_packages() if not. library(tidyverse) library(tidytext) library(topicmodels) Load the tokenised dataframe created in the term_frequency notebook. load(&#39;tokenised_news_sample&#39;) Like in the tf_idf notebook, make a dataframe of the words in each document, with the count and the tf-idf score. issue_words = tokenised_news_sample %&gt;% mutate(issue_code = paste0(title, full_date)) %&gt;% group_by(issue_code, word) %&gt;% tally() %&gt;% arrange(desc(n)) issue_words = issue_words %&gt;% bind_tf_idf(word, issue_code, n) Using the function cast_dtm() from the topicmodels package, make a document term matrix. This is a matrix with all the documents on one axis, all the words on the other, and the number of times that word appears as the value. dtm_long &lt;- issue_words %&gt;% filter(tf_idf &gt; 0.00006) %&gt;% filter(n&gt;5) %&gt;% cast_dtm(issue_code, word, n) lda_model_long_1 &lt;- LDA(dtm_long,k = 8, control = list(seed = 1234)) result &lt;- tidytext::tidy(lda_model_long_1, &#39;beta&#39;) result %&gt;% group_by(topic) %&gt;% top_n(12, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 4) + coord_flip() "],
["detecting-text-reuse-in-newspaper-articles-.html", "13 Detecting text reuse in newspaper articles. 13.1 Turn the newspaper sample into a bunch of text documents, one per article 13.2 Load the files as a TextReuseCorpus 13.3 Further reading", " 13 Detecting text reuse in newspaper articles. 19th century newspapers shared text all the time. Sometimes this took the form of credited reports from other titles. For much of the century, newspapers paid the post office to give them a copy of all other titles. Official reused dispatches were not the only way text was reused: advertisements, of course, were placed in multiple titles at the same time, and editors were happy to use snippets, jokes, and so forth Detecting the extent of this reuse is a great use of digital tools. R has a library, textreuse, which allows you to do this reasonably simply. It was intended to be used for plagiarism detection and to find duplicated documents, but it can also be repurposed to find shared articles. Some of the most inspiring news data projects at the moment are looking at text reuse. The Oceanic Exchanges project is a multi-partner project using various methods to detect this overlap. This methods paper is really interesting, and used a similar starting point, though it then does an extra step of calculating ‘local alignment’ with each candidate pair, to improve the accuracy.15 Melodee Beals’s Scissors and Paste project, at Loughborough and also part of Oceanic Exchanges, also looks at text reuse in 19th century British newspapers. Another project, looking at Finnish newspapers, used a technique usually used to detect protein strains to find clusters of text reuse on particularly inaccurate OCR.16 The steps are the following: Turn the newspaper sample into a bunch of text documents, one per article Load these into R as a special forat called a TextReuseCorpus. Divide the text into a series of overlapping sequences of words, known as n-grams. ‘Hash’ the n-grams - each one is given a numerical code, which is much less memory-hungry. Randomly select 200 of these hashes to represent each document. Use a local sensitivity hashing algorithm (I’ll explain a bit below) to generate a list of potential candidates for text reuse Calculate the similarity scores for these candidates Calculate the local alignment of the pairs to find out exactly which bits overlap To set some expectations: this tutorial uses a small sample dataset of one title over a period of months, and unsurprisingly, there’s not really any text re-use. A larger corpus over a short time period, with a number of titles, would probably give more interesting results. Also, these techniques were developed with modern text in mind, and so the results will be limited by the accurary of the OCR, but by setting the parameters reasonably loose we might be able to mitigate for this. http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html http://infolab.stanford.edu/~ullman/mmds/ch3.pdf 13.1 Turn the newspaper sample into a bunch of text documents, one per article Load libaries: the usual suspect, tidyverse, and also the package ‘textreuse’. If it’s not installed, you’ll need to do so using install.packages('textreuse') library(tidyverse) library(textreuse) ## ## Attaching package: &#39;textreuse&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## tokenize 13.1.1 Load the dataframe and preprocess In the extract text chapter ??, you created a dataframe, with one row per article. The first step is to reload that dataframe into memory, and do some minor preprocessing. load(&#39;news_sample_dataframe&#39;) Make a more useful code to use as an article ID. First use str_pad() to add leading zeros up to a maximum of three digits. news_sample_dataframe$article_code = str_pad(news_sample_dataframe$article_code, width = 3, pad = &#39;0&#39;) Use paste0() to add the prefix ‘article’ to this number. news_sample_dataframe$article_code = paste0(&#39;article_&#39;,news_sample_dataframe$article_code) 13.1.2 Make a text file from each article This is a very simple function - it says, for each row in the news_sample_dataframe, write the third cell (which is where the text of the article is stored), using a function from a library called data.table called fwrite(), store it in a folder called textfiles/, and make a filename from the article code concatenated with ‘.txt’. Now you should have a folder in the project folder called textfiles, with 600 or so text documents inside. library(data.table) for(i in 1:nrow(news_sample_dataframe)){ filename = paste0(&quot;textfiles/&quot;, news_sample_dataframe[i,1],&quot;.txt&quot;) fwrite(news_sample_dataframe[i,3], file = filename) } 13.2 Load the files as a TextReuseCorpus 13.2.1 Generate a minhash Use the function minhash_generator() to specify the number of minhashes you want to represent each document. Set the random seed to make it reproducible. minhash &lt;- minhash_generator(n = 400, seed = 1234) 13.2.2 Create the TextReuseCorpus TextReuseCorpus() takes a number of arguments. Going through each in turn: dir = is the directory where all the text files are stored. tokenizer is the function which tokenises the text. Here we’ve used tokenize_ngrams, but it could also be tokenize words. You could build your own: for example, if you thought that comparing similar characters in small sequences would help to detect text reuse, you could use that to compare the documents. n is the number of tokens in the ngram tokeniser. Setting it at 4 turns the following sentence: Here we’ve used tokenize_ngrams, but it could also be tokenize words into: Here we’ve used tokenize_ngrams we’ve used tokenize_ngrams but used tokenize_ngrams but it tokenize_ngrams but it could but it could also it could also be could also be tokenize also be tokenize words minhash_func = is the parameters set using minhash_generator() above keep_tokens = Whether or not you keep the actual tokens, or just the hashes. There’s no real point keeping the tokens as we use the hashes to make the comparisons. reusecorpus &lt;- TextReuseCorpus(dir = &quot;textfiles/&quot;, tokenizer = tokenize_ngrams, n = 3, minhash_func = minhash, keep_tokens = FALSE, progress = FALSE) Now each document is represented by a series of hashes, which are substitutes for small sequences of text. For example, this is the first ten minhashes for the first article: head(minhashes(reusecorpus[[1]]),10) ## [1] -2147136494 -2138512326 -2147425767 -2143106471 -2135745958 -2141828729 ## [7] -2145358196 -2137240488 -2143623091 -2142163990 At this point, you could compare any document’s sequences of hashes to any other, and get its ‘Jacquard Similarity’ score, which counts the number of shared hashes in the documents. The more shared hashes, the higher the similarity. However, it would be very difficult, even for a computer, to use this to compare every document to every other in a corpus. A Local Sensitivity Hashing algorithm is used to solve this problem. This groups the representations together, and finds pairs of documents that should be compared for similarity. LSH breaks the minhashes into a series of bands comprised of rows. For example, 200 minhashes might broken into 50 bands of 4 rows each. Each band is hashed to a bucket. If two documents have the exact same minhashes in a band, they will be hashed to the same bucket, and so will be considered candidate pairs. Each pair of documents has as many chances to be considered a candidate as their are bands, and the fewer rows there are in each band, the more likely it is that each document will match another. (https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html) First create the buckets. You can try other values for the bands. buckets &lt;- lsh(reusecorpus, bands = 80, progress = FALSE) Next, use lsh_candidates() to compare each bucket, and generate a list of candidates. candidates &lt;- lsh_candidates(buckets) Next we go back to the full corpus, and calculate the similarity score for these pairs, using lsh_compare(). The first argument is the candidates, the second is the full corpus, the third is the method (other similarity functions could be used). jacsimilarity_both = lsh_compare(candidates, reusecorpus, jaccard_similarity, progress = FALSE) %&gt;% arrange(desc(score)) jacsimilarity_both ## # A tibble: 56 x 3 ## a b score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 article_017 article_342 1 ## 2 article_019 article_043 1 ## 3 article_019 article_068 1 ## 4 article_019 article_395 1 ## 5 article_019 article_400 1 ## 6 article_019 article_461 1 ## 7 article_019 article_463 1 ## 8 article_019 article_513 1 ## 9 article_019 article_603 1 ## 10 article_043 article_068 1 ## # … with 46 more rows It returns a similarity score for each pair: The first pair have a 25% overlap, and the second a much smaller number. The last thing is to join up the article codes to the full text dataset, and actually see what pairs have been detected. This is done using two left_join() commands, one for a and one for b. Also select just the relevant columns. matchedtexts = jacsimilarity_both %&gt;% left_join(news_sample_dataframe, by = c(&#39;a&#39; = &#39;article_code&#39;)) %&gt;% left_join(news_sample_dataframe, by = c(&#39;b&#39; = &#39;article_code&#39;))%&gt;% select(a,b,score, text.x, text.y) matchedtexts ## # A tibble: 56 x 5 ## a b score text.x text.y ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 article_017 article_342 1 TIIE NATIONAL REGIST… TIIE NATIONAL REGIST… ## 2 article_019 article_043 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 3 article_019 article_068 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 4 article_019 article_395 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 5 article_019 article_400 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 6 article_019 article_461 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 7 article_019 article_463 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 8 article_019 article_513 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 9 article_019 article_603 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## 10 article_043 article_068 1 THE NATIONAL REGISTE… THE NATIONAL REGISTE… ## # … with 46 more rows Well, the first is just the newspaper title, with a couple of extra words, so that’s no use. How about the second pair? We can use another function from textreuse to check the ‘local alignment’. This is like comparing two documents in Microsoft Word: it finds the bit of the text with the most overlap, and it points out where in this overlap there are different words, replacing them with ###### First turn the text in each cell into a string: a = paste(matchedtexts$text.x[2], sep=&quot;&quot;, collapse=&quot;&quot;) b = paste(matchedtexts$text.y[2], sep=&quot;&quot;, collapse=&quot;&quot;) Call the align_local() function, giving it the two strings to compare. align_local(a, b) ## TextReuse alignment ## Alignment score: 6 ## Document A: ## THE NATIONAL REGISTER ## ## Document B: ## THE NATIONAL REGISTER Looks like we found a repeated advert for a series of portraits! This is very much a beginning, but I hope you can see the potential. It’s worth noting that the article segmentation in these newspapers might actually work against the process, because it often lumps multiple articles into one document. Consequently, the software won’t find potential matches if there’s too much other non-matching same text in the same document. A potential work-around would be to split the document into chunks of text, and compare these chunks. The chunks could be joined back to the full articles, and using local alignment, the specific bits that overlapped could be found. 13.3 Further reading David A. Smith, Ryan Cordell, and Abby Mullen, ‘Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers’, American Literary History, 27.3 (2015), E1–E15 &lt;https://doi.org/10.1093/alh/ajv029&gt;.↩ ???, @inproceedings-blast.↩ "],
["further-reading-1.html", "14 Further reading", " 14 Further reading For now this is just a list of useful stuff: https://scottbot.net/teaching-yourself-to-code-in-dh/ http://dh-r.lincolnmullen.com https://bookdown.org/robinlovelace/geocompr/ https://bookdown.org/yihui/blogdown/ References to all the R cheat sheets https://www.tidytextmining.com https://r4ds.had.co.nz "]
]
