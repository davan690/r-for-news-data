<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13 Detecting text reuse in newspaper articles. | Demystifying Newspaper Data with R (and a tiny bit of Python)</title>
  <meta name="description" content="This is a handbook to help new and existing users find, process and analyse historical newspaper data, using the programming language R, and its IDE R-Studio" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="13 Detecting text reuse in newspaper articles. | Demystifying Newspaper Data with R (and a tiny bit of Python)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a handbook to help new and existing users find, process and analyse historical newspaper data, using the programming language R, and its IDE R-Studio" />
  <meta name="github-repo" content="yannryanBL/r-for-news-data" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 Detecting text reuse in newspaper articles. | Demystifying Newspaper Data with R (and a tiny bit of Python)" />
  
  <meta name="twitter:description" content="This is a handbook to help new and existing users find, process and analyse historical newspaper data, using the programming language R, and its IDE R-Studio" />
  

<meta name="author" content="Yann Ryan" />


<meta name="date" content="2020-02-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="topic-modelling.html"/>
<link rel="next" href="further-reading-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface &amp; Acknowledgements</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#unlocking-the-past-with-newspaper-data"><i class="fa fa-check"></i><b>2.1</b> Unlocking the past with newspaper data</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#why-is-it-useful"><i class="fa fa-check"></i><b>2.2</b> Why is it useful?</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>2.3</b> Goals</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>2.4</b> Why R?</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#who-is-the-book-for"><i class="fa fa-check"></i><b>2.5</b> Who is the book for?</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#format-of-the-book"><i class="fa fa-check"></i><b>2.6</b> Format of the book</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html"><i class="fa fa-check"></i><b>3</b> UK Newspaper Data</a><ul>
<li class="chapter" data-level="3.1" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#intro-to-british-library-newspapers"><i class="fa fa-check"></i><b>3.1</b> Intro to British Library Newspapers</a></li>
<li class="chapter" data-level="3.2" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#burney-collection"><i class="fa fa-check"></i><b>3.2</b> Burney Collection</a></li>
<li class="chapter" data-level="3.3" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#jisc-newspaper-digitisation-projects"><i class="fa fa-check"></i><b>3.3</b> JISC Newspaper digitisation projects</a></li>
<li class="chapter" data-level="3.4" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#british-newspaper-archive"><i class="fa fa-check"></i><b>3.4</b> BRITISH NEWSPAPER ARCHIVE</a></li>
<li class="chapter" data-level="3.5" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#hmd-data--on-repository"><i class="fa fa-check"></i><b>3.5</b> HMD data- on repository</a></li>
<li class="chapter" data-level="3.6" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#what-access-do-you-need"><i class="fa fa-check"></i><b>3.6</b> What access do you need?</a><ul>
<li class="chapter" data-level="3.6.1" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#want-to-find-individual-articles."><i class="fa fa-check"></i><b>3.6.1</b> Want to find individual articles.</a></li>
<li class="chapter" data-level="3.6.2" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#want-to-do-text-mining-on-a-large-corpus"><i class="fa fa-check"></i><b>3.6.2</b> Want to do text mining on a large corpus</a></li>
<li class="chapter" data-level="3.6.3" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#want-to-do-text-mining-on-the-entire-digitised-collection"><i class="fa fa-check"></i><b>3.6.3</b> Want to do text mining on the entire digitised collection</a></li>
<li class="chapter" data-level="3.6.4" data-path="uk-newspaper-data.html"><a href="uk-newspaper-data.html#want-to-do-something-involving-the-images-such-as-computer-vision-techniques"><i class="fa fa-check"></i><b>3.6.4</b> Want to do something involving the images, such as computer vision techniques,</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html"><i class="fa fa-check"></i><b>4</b> OCR and its problems</a><ul>
<li class="chapter" data-level="4.1" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#what-is-ocr"><i class="fa fa-check"></i><b>4.1</b> What is OCR?</a></li>
<li class="chapter" data-level="4.2" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#what-is-it-like-in-bl-newspapers"><i class="fa fa-check"></i><b>4.2</b> What is it like in BL newspapers?</a></li>
<li class="chapter" data-level="4.3" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#introduction-1"><i class="fa fa-check"></i><b>4.3</b> Introduction</a></li>
<li class="chapter" data-level="4.4" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#extract-predicted-word-scores-from-the-alto-pages"><i class="fa fa-check"></i><b>4.4</b> Extract predicted word scores from the ALTO pages</a></li>
<li class="chapter" data-level="4.5" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#visualisations"><i class="fa fa-check"></i><b>4.5</b> Visualisations:</a><ul>
<li class="chapter" data-level="4.5.1" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#whats-in-the-data"><i class="fa fa-check"></i><b>4.5.1</b> What’s in the data?</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#highest-and-lowest-results"><i class="fa fa-check"></i><b>4.6</b> Highest and lowest results:</a></li>
<li class="chapter" data-level="4.7" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#page-by-page-ocr-visualisation"><i class="fa fa-check"></i><b>4.7</b> Page-by-page OCR visualisation</a></li>
<li class="chapter" data-level="4.8" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#microfilm-vs-print"><i class="fa fa-check"></i><b>4.8</b> Microfilm vs print:</a></li>
<li class="chapter" data-level="4.9" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#conclusions"><i class="fa fa-check"></i><b>4.9</b> Conclusions</a></li>
<li class="chapter" data-level="4.10" data-path="ocr-and-its-problems.html"><a href="ocr-and-its-problems.html#impact-on-analysis"><i class="fa fa-check"></i><b>4.10</b> Impact on analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html"><i class="fa fa-check"></i><b>5</b> Quick introduction to R and the tidyverse</a><ul>
<li class="chapter" data-level="5.1" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#what-and-why"><i class="fa fa-check"></i><b>5.1</b> What and why?</a></li>
<li class="chapter" data-level="5.2" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#using-r"><i class="fa fa-check"></i><b>5.2</b> Using R</a><ul>
<li class="chapter" data-level="5.2.1" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#base-r-commands"><i class="fa fa-check"></i><b>5.2.1</b> Base R commands</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#tidyverse"><i class="fa fa-check"></i><b>5.3</b> Tidyverse</a><ul>
<li class="chapter" data-level="5.3.1" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#select-pull"><i class="fa fa-check"></i><b>5.3.1</b> select(), pull()</a></li>
<li class="chapter" data-level="5.3.2" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#group_by-tally-summarise"><i class="fa fa-check"></i><b>5.3.2</b> group_by(), tally(), summarise()</a></li>
<li class="chapter" data-level="5.3.3" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#filter"><i class="fa fa-check"></i><b>5.3.3</b> filter()</a></li>
<li class="chapter" data-level="5.3.4" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#sort-arrange-top_n"><i class="fa fa-check"></i><b>5.3.4</b> sort(), arrange(), top_n()</a></li>
<li class="chapter" data-level="5.3.5" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#left_join-inner_join-anti_join"><i class="fa fa-check"></i><b>5.3.5</b> left_join(), inner_join(), anti_join()</a></li>
<li class="chapter" data-level="5.3.6" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#piping"><i class="fa fa-check"></i><b>5.3.6</b> Piping</a></li>
<li class="chapter" data-level="5.3.7" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#plotting-using-ggplot"><i class="fa fa-check"></i><b>5.3.7</b> Plotting using ggplot()</a></li>
<li class="chapter" data-level="5.3.8" data-path="quick-introduction-to-r-and-the-tidyverse.html"><a href="quick-introduction-to-r-and-the-tidyverse.html#doing-this-with-newspaper-data"><i class="fa fa-check"></i><b>5.3.8</b> Doing this with newspaper data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html"><i class="fa fa-check"></i><b>6</b> Geocode and map newspaper titles</a><ul>
<li class="chapter" data-level="6.1" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#a-map-of-british-newspapers-by-city"><i class="fa fa-check"></i><b>6.1</b> A map of British Newspapers by City</a></li>
<li class="chapter" data-level="6.2" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#drawing-a-background-map."><i class="fa fa-check"></i><b>6.2</b> Drawing a background map. `</a><ul>
<li class="chapter" data-level="6.2.1" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#mapping-with-ggplot2-and-mapdata"><i class="fa fa-check"></i><b>6.2.1</b> Mapping with ggplot2 and mapdata</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#add-some-points"><i class="fa fa-check"></i><b>6.3</b> Add some points</a><ul>
<li class="chapter" data-level="6.3.1" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#get-a-count-of-the-total-titles-for-each-city"><i class="fa fa-check"></i><b>6.3.1</b> Get a count of the total titles for each city</a></li>
<li class="chapter" data-level="6.3.2" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#get-a-list-of-points."><i class="fa fa-check"></i><b>6.3.2</b> Get a list of points.</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#choropleth-map"><i class="fa fa-check"></i><b>6.4</b> Choropleth map</a></li>
<li class="chapter" data-level="6.5" data-path="geocode-and-map-newspaper-titles.html"><a href="geocode-and-map-newspaper-titles.html#make-the-points-object."><i class="fa fa-check"></i><b>6.5</b> Make the points object.</a></li>
</ul></li>
<li class="part"><span><b>I Part III: Examples</b></span></li>
<li class="chapter" data-level="7" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>7</b> Text mining</a><ul>
<li class="chapter" data-level="7.1" data-path="text-mining.html"><a href="text-mining.html#what-were-the-most-common-words-used-in-newspaper-titles-in-the-nineteenth-century_"><i class="fa fa-check"></i><b>7.1</b> What were the most common words used in newspaper titles in the nineteenth century?_</a><ul>
<li class="chapter" data-level="7.1.1" data-path="text-mining.html"><a href="text-mining.html#titles-dont-just-help-you-identity-a-newspaper-but-they-might-tell-you-a-little-bit-about-the-time-in-which-they-were-established.-with-a-bibliographic-list-of-all-our-uk-and-irish-titles-we-can-count-the-most-frequent-words-and-track-them-over-time-and-place-using-text-mining-and-data-analysis."><i class="fa fa-check"></i><b>7.1.1</b> Titles don’t just help you identity a newspaper, but they might tell you a little bit about the time in which they were established. With a bibliographic list of <em>all</em> our UK and Irish titles, we can count the most frequent words and track them over time and place, using text mining and data analysis.</a></li>
<li class="chapter" data-level="7.1.2" data-path="text-mining.html"><a href="text-mining.html#what-is-this-document"><i class="fa fa-check"></i><b>7.1.2</b> What is this document?</a></li>
<li class="chapter" data-level="7.1.3" data-path="text-mining.html"><a href="text-mining.html#how-about-differences-by-country"><i class="fa fa-check"></i><b>7.1.3</b> How about differences by country?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="extract-text.html"><a href="extract-text.html"><i class="fa fa-check"></i><b>8</b> Make a Text Corpus</a><ul>
<li class="chapter" data-level="8.1" data-path="extract-text.html"><a href="extract-text.html#where-is-this-data"><i class="fa fa-check"></i><b>8.1</b> Where is this data?</a></li>
<li class="chapter" data-level="8.2" data-path="extract-text.html"><a href="extract-text.html#folder-structure"><i class="fa fa-check"></i><b>8.2</b> Folder structure</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="term-frequencies.html"><a href="term-frequencies.html"><i class="fa fa-check"></i><b>9</b> Term Frequencies</a></li>
<li class="chapter" data-level="10" data-path="tf-idf.html"><a href="tf-idf.html"><i class="fa fa-check"></i><b>10</b> Tf_idf:</a></li>
<li class="chapter" data-level="11" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html"><i class="fa fa-check"></i><b>11</b> Sentiment analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#install-and-load-relevant-packages"><i class="fa fa-check"></i><b>11.1</b> Install and load relevant packages</a></li>
<li class="chapter" data-level="11.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#fetch-sentiment-data"><i class="fa fa-check"></i><b>11.2</b> Fetch sentiment data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#afinn-dataset"><i class="fa fa-check"></i><b>11.2.1</b> Afinn dataset</a></li>
<li class="chapter" data-level="11.2.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#bing-dataset"><i class="fa fa-check"></i><b>11.2.2</b> Bing dataset</a></li>
<li class="chapter" data-level="11.2.3" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#loughran-dataset"><i class="fa fa-check"></i><b>11.2.3</b> Loughran dataset</a></li>
<li class="chapter" data-level="11.2.4" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#nrc-dataset"><i class="fa fa-check"></i><b>11.2.4</b> NRC dataset</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#load-the-tokenised-news-sample"><i class="fa fa-check"></i><b>11.3</b> Load the tokenised news sample</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="topic-modelling.html"><a href="topic-modelling.html"><i class="fa fa-check"></i><b>12</b> Topic modelling</a></li>
<li class="chapter" data-level="13" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html"><i class="fa fa-check"></i><b>13</b> Detecting text reuse in newspaper articles.</a><ul>
<li class="chapter" data-level="13.1" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#turn-the-newspaper-sample-into-a-bunch-of-text-documents-one-per-article"><i class="fa fa-check"></i><b>13.1</b> Turn the newspaper sample into a bunch of text documents, one per article</a><ul>
<li class="chapter" data-level="13.1.1" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#load-the-dataframe-and-preprocess"><i class="fa fa-check"></i><b>13.1.1</b> Load the dataframe and preprocess</a></li>
<li class="chapter" data-level="13.1.2" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#make-a-text-file-from-each-article"><i class="fa fa-check"></i><b>13.1.2</b> Make a text file from each article</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#load-the-files-as-a-textreusecorpus"><i class="fa fa-check"></i><b>13.2</b> Load the files as a TextReuseCorpus</a><ul>
<li class="chapter" data-level="13.2.1" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#generate-a-minhash"><i class="fa fa-check"></i><b>13.2.1</b> Generate a minhash</a></li>
<li class="chapter" data-level="13.2.2" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#create-the-textreusecorpus"><i class="fa fa-check"></i><b>13.2.2</b> Create the TextReuseCorpus</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="detecting-text-reuse-in-newspaper-articles-.html"><a href="detecting-text-reuse-in-newspaper-articles-.html#further-reading"><i class="fa fa-check"></i><b>13.3</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>14</b> Further reading</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Demystifying Newspaper Data with R (and a tiny bit of Python)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="detecting-text-reuse-in-newspaper-articles." class="section level1">
<h1><span class="header-section-number">13</span> Detecting text reuse in newspaper articles.</h1>
<p>19th century newspapers shared text all the time. Sometimes this took the form of credited reports from other titles. For much of the century, newspapers paid the post office to give them a copy of all other titles. Official reused dispatches were not the only way text was reused: advertisements, of course, were placed in multiple titles at the same time, and editors were happy to use snippets, jokes, and so forth</p>
<p>Detecting the extent of this reuse is a great use of digital tools. R has a library, <em>textreuse</em>, which allows you to do this reasonably simply. It was intended to be used for plagiarism detection and to find duplicated documents, but it can also be repurposed to find shared articles.</p>
<p>Some of the most inspiring news data projects at the moment are looking at text reuse. The <em>Oceanic Exchanges</em> project is a multi-partner project using various methods to detect this overlap. This methods paper is really interesting, and used a similar starting point, though it then does an extra step of calculating ‘local alignment’ with each candidate pair, to improve the accuracy.<span class="citation"><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></span></p>
<p>Melodee Beals’s <a href="http://scissorsandpaste.net"><em>Scissors and Paste</em></a> project, at Loughborough and also part of <em>Oceanic Exchanges</em>, also looks at text reuse in 19th century British newspapers. <a href="http://comhis.fi/clusters">Another project</a>, looking at Finnish newspapers, used a technique usually used to detect protein strains to find clusters of text reuse on particularly inaccurate OCR.<span class="citation"><a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></span></p>
<p>The steps are the following:</p>
<ul>
<li><p>Turn the newspaper sample into a bunch of text documents, one per article</p></li>
<li><p>Load these into R as a special forat called a TextReuseCorpus.</p></li>
<li><p>Divide the text into a series of overlapping sequences of words, known as n-grams.</p></li>
<li><p>‘Hash’ the n-grams - each one is given a numerical code, which is much less memory-hungry. Randomly select 200 of these hashes to represent each document.</p></li>
<li><p>Use a local sensitivity hashing algorithm (I’ll explain a bit below) to generate a list of potential candidates for text reuse</p></li>
<li><p>Calculate the similarity scores for these candidates</p></li>
<li><p>Calculate the local alignment of the pairs to find out exactly which bits overlap</p></li>
</ul>
<p>To set some expectations: this tutorial uses a small sample dataset of one title over a period of months, and unsurprisingly, there’s not really any text re-use. A larger corpus over a short time period, with a number of titles, would probably give more interesting results.</p>
<p>Also, these techniques were developed with modern text in mind, and so the results will be limited by the accurary of the OCR, but by setting the parameters reasonably loose we might be able to mitigate for this.</p>
<p><a href="http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html" class="uri">http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html</a></p>
<p><a href="http://infolab.stanford.edu/~ullman/mmds/ch3.pdf" class="uri">http://infolab.stanford.edu/~ullman/mmds/ch3.pdf</a></p>
<div id="turn-the-newspaper-sample-into-a-bunch-of-text-documents-one-per-article" class="section level2">
<h2><span class="header-section-number">13.1</span> Turn the newspaper sample into a bunch of text documents, one per article</h2>
<p>Load libaries: the usual suspect, tidyverse, and also the package ‘textreuse’. If it’s not installed, you’ll need to do so using <code>install.packages('textreuse')</code></p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb274-2" data-line-number="2"><span class="kw">library</span>(textreuse)</a></code></pre></div>
<pre><code>## 
## Attaching package: &#39;textreuse&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:readr&#39;:
## 
##     tokenize</code></pre>
<div id="load-the-dataframe-and-preprocess" class="section level3">
<h3><span class="header-section-number">13.1.1</span> Load the dataframe and preprocess</h3>
<p>In the <em>extract text</em> chapter <a href="#label"><strong>??</strong></a>, you created a dataframe, with one row per article. The first step is to reload that dataframe into memory, and do some minor preprocessing.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" data-line-number="1"><span class="kw">load</span>(<span class="st">&#39;news_sample_dataframe&#39;</span>)</a></code></pre></div>
<p>Make a more useful code to use as an article ID.
First use <code>str_pad()</code> to add leading zeros up to a maximum of three digits.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb278-1" data-line-number="1">news_sample_dataframe<span class="op">$</span>article_code =<span class="st"> </span><span class="kw">str_pad</span>(news_sample_dataframe<span class="op">$</span>article_code, <span class="dt">width =</span> <span class="dv">3</span>, <span class="dt">pad =</span> <span class="st">&#39;0&#39;</span>)</a></code></pre></div>
<p>Use <code>paste0()</code> to add the prefix ‘article’ to this number.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" data-line-number="1">news_sample_dataframe<span class="op">$</span>article_code =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&#39;article_&#39;</span>,news_sample_dataframe<span class="op">$</span>article_code)</a></code></pre></div>
</div>
<div id="make-a-text-file-from-each-article" class="section level3">
<h3><span class="header-section-number">13.1.2</span> Make a text file from each article</h3>
<p>This is a very simple function - it says, for each row in the news_sample_dataframe, write the third cell (which is where the text of the article is stored), using a function from a library called data.table called fwrite(), store it in a folder called textfiles/, and make a filename from the article code concatenated with ‘.txt’.</p>
<p>Now you should have a folder in the project folder called textfiles, with 600 or so text documents inside.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb280-1" data-line-number="1"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb280-2" data-line-number="2"></a>
<a class="sourceLine" id="cb280-3" data-line-number="3"></a>
<a class="sourceLine" id="cb280-4" data-line-number="4"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(news_sample_dataframe)){</a>
<a class="sourceLine" id="cb280-5" data-line-number="5">    </a>
<a class="sourceLine" id="cb280-6" data-line-number="6">    </a>
<a class="sourceLine" id="cb280-7" data-line-number="7">  filename =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;textfiles/&quot;</span>, news_sample_dataframe[i,<span class="dv">1</span>],<span class="st">&quot;.txt&quot;</span>)</a>
<a class="sourceLine" id="cb280-8" data-line-number="8">  </a>
<a class="sourceLine" id="cb280-9" data-line-number="9">  <span class="kw">fwrite</span>(news_sample_dataframe[i,<span class="dv">3</span>], <span class="dt">file =</span> filename)</a>
<a class="sourceLine" id="cb280-10" data-line-number="10">}</a></code></pre></div>
</div>
</div>
<div id="load-the-files-as-a-textreusecorpus" class="section level2">
<h2><span class="header-section-number">13.2</span> Load the files as a TextReuseCorpus</h2>
<div id="generate-a-minhash" class="section level3">
<h3><span class="header-section-number">13.2.1</span> Generate a minhash</h3>
<p>Use the function minhash_generator() to specify the number of minhashes you want to represent each document. Set the random seed to make it reproducible.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb281-1" data-line-number="1">minhash &lt;-<span class="st"> </span><span class="kw">minhash_generator</span>(<span class="dt">n =</span> <span class="dv">400</span>, <span class="dt">seed =</span> <span class="dv">1234</span>)</a></code></pre></div>
</div>
<div id="create-the-textreusecorpus" class="section level3">
<h3><span class="header-section-number">13.2.2</span> Create the TextReuseCorpus</h3>
<p><code>TextReuseCorpus()</code> takes a number of arguments. Going through each in turn:</p>
<p><em>dir =</em> is the directory where all the text files are stored.</p>
<p><em>tokenizer</em> is the function which tokenises the text. Here we’ve used tokenize_ngrams, but it could also be tokenize words. You could build your own: for example, if you thought that comparing similar characters in small sequences would help to detect text reuse, you could use that to compare the documents.</p>
<p><em>n</em> is the number of tokens in the ngram tokeniser. Setting it at 4 turns the following sentence:</p>
<blockquote>
<p>Here we’ve used tokenize_ngrams, but it could also be tokenize words</p>
</blockquote>
<p>into:</p>
<p>Here we’ve used tokenize_ngrams
we’ve used tokenize_ngrams but
used tokenize_ngrams but it
tokenize_ngrams but it could
but it could also
it could also be
could also be tokenize
also be tokenize words</p>
<p><em>minhash_func =</em> is the parameters set using <code>minhash_generator()</code> above</p>
<p><em>keep_tokens =</em> Whether or not you keep the actual tokens, or just the hashes. There’s no real point keeping the tokens as we use the hashes to make the comparisons.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" data-line-number="1">reusecorpus &lt;-<span class="st"> </span><span class="kw">TextReuseCorpus</span>(<span class="dt">dir =</span> <span class="st">&quot;textfiles/&quot;</span>, <span class="dt">tokenizer =</span> tokenize_ngrams, <span class="dt">n =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb282-2" data-line-number="2">                          <span class="dt">minhash_func =</span> minhash, <span class="dt">keep_tokens =</span> <span class="ot">FALSE</span>, <span class="dt">progress =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Now each document is represented by a series of hashes, which are substitutes for small sequences of text. For example, this is the first ten minhashes for the first article:</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">minhashes</span>(reusecorpus[[<span class="dv">1</span>]]),<span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] -2147136494 -2138512326 -2147425767 -2143106471 -2135745958 -2141828729
##  [7] -2145358196 -2137240488 -2143623091 -2142163990</code></pre>
<p>At this point, you could compare any document’s sequences of hashes to any other, and get its ‘Jacquard Similarity’ score, which counts the number of shared hashes in the documents. The more shared hashes, the higher the similarity.</p>
<p>However, it would be very difficult, even for a computer, to use this to compare every document to every other in a corpus. A Local Sensitivity Hashing algorithm is used to solve this problem. This groups the representations together, and finds pairs of documents that should be compared for similarity.</p>
<blockquote>
<p>LSH breaks the minhashes into a series of bands comprised of rows. For example, 200 minhashes might broken into 50 bands of 4 rows each. Each band is hashed to a bucket. If two documents have the exact same minhashes in a band, they will be hashed to the same bucket, and so will be considered candidate pairs. Each pair of documents has as many chances to be considered a candidate as their are bands, and the fewer rows there are in each band, the more likely it is that each document will match another. (<a href="https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html" class="uri">https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html</a>)</p>
</blockquote>
<p>First create the buckets. You can try other values for the bands.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" data-line-number="1">buckets &lt;-<span class="st"> </span><span class="kw">lsh</span>(reusecorpus, <span class="dt">bands =</span> <span class="dv">80</span>, <span class="dt">progress =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Next, use <code>lsh_candidates()</code> to compare each bucket, and generate a list of candidates.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" data-line-number="1">candidates &lt;-<span class="st"> </span><span class="kw">lsh_candidates</span>(buckets)</a></code></pre></div>
<p>Next we go back to the full corpus, and calculate the similarity score for these pairs, using <code>lsh_compare()</code>. The first argument is the candidates, the second is the full corpus, the third is the method (other similarity functions could be used).</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" data-line-number="1">jacsimilarity_both =<span class="st"> </span><span class="kw">lsh_compare</span>(candidates, </a>
<a class="sourceLine" id="cb287-2" data-line-number="2">                                 reusecorpus, </a>
<a class="sourceLine" id="cb287-3" data-line-number="3">                                 jaccard_similarity, </a>
<a class="sourceLine" id="cb287-4" data-line-number="4">                                 <span class="dt">progress =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb287-5" data-line-number="5"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(score))</a>
<a class="sourceLine" id="cb287-6" data-line-number="6"></a>
<a class="sourceLine" id="cb287-7" data-line-number="7">jacsimilarity_both</a></code></pre></div>
<pre><code>## # A tibble: 56 x 3
##    a           b           score
##    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;
##  1 article_017 article_342     1
##  2 article_019 article_043     1
##  3 article_019 article_068     1
##  4 article_019 article_395     1
##  5 article_019 article_400     1
##  6 article_019 article_461     1
##  7 article_019 article_463     1
##  8 article_019 article_513     1
##  9 article_019 article_603     1
## 10 article_043 article_068     1
## # … with 46 more rows</code></pre>
<p>It returns a similarity score for each pair: The first pair have a 25% overlap, and the second a much smaller number.</p>
<p>The last thing is to join up the article codes to the full text dataset, and actually see what pairs have been detected. This is done using two <code>left_join()</code> commands, one for a and one for b. Also select just the relevant columns.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" data-line-number="1">matchedtexts =<span class="st"> </span>jacsimilarity_both <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb289-2" data-line-number="2"><span class="st">  </span><span class="kw">left_join</span>(news_sample_dataframe, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&#39;a&#39;</span> =<span class="st"> &#39;article_code&#39;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb289-3" data-line-number="3"><span class="st">  </span><span class="kw">left_join</span>(news_sample_dataframe, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&#39;b&#39;</span> =<span class="st"> &#39;article_code&#39;</span>))<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(a,b,score, text.x, text.y)</a>
<a class="sourceLine" id="cb289-4" data-line-number="4"></a>
<a class="sourceLine" id="cb289-5" data-line-number="5">matchedtexts</a></code></pre></div>
<pre><code>## # A tibble: 56 x 5
##    a           b           score text.x                  text.y                 
##    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;                  
##  1 article_017 article_342     1 TIIE  NATIONAL  REGIST… TIIE  NATIONAL  REGIST…
##  2 article_019 article_043     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  3 article_019 article_068     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  4 article_019 article_395     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  5 article_019 article_400     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  6 article_019 article_461     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  7 article_019 article_463     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  8 article_019 article_513     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
##  9 article_019 article_603     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
## 10 article_043 article_068     1 THE  NATIONAL  REGISTE… THE  NATIONAL  REGISTE…
## # … with 46 more rows</code></pre>
<p>Well, the first is just the newspaper title, with a couple of extra words, so that’s no use.</p>
<p>How about the second pair? We can use another function from textreuse to check the ‘local alignment’. This is like comparing two documents in Microsoft Word: it finds the bit of the text with the most overlap, and it points out where in this overlap there are different words, replacing them with ######</p>
<p>First turn the text in each cell into a string:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" data-line-number="1">a =<span class="st"> </span><span class="kw">paste</span>(matchedtexts<span class="op">$</span>text.x[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>, <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb291-2" data-line-number="2"></a>
<a class="sourceLine" id="cb291-3" data-line-number="3">b =<span class="st">  </span><span class="kw">paste</span>(matchedtexts<span class="op">$</span>text.y[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>, <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>)</a></code></pre></div>
<p>Call the <code>align_local()</code> function, giving it the two strings to compare.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" data-line-number="1"><span class="kw">align_local</span>(a, b)</a></code></pre></div>
<pre><code>## TextReuse alignment
## Alignment score: 6 
## Document A:
## THE NATIONAL REGISTER
## 
## Document B:
## THE NATIONAL REGISTER</code></pre>
<p>Looks like we found a repeated advert for a series of portraits!</p>
<p>This is very much a beginning, but I hope you can see the potential. It’s worth noting that the article segmentation in these newspapers might actually work against the process, because it often lumps multiple articles into one document. Consequently, the software won’t find potential matches if there’s too much other non-matching same text in the same document.</p>
<p>A potential work-around would be to split the document into chunks of text, and compare these chunks. The chunks could be joined back to the full articles, and using local alignment, the specific bits that overlapped could be found.</p>
</div>
</div>
<div id="further-reading" class="section level2">
<h2><span class="header-section-number">13.3</span> Further reading</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>David A. Smith, Ryan Cordell, and Abby Mullen, ‘Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers’, <em>American Literary History</em>, 27.3 (2015), E1–E15 &lt;<a href="https://doi.org/10.1093/alh/ajv029">https://doi.org/10.1093/alh/ajv029</a>&gt;.<a href="detecting-text-reuse-in-newspaper-articles-.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p><span class="citeproc-not-found" data-reference-id="inproceedings-salmi"><strong>???</strong></span>, <span class="citation">@inproceedings-blast</span>.<a href="detecting-text-reuse-in-newspaper-articles-.html#fnref16" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topic-modelling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-reading-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
